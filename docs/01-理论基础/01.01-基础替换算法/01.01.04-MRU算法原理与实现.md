# 01.01.04 MRU算法原理与实现

## 概述

MRU（Most Recently Used，最近使用）是LRU的反向算法，优先淘汰最近使用的数据。MRU适用于某些特殊场景，如扫描模式访问。

## 核心思想

### 设计原则

1. **反向LRU**：与LRU相反，淘汰最近使用的数据
2. **扫描模式**：适用于顺序扫描场景
3. **简单实现**：实现简单，O(1)时间复杂度

### 工作流程

```
新数据进入 → 添加到队列头部 → 队列满时 → 淘汰队列头部数据（最近使用的）
```

## 数据结构设计

### 队列实现

```c
// MRU缓存结构（与LRU类似，但淘汰策略相反）
typedef struct MRUCache {
    int capacity;
    int size;
    struct Node *head;  // 队列头（最近使用）
    struct Node *tail;  // 队列尾（最久使用）
    struct Node **hash; // 哈希表（快速查找）
} MRUCache;

// 节点结构
typedef struct Node {
    int key;
    int value;
    struct Node *next;
    struct Node *prev;
} Node;
```

## 核心操作

### 1. 查找操作

```c
int mruGet(MRUCache *cache, int key) {
    // 1. 查找节点
    int index = key % cache->capacity;
    Node *node = cache->hash[index];

    while (node) {
        if (node->key == key) {
            // 2. 移动到头部（标记为最近使用）
            moveToHead(cache, node);
            return node->value;
        }
        node = node->next;
    }

    return -1;
}
```

### 2. 插入操作

```c
void mruPut(MRUCache *cache, int key, int value) {
    // 1. 检查是否已存在
    int index = key % cache->capacity;
    Node *node = cache->hash[index];

    while (node) {
        if (node->key == key) {
            // 更新值并移动到头部
            node->value = value;
            moveToHead(cache, node);
            return;
        }
        node = node->next;
    }

    // 2. 如果缓存已满，淘汰头部（最近使用的）
    if (cache->size >= cache->capacity) {
        Node *evict = cache->head;
        cache->head = evict->next;
        if (cache->head) {
            cache->head->prev = NULL;
        } else {
            cache->tail = NULL;
        }

        // 从哈希表删除
        int evict_index = evict->key % cache->capacity;
        Node **prev = &cache->hash[evict_index];
        while (*prev != evict) {
            prev = &(*prev)->next;
        }
        *prev = evict->next;

        free(evict);
        cache->size--;
    }

    // 3. 添加新节点到头部
    Node *new_node = (Node *)malloc(sizeof(Node));
    new_node->key = key;
    new_node->value = value;
    new_node->next = cache->head;
    new_node->prev = NULL;

    if (cache->head) {
        cache->head->prev = new_node;
    } else {
        cache->tail = new_node;
    }
    cache->head = new_node;

    // 添加到哈希表
    index = key % cache->capacity;
    new_node->next = cache->hash[index];
    cache->hash[index] = new_node;

    cache->size++;
}
```

## 适用场景

### 优势场景

1. **顺序扫描**
   - 数据按顺序访问
   - 访问过的数据不再访问
   - MRU可以提前淘汰

2. **循环访问**
   - 数据循环访问
   - 最近访问的数据不会立即再访问
   - MRU可以保留更久未访问的数据

3. **反向时间局部性**
   - 访问模式与时间局部性相反
   - 最近访问的数据不会再次访问

### 示例场景

```python
# 顺序扫描场景
def sequential_scan():
    # 顺序访问数据：1, 2, 3, 4, 5, ...
    # 访问过的数据不会再次访问
    # MRU：访问1后，立即淘汰1，为后续数据腾出空间
    # LRU：访问1后，保留1，浪费空间
```

## 性能特征

### 优势

| 特性 | 说明 |
|------|------|
| **扫描模式** | 适合顺序扫描场景 |
| **实现简单** | 代码量少 |
| **时间复杂度** | O(1) |

### 劣势

| 特性 | 说明 |
|------|------|
| **通用性差** | 只适用于特殊场景 |
| **命中率低** | 大多数场景命中率低 |

## 与其他算法对比

### MRU vs LRU

| 维度 | MRU | LRU |
|------|-----|-----|
| **淘汰策略** | 最近使用 | 最久未使用 |
| **适用场景** | 顺序扫描 | 时间局部性 |
| **命中率** | 扫描场景高 | 一般场景高 |

### MRU vs FIFO

| 维度 | MRU | FIFO |
|------|-----|------|
| **淘汰策略** | 最近使用 | 先进先出 |
| **实现复杂度** | 中 | 低 |
| **适用场景** | 顺序扫描 | 随机访问 |

## 扩展阅读

- [LRU算法原理与实现](./01.01.01-LRU算法原理与实现.md)
- [FIFO算法原理与实现](./01.01.03-FIFO算法原理与实现.md)
- [算法对比与决策](../01.05-算法对比与决策/README.md)

## 权威参考

- **《算法导论》** - MIT Press
- **《数据结构与算法分析》** - Mark Allen Weiss
- **缓存替换算法论文** - "A Study of Replacement Algorithms"
