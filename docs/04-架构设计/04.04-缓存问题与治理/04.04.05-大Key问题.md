# 04.04.05 大Key问题

## 概述

大Key（Big Key）是指**单个key的value过大**，导致Redis操作延迟增加、内存占用过高、网络传输阻塞等问题。

## 问题定义

### 大Key标准

```python
# 大Key判断标准
def is_big_key(key, value):
    # 1. 字符串类型：value > 10KB
    if isinstance(value, str):
        return len(value) > 10 * 1024

    # 2. Hash类型：field数量 > 1000 或 value > 10MB
    if isinstance(value, dict):
        return len(value) > 1000 or get_size(value) > 10 * 1024 * 1024

    # 3. List类型：元素数量 > 10000
    if isinstance(value, list):
        return len(value) > 10000

    # 4. Set类型：元素数量 > 10000
    if isinstance(value, set):
        return len(value) > 10000

    # 5. ZSet类型：元素数量 > 10000
    if isinstance(value, dict):  # ZSet内部是dict
        return len(value) > 10000

    return False
```

### 常见大Key场景

1. **用户画像**：存储用户所有标签和属性
2. **统计数据**：存储大量统计数据
3. **缓存对象**：缓存整个对象而非部分字段
4. **消息队列**：使用List作为消息队列，积累过多消息

## 问题影响

### 1. 内存占用

```python
# 大Key占用大量内存
# 示例：100MB的Hash
# 问题：
# - 占用大量内存
# - 影响其他key的内存分配
# - 可能导致OOM
```

### 2. 操作延迟

```python
# 大Key操作延迟高
# 示例：100MB的Hash执行HGETALL
# 延迟：~100ms（正常操作<1ms）
# 问题：
# - 阻塞Redis单线程
# - 影响其他请求
# - 可能导致超时
```

### 3. 网络传输

```python
# 大Key网络传输慢
# 示例：100MB的Key通过网络传输
# 时间：~1秒（1Gbps网络）
# 问题：
# - 占用网络带宽
# - 传输时间长
# - 可能导致超时
```

### 4. 主从复制

```python
# 大Key主从复制慢
# 示例：100MB的Key复制到从节点
# 时间：~1秒
# 问题：
# - 阻塞主从复制
# - 影响其他key的复制
# - 可能导致复制延迟
```

## 解决方案

### 方案1：Key拆分

```python
# 将大Key拆分为多个小Key
class ShardedBigKey:
    def __init__(self, redis_client, base_key, shard_size=1000):
        self.redis = redis_client
        self.base_key = base_key
        self.shard_size = shard_size

    def set_hash(self, hash_data):
        # 拆分Hash
        shard_id = 0
        current_shard = {}

        for field, value in hash_data.items():
            current_shard[field] = value

            if len(current_shard) >= self.shard_size:
                shard_key = f"{self.base_key}:shard:{shard_id}"
                self.redis.hmset(shard_key, current_shard)
                current_shard = {}
                shard_id += 1

        # 保存最后一个分片
        if current_shard:
            shard_key = f"{self.base_key}:shard:{shard_id}"
            self.redis.hmset(shard_key, current_shard)

        # 保存分片数量
        self.redis.set(f"{self.base_key}:shard_count", shard_id + 1)

    def get_hash(self):
        # 合并所有分片
        shard_count = int(self.redis.get(f"{self.base_key}:shard_count") or 0)
        result = {}

        for shard_id in range(shard_count):
            shard_key = f"{self.base_key}:shard:{shard_id}"
            shard_data = self.redis.hgetall(shard_key)
            result.update(shard_data)

        return result
```

### 方案2：压缩存储

```python
import gzip
import json

class CompressedBigKey:
    def __init__(self, redis_client):
        self.redis = redis_client

    def set_compressed(self, key, data):
        # 压缩数据
        json_str = json.dumps(data)
        compressed = gzip.compress(json_str.encode())

        # 存储压缩数据
        self.redis.set(key, compressed)

    def get_compressed(self, key):
        # 获取压缩数据
        compressed = self.redis.get(key)
        if not compressed:
            return None

        # 解压数据
        json_str = gzip.decompress(compressed).decode()
        return json.loads(json_str)
```

**压缩效果**：
- 原始：100MB
- 压缩后：10MB（压缩率90%）

### 方案3：异步删除

```python
# Redis 4.0+支持异步删除大Key
# 配置：
# lazyfree-lazy-eviction yes
# lazyfree-lazy-expire yes
# lazyfree-lazy-server-del yes

# 使用UNLINK而非DEL
redis.unlink("big_key")  # 异步删除
# vs
redis.delete("big_key")  # 同步删除（阻塞）
```

### 方案4：数据迁移

```python
# 将大Key迁移到其他存储
class BigKeyMigration:
    def __init__(self, redis_client, db_client):
        self.redis = redis_client
        self.db = db_client

    def migrate_big_key(self, key):
        # 1. 从Redis读取
        value = self.redis.get(key)

        if not value or len(value) < 10 * 1024:
            return  # 不是大Key

        # 2. 存储到数据库
        self.db.set(key, value)

        # 3. 从Redis删除
        self.redis.delete(key)

        # 4. 设置引用
        self.redis.set(f"{key}:migrated", "1")

    def get_migrated_key(self, key):
        # 1. 检查是否已迁移
        if self.redis.get(f"{key}:migrated"):
            # 2. 从数据库读取
            return self.db.get(key)
        else:
            # 3. 从Redis读取
            return self.redis.get(key)
```

## 大Key检测

### 1. Redis命令检测

```bash
# 使用redis-cli检测
redis-cli --bigkeys

# 输出示例：
# [00.00%] Biggest string found so far 'big_key' with 10485760 bytes
# [00.00%] Biggest hash found so far 'big_hash' with 10000 fields
```

### 2. 程序检测

```python
import redis

class BigKeyDetector:
    def __init__(self, redis_client, threshold=10 * 1024):
        self.redis = redis_client
        self.threshold = threshold

    def detect_big_keys(self):
        big_keys = []

        # 扫描所有key
        for key in self.redis.scan_iter():
            key_type = self.redis.type(key)
            size = self.get_key_size(key, key_type)

            if size > self.threshold:
                big_keys.append({
                    'key': key,
                    'type': key_type,
                    'size': size
                })

        return big_keys

    def get_key_size(self, key, key_type):
        if key_type == 'string':
            return len(self.redis.get(key))
        elif key_type == 'hash':
            return self.redis.hlen(key)
        elif key_type == 'list':
            return self.redis.llen(key)
        elif key_type == 'set':
            return self.redis.scard(key)
        elif key_type == 'zset':
            return self.redis.zcard(key)
        return 0
```

### 3. 监控告警

```python
# 监控大Key
class BigKeyMonitor:
    def __init__(self, redis_client, threshold=10 * 1024):
        self.redis = redis_client
        self.threshold = threshold

    def monitor_big_keys(self):
        while True:
            big_keys = self.detect_big_keys()

            for key_info in big_keys:
                if key_info['size'] > self.threshold:
                    send_alert(f"Big key detected: {key_info['key']}, "
                              f"size: {key_info['size']}")

            time.sleep(60)  # 每分钟检查一次
```

## 预防策略

### 1. 设计规范

```python
# 1. 避免存储大对象
# 错误：
redis.set("user:1001", json.dumps(entire_user_object))  # 可能很大

# 正确：
redis.hset("user:1001", "name", user.name)
redis.hset("user:1001", "email", user.email)
# 只存储需要的字段

# 2. 使用分页
# 错误：
redis.lpush("messages", *all_messages)  # 可能很大

# 正确：
# 使用分页，每次只存储部分消息
for page in paginate(messages, page_size=100):
    redis.lpush(f"messages:page:{page_num}", *page)
```

### 2. 定期清理

```python
# 定期清理大Key
def cleanup_big_keys():
    detector = BigKeyDetector(redis_client)
    big_keys = detector.detect_big_keys()

    for key_info in big_keys:
        # 1. 尝试压缩
        if compress_key(key_info['key']):
            continue

        # 2. 尝试拆分
        if split_key(key_info['key']):
            continue

        # 3. 迁移到数据库
        migrate_to_db(key_info['key'])
```

## 扩展阅读

- [热点Key问题](./04.04.04-热点Key问题.md)
- [内存碎片](./04.04.06-内存碎片.md)
- [Redis内存管理](../../03-Redis组件/03.04-内存管理/README.md)

## 权威参考

- **《大型网站技术架构》** - 李智慧著
- **《高并发系统设计》** - 大型互联网公司技术博客
- **Redis官方文档** - <https://redis.io/docs/manual/optimization/>
