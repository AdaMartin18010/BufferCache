# 05.01.05 网卡DMA机制

## 概述

DMA（Direct Memory Access）是网卡直接访问内存的机制，绕过CPU实现高效数据传输。理解DMA机制对于优化Redis网络IO性能至关重要。

## DMA原理

### 基本概念

```c
// DMA基本概念
// 1. 网卡直接访问内存
// 2. 无需CPU参与数据传输
// 3. 减少CPU开销，提高性能

// DMA工作流程
// 1. CPU配置DMA描述符
// 2. 网卡接收数据包
// 3. DMA将数据写入内存
// 4. 网卡发送中断通知CPU
```

### DMA优势

```python
# DMA vs CPU拷贝
class DMAAdvantage:
    def compare(self):
        return {
            'cpu_copy': {
                'steps': ['网卡→CPU寄存器', 'CPU寄存器→内存'],
                'cpu_overhead': '高',
                'throughput': '低',
            },
            'dma': {
                'steps': ['网卡→内存（直接）'],
                'cpu_overhead': '低',
                'throughput': '高',
            },
        }
```

## Linux DMA实现

### 1. DMA描述符

```c
// DMA描述符结构
struct dma_desc {
    uint64_t addr;      // 数据缓冲区地址
    uint32_t length;    // 数据长度
    uint32_t flags;     // 控制标志
};

// 环形缓冲区
struct dma_ring {
    struct dma_desc *descs;  // 描述符数组
    int size;                // 环形缓冲区大小
    int head;                // 写指针
    int tail;                // 读指针
};
```

### 2. DMA映射

```c
// DMA映射（将虚拟地址映射到物理地址）
dma_addr_t dma_map_single(struct device *dev, void *ptr,
                          size_t size, enum dma_data_direction dir) {
    // 1. 获取物理地址
    phys_addr_t phys = virt_to_phys(ptr);

    // 2. 刷新CPU缓存
    dma_sync_single_for_device(dev, phys, size, dir);

    // 3. 返回DMA地址
    return phys_to_dma(dev, phys);
}
```

### 3. DMA传输

```c
// DMA传输流程
void dma_transfer(struct net_device *dev, struct sk_buff *skb) {
    // 1. 分配DMA缓冲区
    void *dma_buf = dma_alloc_coherent(dev, skb->len, &dma_addr, GFP_KERNEL);

    // 2. 拷贝数据到DMA缓冲区
    memcpy(dma_buf, skb->data, skb->len);

    // 3. 配置DMA描述符
    struct dma_desc *desc = get_dma_desc();
    desc->addr = dma_addr;
    desc->length = skb->len;
    desc->flags = DMA_DESC_VALID;

    // 4. 启动DMA传输
    start_dma_transfer(dev, desc);
}
```

## Redis中的应用

### 1. 网络接收

```c
// Redis网络接收（使用DMA）
void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) {
    // 1. 数据已通过DMA写入内核缓冲区
    // 2. epoll通知有数据可读
    // 3. Redis从内核缓冲区读取
    n = read(fd, c->querybuf+qblen, readlen);
}
```

### 2. 零拷贝优化

```c
// 零拷贝（sendfile）
// 1. DMA从磁盘读取数据到内核缓冲区
// 2. DMA从内核缓冲区传输到网卡
// 3. 无需CPU参与数据拷贝
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

## 性能分析

### 1. DMA vs CPU拷贝

```python
# DMA性能对比
class DMAPerformance:
    def benchmark(self):
        return {
            'cpu_copy': {
                'throughput': '1-2 GB/s',
                'cpu_usage': '50-80%',
            },
            'dma': {
                'throughput': '10-40 GB/s',
                'cpu_usage': '5-10%',
            },
        }
```

### 2. DMA延迟

```python
# DMA延迟分解
class DMALatency:
    def analyze(self):
        return {
            'setup': '1-5 μs',      # DMA设置
            'transfer': '10-100 μs', # 数据传输
            'interrupt': '1-10 μs',  # 中断处理
            'total': '12-115 μs',
        }
```

## 优化策略

### 1. DMA缓冲区大小

```c
// DMA缓冲区大小优化
// 1. 增大缓冲区减少中断次数
// 2. 平衡延迟和吞吐量
// 3. 考虑内存限制

// 推荐配置
#define DMA_BUFFER_SIZE 4096  // 4KB
#define DMA_RING_SIZE 256     // 256个描述符
```

### 2. DMA合并

```c
// DMA合并（减少中断）
// 1. 合并多个小包
// 2. 延迟中断处理
// 3. 批量处理

// 配置
#define DMA_COALESCING_ENABLED 1
#define DMA_COALESCING_TIMEOUT 100  // 100μs
```

### 3. NUMA优化

```c
// NUMA优化
// 1. DMA缓冲区分配在本地NUMA节点
// 2. 减少跨节点访问延迟
// 3. 提高缓存命中率

void *dma_alloc_coherent_numa(int node, size_t size) {
    return dma_alloc_coherent(dev, size, &dma_addr,
                              GFP_KERNEL | __GFP_THISNODE);
}
```

## 扩展阅读

- [NUMA架构影响](./05.01.02-NUMA架构影响.md)
- [零拷贝技术对比](../05.02-网络通信层/05.02.03-零拷贝技术对比.md)
- [epoll事件循环机制](../05.02-网络通信层/05.02.02-epoll事件循环机制.md)

## 权威参考

- **Linux内核文档** - <https://www.kernel.org/doc/html/latest/core-api/dma-api.html>
- **《深入理解Linux网络技术内幕》** - Christian Benvenuti著
- **Intel网卡文档** - <https://www.intel.com/content/www/us/en/products/network-io/ethernet.html>
