# 05.04.03 复制数据流

## 目录

- [05.04.03 复制数据流](#050403-复制数据流)
  - [目录](#目录)
  - [1. 概述](#1-概述)
    - [1.1 定义与历史背景](#11-定义与历史背景)
    - [1.2 应用价值](#12-应用价值)
  - [2. 复制数据流概览](#2-复制数据流概览)
    - [2.1 完整流程](#21-完整流程)
    - [2.2 关键路径](#22-关键路径)
  - [3. 全量同步数据流](#3-全量同步数据流)
    - [3.1 RDB传输流程](#31-rdb传输流程)
    - [3.2 RDB传输](#32-rdb传输)
  - [4. 增量同步数据流](#4-增量同步数据流)
    - [4.1 命令传播](#41-命令传播)
    - [4.2 复制积压缓冲区](#42-复制积压缓冲区)
  - [5. 从节点接收数据流](#5-从节点接收数据流)
    - [5.1 命令接收](#51-命令接收)
    - [5.2 命令执行](#52-命令执行)
  - [6. 部分重同步数据流](#6-部分重同步数据流)
    - [6.1 PSYNC流程](#61-psync流程)
    - [6.2 发送积压缓冲区](#62-发送积压缓冲区)
  - [7. 数据流延迟分解](#7-数据流延迟分解)
    - [7.1 各阶段延迟](#71-各阶段延迟)
  - [8. 性能优化](#8-性能优化)
    - [8.1 无盘复制](#81-无盘复制)
    - [8.2 并行复制](#82-并行复制)
    - [8.3 压缩传输](#83-压缩传输)
  - [9. 扩展阅读](#9-扩展阅读)
  - [10. 权威参考](#10-权威参考)
    - [10.1 经典书籍](#101-经典书籍)
    - [10.2 官方文档](#102-官方文档)
    - [10.3 在线资源](#103-在线资源)

---

## 1. 概述

### 1.1 定义与历史背景

**复制数据流**是Redis主从复制的完整数据流，包括全量同步、增量同步、命令传播等场景的数据流转路径。本文档深入分析Redis主从复制的完整数据流。

**历史发展**：

- **2009年**：Redis引入主从复制机制
- **2010年**：Redis引入部分重同步（PSYNC）
- **2015年**：Redis引入无盘复制
- **2020年代**：复制数据流优化和性能提升

### 1.2 应用价值

复制数据流在Redis高可用中具有重要价值：

1. **性能分析**：理解数据流有助于性能优化
2. **延迟分析**：分析各阶段延迟，定位性能瓶颈
3. **系统优化**：优化数据流，提升整体性能
4. **问题诊断**：通过数据流分析诊断问题

## 2. 复制数据流概览

### 2.1 完整流程

```text
主节点 → 命令执行 → 复制积压缓冲区 → 从节点接收 → 命令重放 → 数据同步
```

**完整流程**：主节点 → 命令执行 → 复制积压缓冲区 → 从节点接收 → 命令重放 → 数据同步。

### 2.2 关键路径

```text
主节点命令 → 复制积压缓冲区 → 从节点Socket → 从节点命令执行 → 数据更新
```

**关键路径**：主节点命令 → 复制积压缓冲区 → 从节点Socket → 从节点命令执行 → 数据更新。

## 3. 全量同步数据流

### 3.1 RDB传输流程

```c
// 主节点：生成RDB并发送
void replicationFeedSlaves(list *slaves, int dictid,
                          robj **argv, int argc) {
    listIter li;
    listNode *ln;
    client *slave;

    if (server.masterhost != NULL) return;
    if (slaves == NULL) return;

    listRewind(slaves, &li);
    while((ln = listNext(&li))) {
        slave = ln->value;

        // 1. 检查是否需要全量同步
        if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
            // 启动后台RDB生成
            if (server.rdb_child_pid == -1) {
                startBgsaveForReplication(slave);
            }
            continue;
        }

        if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_END) {
            // 等待RDB生成完成
            continue;
        }

        // 2. 增量同步：发送命令
        addReplyMultiBulkLen(slave, argc);
        for (int j = 0; j < argc; j++) {
            addReplyBulk(slave, argv[j]);
        }
    }
}

// 启动后台RDB生成
int startBgsaveForReplication(client *slave) {
    char filename[256];
    int fd;

    // 1. 创建RDB文件
    snprintf(filename, 256, "temp-repl-%d.rdb", (int)getpid());
    fd = open(filename, O_CREAT | O_WRONLY | O_TRUNC, 0644);
    if (fd == -1) {
        serverLog(LL_WARNING, "Opening temp file for replication: %s", strerror(errno));
        return C_ERR;
    }

    // 2. fork子进程
    pid_t pid = fork();
    if (pid == 0) {
        // 子进程：生成RDB
        close(server.fd);
        rdbSave(filename);
        exit(0);
    } else {
        // 父进程：记录状态
        server.rdb_child_pid = pid;
        server.rdb_child_type = RDB_CHILD_TYPE_DISK;
        server.rdb_save_time_start = time(NULL);

        // 3. 注册文件事件：RDB生成完成后发送
        aeCreateFileEvent(server.el, server.repl_transfer_fd,
                         AE_WRITABLE, sendBulkToSlave, slave);
        return C_OK;
    }
}
```

**RDB传输流程**：主节点生成RDB并发送，从节点接收RDB并加载。

### 3.2 RDB传输

```c
// 发送RDB文件到从节点
void sendBulkToSlave(aeEventLoop *el, int fd, void *privdata, int mask) {
    client *slave = privdata;
    UNUSED(el);
    UNUSED(mask);
    char buf[PROTO_IOBUF_LEN];
    ssize_t nwritten, buflen;

    // 1. 读取RDB文件
    buflen = read(server.repl_transfer_fd, buf, PROTO_IOBUF_LEN);
    if (buflen <= 0) {
        serverLog(LL_WARNING, "Read error sending DB to slave: %s",
                 (buflen == 0) ? "premature EOF" : strerror(errno));
        freeClient(slave);
        return;
    }

    // 2. 发送到从节点
    if ((nwritten = write(fd, buf, buflen)) == -1) {
        if (errno != EAGAIN) {
            serverLog(LL_WARNING, "Write error sending DB to slave: %s",
                     strerror(errno));
            freeClient(slave);
            return;
        }
        return;
    }

    server.repl_transfer_read += nwritten;

    // 3. 检查是否传输完成
    if (server.repl_transfer_read == server.repl_transfer_size) {
        close(server.repl_transfer_fd);
        server.repl_transfer_fd = -1;
        aeDeleteFileEvent(server.el, fd, AE_WRITABLE);
        putSlaveOnline(slave);
    }
}
```

**RDB传输**：发送RDB文件到从节点，检查是否传输完成。

**定理 3.1**：全量同步的时间复杂度为O(N)，其中N为key数量。

**证明**：

- RDB生成：O(N)（遍历所有key）
- RDB传输：O(N)（传输所有数据）
- RDB加载：O(N)（加载所有数据）
- 总时间复杂度：O(N)

## 4. 增量同步数据流

### 4.1 命令传播

```c
// 主节点：传播命令到从节点
void replicationFeedSlaves(list *slaves, int dictid,
                          robj **argv, int argc) {
    listIter li;
    listNode *ln;
    client *slave;

    if (server.masterhost != NULL) return;
    if (slaves == NULL) return;

    listRewind(slaves, &li);
    while((ln = listNext(&li))) {
        slave = ln->value;

        // 1. 跳过需要全量同步的从节点
        if (slave->replstate != SLAVE_STATE_ONLINE) continue;

        // 2. 写入复制积压缓冲区
        if (server.repl_backlog) {
            feedReplicationBacklog(argv, argc);
        }

        // 3. 发送命令到从节点
        addReplyMultiBulkLen(slave, argc);
        for (int j = 0; j < argc; j++) {
            addReplyBulk(slave, argv[j]);
        }
    }
}
```

**命令传播**：主节点传播命令到从节点，写入复制积压缓冲区。

### 4.2 复制积压缓冲区

```c
// 写入复制积压缓冲区
void feedReplicationBacklog(void *ptr, size_t len) {
    unsigned char *p = ptr;

    server.master_repl_offset += len;

    while(len) {
        size_t thislen = server.repl_backlog_size - server.repl_backlog_idx;
        if (thislen > len) thislen = len;

        // 写入缓冲区
        memcpy(server.repl_backlog + server.repl_backlog_idx, p, thislen);
        server.repl_backlog_idx += thislen;
        if (server.repl_backlog_idx == server.repl_backlog_size)
            server.repl_backlog_idx = 0;

        server.repl_backlog_histlen += thislen;
        len -= thislen;
        p += thislen;
    }

    // 更新偏移量
    if (server.repl_backlog_histlen > server.repl_backlog_size)
        server.repl_backlog_histlen = server.repl_backlog_size;

    server.repl_backlog_off = server.master_repl_offset -
                              server.repl_backlog_histlen + 1;
}
```

**复制积压缓冲区**：写入复制积压缓冲区，更新偏移量。

**定理 4.1**：增量同步的时间复杂度为O(1)（单条命令）。

**证明**：

- 命令传播：O(1)（单条命令）
- 命令接收：O(1)（单条命令）
- 命令执行：O(1)（单条命令）
- 总时间复杂度：O(1)

## 5. 从节点接收数据流

### 5.1 命令接收

```c
// 从节点：接收主节点命令
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;

    // 1. 读取数据
    nread = connRead(c->conn, c->querybuf + sdslen(c->querybuf), readlen);

    if (nread == -1) {
        if (errno == EAGAIN) {
            return;
        } else {
            serverLog(LL_VERBOSE, "Reading from client: %s", strerror(errno));
            freeClientAsync(c);
            return;
        }
    } else if (nread == 0) {
        serverLog(LL_VERBOSE, "Client closed connection");
        freeClientAsync(c);
        return;
    }

    // 2. 更新查询缓冲区
    sdsIncrLen(c->querybuf, nread);
    c->lastinteraction = server.unixtime;

    // 3. 处理查询缓冲区
    processInputBuffer(c);
}
```

**命令接收**：从节点接收主节点命令，更新查询缓冲区。

### 5.2 命令执行

```c
// 从节点：执行主节点命令
void processInputBuffer(client *c) {
    server.current_client = c;

    while (c->qb_pos < sdslen(c->querybuf)) {
        // 1. 解析命令
        if (c->reqtype == PROTO_REQ_INLINE) {
            if (processInlineBuffer(c) != C_OK) break;
        } else if (c->reqtype == PROTO_REQ_MULTIBULK) {
            if (processMultibulkBuffer(c) != C_OK) break;
        } else {
            serverPanic("Unknown request type");
        }

        // 2. 执行命令（从节点模式）
        if (c->argc == 0) {
            resetClient(c);
        } else {
            // 从节点执行命令（不传播）
            c->flags |= CLIENT_SLAVE;
            call(c, CMD_CALL_NONE);
            c->flags &= ~CLIENT_SLAVE;
        }
    }

    server.current_client = NULL;
}
```

**命令执行**：从节点执行主节点命令（不传播）。

## 6. 部分重同步数据流

### 6.1 PSYNC流程

```c
// 从节点：请求部分重同步
void replicationSendAck(void) {
    client *c = server.master;

    if (c == NULL) return;

    // 发送REPLCONF ACK
    if (c->flags & CLIENT_MASTER) {
        addReplyMultiBulkLen(c, 3);
        addReplyBulkCString(c, "REPLCONF");
        addReplyBulkCString(c, "ACK");
        addReplyBulkLongLong(c, c->reploff);
    }
}

// 主节点：处理PSYNC请求
void replicationReceiveAck(client *c, long long offset) {
    if (server.masterhost != NULL) return;

    // 1. 检查偏移量是否在积压缓冲区中
    if (server.repl_backlog &&
        offset >= server.repl_backlog_off &&
        offset < server.repl_backlog_off + server.repl_backlog_histlen) {

        // 2. 可以部分重同步
        c->replstate = SLAVE_STATE_ONLINE;
        c->repl_ack_off = offset;
        c->repl_ack_time = server.unixtime;

        // 3. 发送积压缓冲区中的数据
        sendReplicationBacklog(c, offset);
    } else {
        // 4. 需要全量同步
        sendBulkToSlave(c);
    }
}
```

**PSYNC流程**：从节点请求部分重同步，主节点处理PSYNC请求。

### 6.2 发送积压缓冲区

```c
// 发送积压缓冲区数据
void sendReplicationBacklog(client *c, long long offset) {
    long long skip = offset - server.repl_backlog_off;
    long long j;

    serverLog(LL_NOTICE, "Partial resynchronization request from %s accepted. "
             "Sending %lld bytes of backlog starting from offset %lld.",
             c->slave_ip, server.repl_backlog_histlen - skip, offset);

    // 1. 计算起始位置
    j = (server.repl_backlog_idx +
         (server.repl_backlog_size - server.repl_backlog_histlen + skip)) %
        server.repl_backlog_size;

    // 2. 发送数据
    while (j != server.repl_backlog_idx) {
        size_t len;

        if (server.repl_backlog[j] == '\n') {
            j++;
            if (j == server.repl_backlog_size) j = 0;
            continue;
        }

        len = server.repl_backlog_size - j;
        if (j + len > server.repl_backlog_idx)
            len = server.repl_backlog_idx - j;

        addReply(c, server.repl_backlog + j, len);
        j += len;
        if (j == server.repl_backlog_size) j = 0;
    }

    // 3. 更新状态
    c->replstate = SLAVE_STATE_ONLINE;
    c->repl_ack_off = server.master_repl_offset;
}
```

**发送积压缓冲区**：发送积压缓冲区数据，更新状态。

## 7. 数据流延迟分解

### 7.1 各阶段延迟

```text
全量同步延迟：
- RDB生成：100-1000ms（取决于数据量）
- RDB传输：100-1000ms（取决于网络和文件大小）
- RDB加载：100-1000ms（取决于数据量）
- 总延迟：300-3000ms

增量同步延迟：
- 命令传播：<1ms（本地）或10-100ms（远程）
- 命令接收：<1ms
- 命令执行：0.01-10ms（取决于命令复杂度）
- 总延迟：<1ms（本地）或10-100ms（远程）
```

**各阶段延迟**：全量同步延迟（RDB生成、RDB传输、RDB加载），增量同步延迟（命令传播、命令接收、命令执行）。

**定理 7.1**：增量同步的总延迟约为$T_{network} + T_{exec}$，其中$T_{network}$为网络延迟，$T_{exec}$为命令执行延迟。

**证明**：

- 命令传播：$T_{network}$（网络延迟）
- 命令接收：$T_{network}$（网络延迟）
- 命令执行：$T_{exec}$（命令执行延迟）
- 总延迟：$T_{total} \approx T_{network} + T_{exec}$

## 8. 性能优化

### 8.1 无盘复制

```c
// 无盘复制：直接通过网络传输，不写磁盘
int rdbSaveToSlavesSockets(void) {
    // 直接通过网络发送RDB数据
    // 减少磁盘IO
}
```

**无盘复制**：直接通过网络传输，不写磁盘，减少磁盘IO。

### 8.2 并行复制

```c
// 并行复制：多个从节点并行接收RDB
// Redis 4.0+支持
```

**并行复制**：多个从节点并行接收RDB，提升复制效率。

### 8.3 压缩传输

```c
// RDB压缩：减少网络传输量
// 默认开启LZF压缩
```

**压缩传输**：RDB压缩，减少网络传输量，默认开启LZF压缩。

## 9. 扩展阅读

- [主从复制机制](../../03-Redis组件/03.03-高可用架构/03.03.01-主从复制机制.md)
- [持久化数据流](./05.04.02-持久化数据流.md)
- [请求-响应数据流](./05.04.01-请求-响应数据流.md)

## 10. 权威参考

### 10.1 经典书籍

1. **《Redis设计与实现（第2版）》** - 黄健宏
   - 出版社: 机械工业出版社
   - ISBN: 978-7111544937
   - 第15章：复制（复制数据流详解）

2. **《Redis深度历险：核心原理与应用实践》** - 钱文品
   - 出版社: 电子工业出版社
   - ISBN: 978-7121355952
   - 第4章：高可用（复制数据流详解）

### 10.2 官方文档

1. **Redis源码 - replication.c**
   - URL: <https://github.com/redis/redis/blob/unstable/src/replication.c>
   - Redis的复制实现源码

2. **Redis官方文档 - Replication**
   - URL: <https://redis.io/docs/manual/replication/>
   - Redis复制的官方文档

### 10.3 在线资源

1. **Wikipedia - Master-Slave Replication**
   - URL: <https://en.wikipedia.org/wiki/Master%E2%80%93slave_replication>
   - 提供主从复制的详细说明
