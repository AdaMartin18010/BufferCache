# 05.04.03 复制数据流

## 概述

本文档深入分析Redis主从复制的完整数据流，包括全量同步、增量同步、命令传播等场景的数据流转路径。

## 复制数据流概览

### 完整流程

```
主节点 → 命令执行 → 复制积压缓冲区 → 从节点接收 → 命令重放 → 数据同步
```

### 关键路径

```
主节点命令 → 复制积压缓冲区 → 从节点Socket → 从节点命令执行 → 数据更新
```

## 全量同步数据流

### RDB传输流程

```c
// 主节点：生成RDB并发送
void replicationFeedSlaves(list *slaves, int dictid,
                          robj **argv, int argc) {
    listIter li;
    listNode *ln;
    client *slave;

    if (server.masterhost != NULL) return;
    if (slaves == NULL) return;

    listRewind(slaves, &li);
    while((ln = listNext(&li))) {
        slave = ln->value;

        // 1. 检查是否需要全量同步
        if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) {
            // 启动后台RDB生成
            if (server.rdb_child_pid == -1) {
                startBgsaveForReplication(slave);
            }
            continue;
        }

        if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_END) {
            // 等待RDB生成完成
            continue;
        }

        // 2. 增量同步：发送命令
        addReplyMultiBulkLen(slave, argc);
        for (int j = 0; j < argc; j++) {
            addReplyBulk(slave, argv[j]);
        }
    }
}

// 启动后台RDB生成
int startBgsaveForReplication(client *slave) {
    char filename[256];
    int fd;

    // 1. 创建RDB文件
    snprintf(filename, 256, "temp-repl-%d.rdb", (int)getpid());
    fd = open(filename, O_CREAT | O_WRONLY | O_TRUNC, 0644);
    if (fd == -1) {
        serverLog(LL_WARNING, "Opening temp file for replication: %s", strerror(errno));
        return C_ERR;
    }

    // 2. fork子进程
    pid_t pid = fork();
    if (pid == 0) {
        // 子进程：生成RDB
        close(server.fd);
        rdbSave(filename);
        exit(0);
    } else {
        // 父进程：记录状态
        server.rdb_child_pid = pid;
        server.rdb_child_type = RDB_CHILD_TYPE_DISK;
        server.rdb_save_time_start = time(NULL);

        // 3. 注册文件事件：RDB生成完成后发送
        aeCreateFileEvent(server.el, server.repl_transfer_fd,
                         AE_WRITABLE, sendBulkToSlave, slave);
        return C_OK;
    }
}
```

### RDB传输

```c
// 发送RDB文件到从节点
void sendBulkToSlave(aeEventLoop *el, int fd, void *privdata, int mask) {
    client *slave = privdata;
    UNUSED(el);
    UNUSED(mask);
    char buf[PROTO_IOBUF_LEN];
    ssize_t nwritten, buflen;

    // 1. 读取RDB文件
    buflen = read(server.repl_transfer_fd, buf, PROTO_IOBUF_LEN);
    if (buflen <= 0) {
        serverLog(LL_WARNING, "Read error sending DB to slave: %s",
                 (buflen == 0) ? "premature EOF" : strerror(errno));
        freeClient(slave);
        return;
    }

    // 2. 发送到从节点
    if ((nwritten = write(fd, buf, buflen)) == -1) {
        if (errno != EAGAIN) {
            serverLog(LL_WARNING, "Write error sending DB to slave: %s",
                     strerror(errno));
            freeClient(slave);
            return;
        }
        return;
    }

    server.repl_transfer_read += nwritten;

    // 3. 检查是否传输完成
    if (server.repl_transfer_read == server.repl_transfer_size) {
        close(server.repl_transfer_fd);
        server.repl_transfer_fd = -1;
        aeDeleteFileEvent(server.el, fd, AE_WRITABLE);
        putSlaveOnline(slave);
    }
}
```

## 增量同步数据流

### 命令传播

```c
// 主节点：传播命令到从节点
void replicationFeedSlaves(list *slaves, int dictid,
                          robj **argv, int argc) {
    listIter li;
    listNode *ln;
    client *slave;

    if (server.masterhost != NULL) return;
    if (slaves == NULL) return;

    listRewind(slaves, &li);
    while((ln = listNext(&li))) {
        slave = ln->value;

        // 1. 跳过需要全量同步的从节点
        if (slave->replstate != SLAVE_STATE_ONLINE) continue;

        // 2. 写入复制积压缓冲区
        if (server.repl_backlog) {
            feedReplicationBacklog(argv, argc);
        }

        // 3. 发送命令到从节点
        addReplyMultiBulkLen(slave, argc);
        for (int j = 0; j < argc; j++) {
            addReplyBulk(slave, argv[j]);
        }
    }
}
```

### 复制积压缓冲区

```c
// 写入复制积压缓冲区
void feedReplicationBacklog(void *ptr, size_t len) {
    unsigned char *p = ptr;

    server.master_repl_offset += len;

    while(len) {
        size_t thislen = server.repl_backlog_size - server.repl_backlog_idx;
        if (thislen > len) thislen = len;

        // 写入缓冲区
        memcpy(server.repl_backlog + server.repl_backlog_idx, p, thislen);
        server.repl_backlog_idx += thislen;
        if (server.repl_backlog_idx == server.repl_backlog_size)
            server.repl_backlog_idx = 0;

        server.repl_backlog_histlen += thislen;
        len -= thislen;
        p += thislen;
    }

    // 更新偏移量
    if (server.repl_backlog_histlen > server.repl_backlog_size)
        server.repl_backlog_histlen = server.repl_backlog_size;

    server.repl_backlog_off = server.master_repl_offset -
                              server.repl_backlog_histlen + 1;
}
```

## 从节点接收数据流

### 命令接收

```c
// 从节点：接收主节点命令
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;

    // 1. 读取数据
    nread = connRead(c->conn, c->querybuf + sdslen(c->querybuf), readlen);

    if (nread == -1) {
        if (errno == EAGAIN) {
            return;
        } else {
            serverLog(LL_VERBOSE, "Reading from client: %s", strerror(errno));
            freeClientAsync(c);
            return;
        }
    } else if (nread == 0) {
        serverLog(LL_VERBOSE, "Client closed connection");
        freeClientAsync(c);
        return;
    }

    // 2. 更新查询缓冲区
    sdsIncrLen(c->querybuf, nread);
    c->lastinteraction = server.unixtime;

    // 3. 处理查询缓冲区
    processInputBuffer(c);
}
```

### 命令执行

```c
// 从节点：执行主节点命令
void processInputBuffer(client *c) {
    server.current_client = c;

    while (c->qb_pos < sdslen(c->querybuf)) {
        // 1. 解析命令
        if (c->reqtype == PROTO_REQ_INLINE) {
            if (processInlineBuffer(c) != C_OK) break;
        } else if (c->reqtype == PROTO_REQ_MULTIBULK) {
            if (processMultibulkBuffer(c) != C_OK) break;
        } else {
            serverPanic("Unknown request type");
        }

        // 2. 执行命令（从节点模式）
        if (c->argc == 0) {
            resetClient(c);
        } else {
            // 从节点执行命令（不传播）
            c->flags |= CLIENT_SLAVE;
            call(c, CMD_CALL_NONE);
            c->flags &= ~CLIENT_SLAVE;
        }
    }

    server.current_client = NULL;
}
```

## 部分重同步数据流

### PSYNC流程

```c
// 从节点：请求部分重同步
void replicationSendAck(void) {
    client *c = server.master;

    if (c == NULL) return;

    // 发送REPLCONF ACK
    if (c->flags & CLIENT_MASTER) {
        addReplyMultiBulkLen(c, 3);
        addReplyBulkCString(c, "REPLCONF");
        addReplyBulkCString(c, "ACK");
        addReplyBulkLongLong(c, c->reploff);
    }
}

// 主节点：处理PSYNC请求
void replicationReceiveAck(client *c, long long offset) {
    if (server.masterhost != NULL) return;

    // 1. 检查偏移量是否在积压缓冲区中
    if (server.repl_backlog &&
        offset >= server.repl_backlog_off &&
        offset < server.repl_backlog_off + server.repl_backlog_histlen) {

        // 2. 可以部分重同步
        c->replstate = SLAVE_STATE_ONLINE;
        c->repl_ack_off = offset;
        c->repl_ack_time = server.unixtime;

        // 3. 发送积压缓冲区中的数据
        sendReplicationBacklog(c, offset);
    } else {
        // 4. 需要全量同步
        sendBulkToSlave(c);
    }
}
```

### 发送积压缓冲区

```c
// 发送积压缓冲区数据
void sendReplicationBacklog(client *c, long long offset) {
    long long skip = offset - server.repl_backlog_off;
    long long j;

    serverLog(LL_NOTICE, "Partial resynchronization request from %s accepted. "
             "Sending %lld bytes of backlog starting from offset %lld.",
             c->slave_ip, server.repl_backlog_histlen - skip, offset);

    // 1. 计算起始位置
    j = (server.repl_backlog_idx +
         (server.repl_backlog_size - server.repl_backlog_histlen + skip)) %
        server.repl_backlog_size;

    // 2. 发送数据
    while (j != server.repl_backlog_idx) {
        size_t len;

        if (server.repl_backlog[j] == '\n') {
            j++;
            if (j == server.repl_backlog_size) j = 0;
            continue;
        }

        len = server.repl_backlog_size - j;
        if (j + len > server.repl_backlog_idx)
            len = server.repl_backlog_idx - j;

        addReply(c, server.repl_backlog + j, len);
        j += len;
        if (j == server.repl_backlog_size) j = 0;
    }

    // 3. 更新状态
    c->replstate = SLAVE_STATE_ONLINE;
    c->repl_ack_off = server.master_repl_offset;
}
```

## 数据流延迟分解

### 各阶段延迟

```
全量同步延迟：
- RDB生成：100-1000ms（取决于数据量）
- RDB传输：100-1000ms（取决于网络和文件大小）
- RDB加载：100-1000ms（取决于数据量）
- 总延迟：300-3000ms

增量同步延迟：
- 命令传播：<1ms（本地）或10-100ms（远程）
- 命令接收：<1ms
- 命令执行：0.01-10ms（取决于命令复杂度）
- 总延迟：<1ms（本地）或10-100ms（远程）
```

## 性能优化

### 1. 无盘复制

```c
// 无盘复制：直接通过网络传输，不写磁盘
int rdbSaveToSlavesSockets(void) {
    // 直接通过网络发送RDB数据
    // 减少磁盘IO
}
```

### 2. 并行复制

```c
// 并行复制：多个从节点并行接收RDB
// Redis 4.0+支持
```

### 3. 压缩传输

```c
// RDB压缩：减少网络传输量
// 默认开启LZF压缩
```

## 扩展阅读

- [主从复制机制](../../03-Redis组件/03.03-高可用架构/03.03.01-主从复制机制.md)
- [持久化数据流](./05.04.02-持久化数据流.md)
- [请求-响应数据流](./05.04.01-请求-响应数据流.md)

## 权威参考

- **Redis源码** - <https://github.com/redis/redis>
- **《Redis设计与实现》** - 黄健宏著
- **Redis官方文档** - <https://redis.io/docs/manual/replication/>
