# 07.03.03 å‘Šè­¦è§„åˆ™ä¸é€šçŸ¥

## ç›®å½•

- [07.03.03 å‘Šè­¦è§„åˆ™ä¸é€šçŸ¥](#070303-å‘Šè­¦è§„åˆ™ä¸é€šçŸ¥)
  - [ç›®å½•](#ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
  - [2. å‘Šè­¦è§„åˆ™è®¾è®¡](#2-å‘Šè­¦è§„åˆ™è®¾è®¡)
    - [2.1 å‘Šè­¦è§„åˆ™åˆ†ç±»](#21-å‘Šè­¦è§„åˆ™åˆ†ç±»)
    - [2.2 å…³é”®å‘Šè­¦è§„åˆ™](#22-å…³é”®å‘Šè­¦è§„åˆ™)
    - [2.3 å‘Šè­¦è§„åˆ™æœ€ä½³å®è·µ](#23-å‘Šè­¦è§„åˆ™æœ€ä½³å®è·µ)
  - [3. Alertmanageré…ç½®](#3-alertmanageré…ç½®)
    - [3.1 Alertmanageréƒ¨ç½²](#31-alertmanageréƒ¨ç½²)
    - [3.2 Alertmanageré…ç½®](#32-alertmanageré…ç½®)
    - [3.3 é«˜å¯ç”¨é…ç½®](#33-é«˜å¯ç”¨é…ç½®)
  - [4. é€šçŸ¥æ¸ é“é…ç½®](#4-é€šçŸ¥æ¸ é“é…ç½®)
    - [4.1 é‚®ä»¶é€šçŸ¥](#41-é‚®ä»¶é€šçŸ¥)
    - [4.2 çŸ­ä¿¡é€šçŸ¥](#42-çŸ­ä¿¡é€šçŸ¥)
    - [4.3 é’‰é’‰/ä¼ä¸šå¾®ä¿¡](#43-é’‰é’‰ä¼ä¸šå¾®ä¿¡)
    - [4.4 Slack/Teams](#44-slackteams)
    - [4.5 Webhooké€šçŸ¥](#45-webhooké€šçŸ¥)
    - [4.6 PagerDuty](#46-pagerduty)
  - [5. å‘Šè­¦è·¯ç”±ä¸åˆ†ç»„](#5-å‘Šè­¦è·¯ç”±ä¸åˆ†ç»„)
    - [5.1 å‘Šè­¦è·¯ç”±](#51-å‘Šè­¦è·¯ç”±)
    - [5.2 å‘Šè­¦åˆ†ç»„](#52-å‘Šè­¦åˆ†ç»„)
    - [5.3 å‘Šè­¦æŠ‘åˆ¶](#53-å‘Šè­¦æŠ‘åˆ¶)
    - [5.4 å‘Šè­¦é™é»˜](#54-å‘Šè­¦é™é»˜)
  - [6. å‘Šè­¦å‡çº§æœºåˆ¶](#6-å‘Šè­¦å‡çº§æœºåˆ¶)
    - [6.1 å‡çº§ç­–ç•¥](#61-å‡çº§ç­–ç•¥)
    - [6.2 å‡çº§æµç¨‹](#62-å‡çº§æµç¨‹)
    - [6.3 å‡çº§é…ç½®](#63-å‡çº§é…ç½®)
  - [7. å‘Šè­¦æ¨¡æ¿](#7-å‘Šè­¦æ¨¡æ¿)
    - [7.1 é‚®ä»¶æ¨¡æ¿](#71-é‚®ä»¶æ¨¡æ¿)
    - [7.2 æ¶ˆæ¯æ¨¡æ¿](#72-æ¶ˆæ¯æ¨¡æ¿)
    - [7.3 è‡ªå®šä¹‰æ¨¡æ¿](#73-è‡ªå®šä¹‰æ¨¡æ¿)
  - [8. å‘Šè­¦æµ‹è¯•ä¸éªŒè¯](#8-å‘Šè­¦æµ‹è¯•ä¸éªŒè¯)
    - [8.1 å‘Šè­¦æµ‹è¯•](#81-å‘Šè­¦æµ‹è¯•)
    - [8.2 å‘Šè­¦éªŒè¯](#82-å‘Šè­¦éªŒè¯)
    - [8.3 å‘Šè­¦æ¼”ç»ƒ](#83-å‘Šè­¦æ¼”ç»ƒ)
  - [9. å‘Šè­¦æœ€ä½³å®è·µ](#9-å‘Šè­¦æœ€ä½³å®è·µ)
    - [9.1 å‘Šè­¦è§„åˆ™è®¾è®¡åŸåˆ™](#91-å‘Šè­¦è§„åˆ™è®¾è®¡åŸåˆ™)
    - [9.2 å‘Šè­¦é€šçŸ¥æœ€ä½³å®è·µ](#92-å‘Šè­¦é€šçŸ¥æœ€ä½³å®è·µ)
    - [9.3 å‘Šè­¦ç®¡ç†æœ€ä½³å®è·µ](#93-å‘Šè­¦ç®¡ç†æœ€ä½³å®è·µ)
  - [10. æ‰©å±•é˜…è¯»](#10-æ‰©å±•é˜…è¯»)
  - [11. æƒå¨å‚è€ƒ](#11-æƒå¨å‚è€ƒ)

---

## 1. æ¦‚è¿°

å‘Šè­¦è§„åˆ™ä¸é€šçŸ¥æ˜¯Redisç›‘æ§ä½“ç³»çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç¡®ä¿åŠæ—¶å‘ç°å’Œå¤„ç†ç³»ç»Ÿå¼‚å¸¸ã€‚

**å‘Šè­¦ç›®æ ‡**ï¼š

- âœ… åŠæ—¶å‘ç°ç³»ç»Ÿå¼‚å¸¸
- âœ… å¿«é€Ÿé€šçŸ¥ç›¸å…³äººå‘˜
- âœ… é¿å…å‘Šè­¦é£æš´
- âœ… æä¾›å¯æ“ä½œçš„å‘Šè­¦ä¿¡æ¯

**å‘Šè­¦èŒƒå›´**ï¼š

- ç³»ç»Ÿçº§å‘Šè­¦ï¼ˆå®ä¾‹å®•æœºã€èµ„æºè€—å°½ï¼‰
- æ€§èƒ½å‘Šè­¦ï¼ˆå»¶è¿Ÿè¿‡é«˜ã€QPSå¼‚å¸¸ï¼‰
- ä¸šåŠ¡å‘Šè­¦ï¼ˆå‘½ä¸­ç‡ä½ã€é”™è¯¯ç‡é«˜ï¼‰
- é›†ç¾¤å‘Šè­¦ï¼ˆèŠ‚ç‚¹æ•…éšœã€æ§½ä½å¼‚å¸¸ï¼‰

---

## 2. å‘Šè­¦è§„åˆ™è®¾è®¡

### 2.1 å‘Šè­¦è§„åˆ™åˆ†ç±»

**å‘Šè­¦ä¸¥é‡ç¨‹åº¦åˆ†ç±»**ï¼š

| çº§åˆ« | æè¿° | å“åº”æ—¶é—´ | é€šçŸ¥æ–¹å¼ |
|------|------|---------|---------|
| **Critical** | ç³»ç»Ÿå®Œå…¨ä¸å¯ç”¨ | <5åˆ†é’Ÿ | ç”µè¯+çŸ­ä¿¡+é‚®ä»¶ |
| **Warning** | æ€§èƒ½ä¸‹é™æˆ–å¼‚å¸¸ | <15åˆ†é’Ÿ | é‚®ä»¶+æ¶ˆæ¯ |
| **Info** | ä¿¡æ¯æ€§å‘Šè­¦ | <1å°æ—¶ | é‚®ä»¶ |

**å‘Šè­¦ç±»å‹åˆ†ç±»**ï¼š

1. **å¯ç”¨æ€§å‘Šè­¦**ï¼šå®ä¾‹å®•æœºã€æœåŠ¡ä¸å¯ç”¨
2. **æ€§èƒ½å‘Šè­¦**ï¼šå»¶è¿Ÿè¿‡é«˜ã€QPSå¼‚å¸¸
3. **èµ„æºå‘Šè­¦**ï¼šå†…å­˜ã€CPUã€ç½‘ç»œä½¿ç”¨ç‡è¿‡é«˜
4. **ä¸šåŠ¡å‘Šè­¦**ï¼šå‘½ä¸­ç‡ä½ã€é”™è¯¯ç‡é«˜
5. **é›†ç¾¤å‘Šè­¦**ï¼šèŠ‚ç‚¹æ•…éšœã€æ§½ä½å¼‚å¸¸

### 2.2 å…³é”®å‘Šè­¦è§„åˆ™

**Prometheuså‘Šè­¦è§„åˆ™æ–‡ä»¶**ï¼š

```yaml
# alerts.yml
groups:
  - name: redis_availability
    interval: 30s
    rules:
      # Rediså®ä¾‹å®•æœº
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          team: platform
          component: redis
        annotations:
          summary: "Redis instance {{ $labels.instance }} is down"
          description: |
            Redis instance {{ $labels.instance }} has been down for more than 1 minute.
            Cluster: {{ $labels.cluster }}
            Role: {{ $labels.role }}

      # Redisè¿æ¥å¤±è´¥
      - alert: RedisConnectionFailed
        expr: redis_exporter_up == 0
        for: 2m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Redis exporter connection failed"
          description: "Cannot connect to Redis instance {{ $labels.instance }}"

  - name: redis_performance
    interval: 30s
    rules:
      # QPSè¿‡é«˜
      - alert: RedisHighQPS
        expr: |
          sum(rate(redis_commands_processed_total[5m])) by (instance) > 100000
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High QPS on {{ $labels.instance }}"
          description: |
            Redis instance {{ $labels.instance }} has QPS of {{ $value }}.
            This is above the threshold of 100,000 QPS.

      # å»¶è¿Ÿè¿‡é«˜
      - alert: RedisHighLatency
        expr: |
          histogram_quantile(0.99, rate(redis_latency_seconds_bucket[5m])) > 0.01
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High latency on {{ $labels.instance }}"
          description: |
            P99 latency on {{ $labels.instance }} is {{ $value }}s.
            This is above the threshold of 10ms.

      # å‘½ä¸­ç‡è¿‡ä½
      - alert: RedisLowHitRate
        expr: |
          (
            sum(rate(redis_keyspace_hits_total[5m])) by (instance) /
            (sum(rate(redis_keyspace_hits_total[5m])) by (instance) +
             sum(rate(redis_keyspace_misses_total[5m])) by (instance))
          ) < 0.8
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Low hit rate on {{ $labels.instance }}"
          description: |
            Cache hit rate on {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            This is below the threshold of 80%.

  - name: redis_resources
    interval: 30s
    rules:
      # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
      - alert: RedisHighMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: |
            Memory usage on {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            Used: {{ $value | humanize1024 }}GB
            Max: {{ $value | humanize1024 }}GB

      # å†…å­˜ä½¿ç”¨ç‡ä¸´ç•Œ
      - alert: RedisCriticalMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) > 0.95
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: |
            Memory usage on {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            Immediate action required!

      # è¿æ¥æ•°è¿‡å¤š
      - alert: RedisTooManyConnections
        expr: |
          redis_connected_clients > 10000
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Too many connections on {{ $labels.instance }}"
          description: |
            Redis instance {{ $labels.instance }} has {{ $value }} connections.
            This is above the threshold of 10,000.

  - name: redis_cluster
    interval: 30s
    rules:
      # é›†ç¾¤æ§½ä½å¼‚å¸¸
      - alert: RedisClusterSlotIssue
        expr: |
          redis_cluster_slots_fail > 0
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Redis cluster slot issue"
          description: |
            Redis cluster has {{ $value }} failed slots.
            Cluster: {{ $labels.cluster }}

      # é›†ç¾¤èŠ‚ç‚¹æ•…éšœ
      - alert: RedisClusterNodeDown
        expr: |
          count(redis_up == 0) by (cluster) > 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Redis cluster node down"
          description: |
            {{ $value }} node(s) down in cluster {{ $labels.cluster }}.

      # å¤åˆ¶å»¶è¿Ÿè¿‡é«˜
      - alert: RedisHighReplicationLag
        expr: |
          (redis_master_repl_offset - redis_replication_offset) > 1000000
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High replication lag on {{ $labels.instance }}"
          description: |
            Replication lag on {{ $labels.instance }} is {{ $value }} bytes.
            This is above the threshold of 1MB.

  - name: redis_business
    interval: 30s
    rules:
      # é”™è¯¯ç‡è¿‡é«˜
      - alert: RedisHighErrorRate
        expr: |
          sum(rate(redis_errors_total[5m])) by (instance) /
          sum(rate(redis_commands_processed_total[5m])) by (instance) > 0.01
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High error rate on {{ $labels.instance }}"
          description: |
            Error rate on {{ $labels.instance }} is {{ $value | humanizePercentage }}.
            This is above the threshold of 1%.

      # æ…¢æŸ¥è¯¢è¿‡å¤š
      - alert: RedisTooManySlowQueries
        expr: |
          sum(rate(redis_slowlog_length[5m])) by (instance) > 100
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Too many slow queries on {{ $labels.instance }}"
          description: |
            {{ $value }} slow queries per second on {{ $labels.instance }}.
            This is above the threshold of 100.
```

### 2.3 å‘Šè­¦è§„åˆ™æœ€ä½³å®è·µ

**å‘Šè­¦è§„åˆ™è®¾è®¡åŸåˆ™**ï¼š

1. **é¿å…å‘Šè­¦é£æš´**ï¼šåˆç†è®¾ç½®`for`æŒç»­æ—¶é—´
2. **é¿å…é‡å¤å‘Šè­¦**ï¼šä½¿ç”¨å‘Šè­¦æŠ‘åˆ¶å’Œåˆ†ç»„
3. **æä¾›å¯æ“ä½œä¿¡æ¯**ï¼šå‘Šè­¦æè¿°åŒ…å«å¤„ç†å»ºè®®
4. **åˆ†çº§å‘Šè­¦**ï¼šæ ¹æ®ä¸¥é‡ç¨‹åº¦è®¾ç½®ä¸åŒå“åº”æ—¶é—´

**å‘Šè­¦è§„åˆ™ä¼˜åŒ–ç¤ºä¾‹**ï¼š

```yaml
# ä¼˜åŒ–å‰ï¼šå¯èƒ½äº§ç”Ÿå‘Šè­¦é£æš´
- alert: RedisHighMemoryUsage
  expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
  for: 0s  # ç«‹å³å‘Šè­¦

# ä¼˜åŒ–åï¼šé¿å…å‘Šè­¦é£æš´
- alert: RedisHighMemoryUsage
  expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
  for: 5m  # æŒç»­5åˆ†é’Ÿæ‰å‘Šè­¦
  labels:
    severity: warning
  annotations:
    summary: "High memory usage"
    runbook_url: "https://wiki.example.com/redis-memory-alert"
```

---

## 3. Alertmanageré…ç½®

### 3.1 Alertmanageréƒ¨ç½²

**Alertmanager Dockeréƒ¨ç½²**ï¼š

```yaml
# docker-compose.yml
version: '3.8'
services:
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    networks:
      - monitoring
    restart: unless-stopped

volumes:
  alertmanager-data:

networks:
  monitoring:
    driver: bridge
```

**Alertmanager Kuberneteséƒ¨ç½²**ï¼š

```yaml
# alertmanager-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.26.0
        ports:
        - containerPort: 9093
          name: web
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--cluster.listen-address=0.0.0.0:9094'
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: storage
          mountPath: /alertmanager
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: storage
        emptyDir: {}
```

### 3.2 Alertmanageré…ç½®

**Alertmanageré…ç½®æ–‡ä»¶**ï¼š

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alertmanager@example.com'
  smtp_auth_password: 'password'

# å‘Šè­¦è·¯ç”±
route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'cluster', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h

  routes:
    # Criticalå‘Šè­¦è·¯ç”±
    - match:
        severity: critical
      receiver: 'critical-receiver'
      group_wait: 5s
      group_interval: 5s
      repeat_interval: 1h
      continue: true

    # Warningå‘Šè­¦è·¯ç”±
    - match:
        severity: warning
      receiver: 'warning-receiver'
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 6h

    # æŒ‰å›¢é˜Ÿè·¯ç”±
    - match:
        team: platform
      receiver: 'platform-receiver'

    # æŒ‰é›†ç¾¤è·¯ç”±
    - match_re:
        cluster: '.*-prod-.*'
      receiver: 'production-receiver'

# å‘Šè­¦æŠ‘åˆ¶è§„åˆ™
inhibit_rules:
  # å¦‚æœRedisDownå‘Šè­¦ï¼ŒæŠ‘åˆ¶å…¶ä»–Redisç›¸å…³å‘Šè­¦
  - source_match:
      alertname: 'RedisDown'
    target_match:
      alertname: 'RedisHighMemoryUsage'
    equal: ['instance']

  # å¦‚æœé›†ç¾¤çº§åˆ«å‘Šè­¦ï¼ŒæŠ‘åˆ¶å®ä¾‹çº§åˆ«å‘Šè­¦
  - source_match:
      alertname: 'RedisClusterDown'
    target_match_re:
      alertname: 'Redis.*'
    equal: ['cluster']

# æ¥æ”¶å™¨é…ç½®
receivers:
  - name: 'default-receiver'
    email_configs:
      - to: 'admin@example.com'
        headers:
          Subject: 'Redis Alert: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'

  - name: 'critical-receiver'
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.critical.html" . }}'
    webhook_configs:
      - url: 'http://pagerduty-webhook:8080'
        send_resolved: true
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts-critical'
        title: 'ğŸš¨ CRITICAL Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'warning-receiver'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.warning.html" . }}'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts-warning'
        title: 'âš ï¸ Warning Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'platform-receiver'
    email_configs:
      - to: 'platform-team@example.com'
        headers:
          Subject: 'Redis Alert: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'

  - name: 'production-receiver'
    email_configs:
      - to: 'production-oncall@example.com'
        headers:
          Subject: 'ğŸš¨ PRODUCTION Alert: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.critical.html" . }}'
    webhook_configs:
      - url: 'http://pagerduty-webhook:8080'
        send_resolved: true

# æ¨¡æ¿é…ç½®
templates:
  - '/etc/alertmanager/templates/*.tmpl'
```

### 3.3 é«˜å¯ç”¨é…ç½®

**Alertmanageré«˜å¯ç”¨é…ç½®**ï¼š

```yaml
# alertmanager-ha.yml
version: '3.8'
services:
  alertmanager-1:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager-1
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-1-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--cluster.listen-address=0.0.0.0:9094'
      - '--cluster.peer=alertmanager-2:9094'
    networks:
      - monitoring

  alertmanager-2:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager-2
    ports:
      - "9094:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-2-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--cluster.listen-address=0.0.0.0:9094'
      - '--cluster.peer=alertmanager-1:9094'
    networks:
      - monitoring

volumes:
  alertmanager-1-data:
  alertmanager-2-data:

networks:
  monitoring:
    driver: bridge
```

---

## 4. é€šçŸ¥æ¸ é“é…ç½®

### 4.1 é‚®ä»¶é€šçŸ¥

**é‚®ä»¶é€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'email-receiver'
    email_configs:
      - to: 'admin@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'password'
        headers:
          Subject: 'Redis Alert: {{ .GroupLabels.alertname }}'
          From: 'Alertmanager <alertmanager@example.com>'
        html: |
          <h2>Redis Alert</h2>
          <p><strong>Alert:</strong> {{ .GroupLabels.alertname }}</p>
          <p><strong>Instance:</strong> {{ .GroupLabels.instance }}</p>
          <p><strong>Severity:</strong> {{ .GroupLabels.severity }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Time:</strong> {{ .StartsAt }}</p>
        send_resolved: true
```

**é‚®ä»¶æ¨¡æ¿**ï¼š

```html
<!-- email.default.html -->
<!DOCTYPE html>
<html>
<head>
    <style>
        body { font-family: Arial, sans-serif; }
        .alert { padding: 10px; margin: 10px 0; border-radius: 5px; }
        .critical { background-color: #ffcccc; }
        .warning { background-color: #fff4cc; }
        .info { background-color: #cce5ff; }
    </style>
</head>
<body>
    <h2>Redis Alert Notification</h2>
    {{ range .Alerts }}
    <div class="alert {{ .Labels.severity }}">
        <h3>{{ .Labels.alertname }}</h3>
        <p><strong>Instance:</strong> {{ .Labels.instance }}</p>
        <p><strong>Cluster:</strong> {{ .Labels.cluster }}</p>
        <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
        <p><strong>Description:</strong> {{ .Annotations.description }}</p>
        <p><strong>Started:</strong> {{ .StartsAt }}</p>
        {{ if .EndsAt }}
        <p><strong>Resolved:</strong> {{ .EndsAt }}</p>
        {{ end }}
    </div>
    {{ end }}
</body>
</html>
```

### 4.2 çŸ­ä¿¡é€šçŸ¥

**çŸ­ä¿¡é€šçŸ¥é…ç½®ï¼ˆé€šè¿‡Webhookï¼‰**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'sms-receiver'
    webhook_configs:
      - url: 'http://sms-gateway:8080/send'
        http_config:
          basic_auth:
            username: 'sms-user'
            password: 'sms-password'
        send_resolved: false
        json:
          to: '{{ .GroupLabels.phone }}'
          message: 'Redis Alert: {{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
```

**çŸ­ä¿¡ç½‘å…³å®ç°**ï¼š

```python
#!/usr/bin/env python3
# sms-gateway.py

from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

@app.route('/send', methods=['POST'])
def send_sms():
    """å‘é€çŸ­ä¿¡"""
    data = request.json

    # è§£æå‘Šè­¦æ•°æ®
    alerts = data.get('alerts', [])

    for alert in alerts:
        labels = alert.get('labels', {})
        annotations = alert.get('annotations', {})

        # æ„å»ºçŸ­ä¿¡å†…å®¹
        message = f"Rediså‘Šè­¦: {labels.get('alertname')}\n"
        message += f"å®ä¾‹: {labels.get('instance')}\n"
        message += f"æè¿°: {annotations.get('description', '')}\n"

        # å‘é€çŸ­ä¿¡ï¼ˆä½¿ç”¨ç¬¬ä¸‰æ–¹çŸ­ä¿¡æœåŠ¡ï¼‰
        phone = labels.get('phone', '')
        send_sms_to_phone(phone, message)

    return jsonify({'status': 'success'})

def send_sms_to_phone(phone, message):
    """å‘é€çŸ­ä¿¡åˆ°æ‰‹æœº"""
    # å®ç°çŸ­ä¿¡å‘é€é€»è¾‘
    # å¯ä»¥ä½¿ç”¨é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰çŸ­ä¿¡æœåŠ¡
    pass

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

### 4.3 é’‰é’‰/ä¼ä¸šå¾®ä¿¡

**é’‰é’‰é€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'dingtalk-receiver'
    webhook_configs:
      - url: 'https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN'
        http_config:
          method: POST
        json:
          msgtype: 'markdown'
          markdown:
            title: 'Rediså‘Šè­¦'
            text: |
              ## Rediså‘Šè­¦é€šçŸ¥

              **å‘Šè­¦åç§°:** {{ .GroupLabels.alertname }}

              **å®ä¾‹:** {{ .GroupLabels.instance }}

              **ä¸¥é‡ç¨‹åº¦:** {{ .GroupLabels.severity }}

              **æè¿°:** {{ .CommonAnnotations.description }}

              **æ—¶é—´:** {{ .StartsAt }}
```

**ä¼ä¸šå¾®ä¿¡é€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'wechat-receiver'
    webhook_configs:
      - url: 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=YOUR_KEY'
        http_config:
          method: POST
        json:
          msgtype: 'markdown'
          markdown:
            content: |
              # Rediså‘Šè­¦é€šçŸ¥

              **å‘Šè­¦åç§°:** {{ .GroupLabels.alertname }}

              **å®ä¾‹:** {{ .GroupLabels.instance }}

              **ä¸¥é‡ç¨‹åº¦:** {{ .GroupLabels.severity }}

              **æè¿°:** {{ .CommonAnnotations.description }}
```

### 4.4 Slack/Teams

**Slacké€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'slack-receiver'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts'
        title: 'Redis Alert'
        text: |
          *Alert:* {{ .GroupLabels.alertname }}
          *Instance:* {{ .GroupLabels.instance }}
          *Severity:* {{ .GroupLabels.severity }}
          *Description:* {{ .CommonAnnotations.description }}
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

**Microsoft Teamsé€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'teams-receiver'
    webhook_configs:
      - url: 'https://outlook.office.com/webhook/YOUR_WEBHOOK_URL'
        http_config:
          method: POST
        json:
          '@type': 'MessageCard'
          '@context': 'https://schema.org/extensions'
          summary: 'Redis Alert'
          themeColor: '{{ if eq .Status "firing" }}FF0000{{ else }}00FF00{{ end }}'
          title: 'Redis Alert: {{ .GroupLabels.alertname }}'
          sections:
            - activityTitle: 'Redis Alert Notification'
              facts:
                - name: 'Instance'
                  value: '{{ .GroupLabels.instance }}'
                - name: 'Severity'
                  value: '{{ .GroupLabels.severity }}'
                - name: 'Description'
                  value: '{{ .CommonAnnotations.description }}'
```

### 4.5 Webhooké€šçŸ¥

**Webhooké€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'webhook-receiver'
    webhook_configs:
      - url: 'http://webhook-handler:8080/alerts'
        http_config:
          method: POST
          basic_auth:
            username: 'webhook-user'
            password: 'webhook-password'
        send_resolved: true
        max_alerts: 100
```

**Webhookå¤„ç†å™¨å®ç°**ï¼š

```python
#!/usr/bin/env python3
# webhook-handler.py

from flask import Flask, request, jsonify
import logging

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

@app.route('/alerts', methods=['POST'])
def handle_alerts():
    """å¤„ç†å‘Šè­¦"""
    data = request.json

    # è§£æå‘Šè­¦æ•°æ®
    alerts = data.get('alerts', [])
    status = data.get('status', '')

    for alert in alerts:
        labels = alert.get('labels', {})
        annotations = alert.get('annotations', {})

        # è®°å½•å‘Šè­¦
        logging.info(f"Alert: {labels.get('alertname')}, Status: {status}")

        # å¤„ç†å‘Šè­¦é€»è¾‘
        handle_alert(labels, annotations, status)

    return jsonify({'status': 'success'})

def handle_alert(labels, annotations, status):
    """å¤„ç†å•ä¸ªå‘Šè­¦"""
    alertname = labels.get('alertname')
    instance = labels.get('instance')
    severity = labels.get('severity')

    if status == 'firing':
        # å‘Šè­¦è§¦å‘
        logging.info(f"Alert firing: {alertname} on {instance}")
        # æ‰§è¡Œå‘Šè­¦å¤„ç†é€»è¾‘
    elif status == 'resolved':
        # å‘Šè­¦æ¢å¤
        logging.info(f"Alert resolved: {alertname} on {instance}")
        # æ‰§è¡Œæ¢å¤å¤„ç†é€»è¾‘

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

### 4.6 PagerDuty

**PagerDutyé€šçŸ¥é…ç½®**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'pagerduty-receiver'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
        details:
          alertname: '{{ .GroupLabels.alertname }}'
          instance: '{{ .GroupLabels.instance }}'
          severity: '{{ .GroupLabels.severity }}'
          description: '{{ .CommonAnnotations.description }}'
        severity: '{{ if eq .GroupLabels.severity "critical" }}critical{{ else }}warning{{ end }}'
        send_resolved: true
```

---

## 5. å‘Šè­¦è·¯ç”±ä¸åˆ†ç»„

### 5.1 å‘Šè­¦è·¯ç”±

**å‘Šè­¦è·¯ç”±é…ç½®**ï¼š

```yaml
# alertmanager.yml
route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'cluster']

  routes:
    # æŒ‰ä¸¥é‡ç¨‹åº¦è·¯ç”±
    - match:
        severity: critical
      receiver: 'critical-receiver'
      continue: false

    - match:
        severity: warning
      receiver: 'warning-receiver'
      continue: false

    # æŒ‰å›¢é˜Ÿè·¯ç”±
    - match:
        team: platform
      receiver: 'platform-receiver'

    # æŒ‰é›†ç¾¤è·¯ç”±
    - match_re:
        cluster: '.*-prod-.*'
      receiver: 'production-receiver'

    # æŒ‰å‘Šè­¦åç§°è·¯ç”±
    - match:
        alertname: RedisDown
      receiver: 'oncall-receiver'
```

### 5.2 å‘Šè­¦åˆ†ç»„

**å‘Šè­¦åˆ†ç»„é…ç½®**ï¼š

```yaml
# alertmanager.yml
route:
  receiver: 'default-receiver'

  # åˆ†ç»„é…ç½®
  group_by: ['alertname', 'cluster', 'severity']
  group_wait: 10s      # ç­‰å¾…10ç§’æ”¶é›†åŒç»„å‘Šè­¦
  group_interval: 10s  # åŒç»„å‘Šè­¦é—´éš”10ç§’
  repeat_interval: 12h # é‡å¤å‘Šè­¦é—´éš”12å°æ—¶

  routes:
    - match:
        severity: critical
      group_by: ['alertname', 'cluster']
      group_wait: 5s      # Criticalå‘Šè­¦å¿«é€Ÿåˆ†ç»„
      group_interval: 5s
      repeat_interval: 1h # Criticalå‘Šè­¦1å°æ—¶é‡å¤
      receiver: 'critical-receiver'
```

### 5.3 å‘Šè­¦æŠ‘åˆ¶

**å‘Šè­¦æŠ‘åˆ¶é…ç½®**ï¼š

```yaml
# alertmanager.yml
inhibit_rules:
  # å¦‚æœRedisDownå‘Šè­¦ï¼ŒæŠ‘åˆ¶å…¶ä»–Redisç›¸å…³å‘Šè­¦
  - source_match:
      alertname: 'RedisDown'
    target_match:
      alertname: 'RedisHighMemoryUsage'
    equal: ['instance']

  # å¦‚æœé›†ç¾¤çº§åˆ«å‘Šè­¦ï¼ŒæŠ‘åˆ¶å®ä¾‹çº§åˆ«å‘Šè­¦
  - source_match:
      alertname: 'RedisClusterDown'
    target_match_re:
      alertname: 'Redis.*'
    equal: ['cluster']

  # å¦‚æœCriticalå‘Šè­¦ï¼ŒæŠ‘åˆ¶Warningå‘Šè­¦
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['instance', 'alertname']
```

### 5.4 å‘Šè­¦é™é»˜

**å‘Šè­¦é™é»˜é…ç½®ï¼ˆé€šè¿‡APIï¼‰**ï¼š

```bash
#!/bin/bash
# silence-alert.sh

ALERTMANAGER_URL="http://alertmanager:9093"
ALERTNAME="$1"
INSTANCE="$2"
DURATION="${3:-1h}"

# åˆ›å»ºé™é»˜è§„åˆ™
curl -X POST "${ALERTMANAGER_URL}/api/v2/silences" \
  -H "Content-Type: application/json" \
  -d "{
    \"matchers\": [
      {\"name\": \"alertname\", \"value\": \"${ALERTNAME}\"},
      {\"name\": \"instance\", \"value\": \"${INSTANCE}\"}
    ],
    \"startsAt\": \"$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)\",
    \"endsAt\": \"$(date -u -d '+1 hour' +%Y-%m-%dT%H:%M:%S.%3NZ)\",
    \"createdBy\": \"admin\",
    \"comment\": \"Maintenance window\"
  }"
```

**é™é»˜ç®¡ç†è„šæœ¬**ï¼š

```python
#!/usr/bin/env python3
# silence-manager.py

import requests
import json
from datetime import datetime, timedelta

class SilenceManager:
    def __init__(self, alertmanager_url):
        self.alertmanager_url = alertmanager_url

    def create_silence(self, matchers, duration_hours=1, comment=""):
        """åˆ›å»ºé™é»˜è§„åˆ™"""
        starts_at = datetime.utcnow()
        ends_at = starts_at + timedelta(hours=duration_hours)

        silence = {
            "matchers": [
                {"name": name, "value": value, "isRegex": False}
                for name, value in matchers.items()
            ],
            "startsAt": starts_at.isoformat() + "Z",
            "endsAt": ends_at.isoformat() + "Z",
            "createdBy": "admin",
            "comment": comment
        }

        response = requests.post(
            f"{self.alertmanager_url}/api/v2/silences",
            json=silence
        )

        return response.json()

    def list_silences(self):
        """åˆ—å‡ºæ‰€æœ‰é™é»˜è§„åˆ™"""
        response = requests.get(
            f"{self.alertmanager_url}/api/v2/silences"
        )
        return response.json()

    def delete_silence(self, silence_id):
        """åˆ é™¤é™é»˜è§„åˆ™"""
        response = requests.delete(
            f"{self.alertmanager_url}/api/v2/silence/{silence_id}"
        )
        return response.status_code == 200

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    manager = SilenceManager('http://alertmanager:9093')

    # åˆ›å»ºé™é»˜è§„åˆ™
    silence = manager.create_silence(
        matchers={
            'alertname': 'RedisHighMemoryUsage',
            'instance': 'redis-01'
        },
        duration_hours=2,
        comment='Maintenance window'
    )

    print(f"Silence created: {silence['id']}")
```

---

## 6. å‘Šè­¦å‡çº§æœºåˆ¶

### 6.1 å‡çº§ç­–ç•¥

**å‘Šè­¦å‡çº§ç­–ç•¥**ï¼š

```yaml
# alertmanager.yml
route:
  receiver: 'default-receiver'

  routes:
    - match:
        severity: critical
      receiver: 'critical-receiver'
      group_wait: 5s
      group_interval: 5s
      repeat_interval: 1h

      # å‡çº§è·¯ç”±
      routes:
        # å¦‚æœ1å°æ—¶å†…æœªå¤„ç†ï¼Œå‡çº§åˆ°oncall
        - match:
            severity: critical
          receiver: 'oncall-receiver'
          repeat_interval: 30m
          continue: true

        # å¦‚æœ2å°æ—¶å†…æœªå¤„ç†ï¼Œå‡çº§åˆ°manager
        - match:
            severity: critical
          receiver: 'manager-receiver'
          repeat_interval: 1h
```

### 6.2 å‡çº§æµç¨‹

**å‘Šè­¦å‡çº§æµç¨‹**ï¼š

```
å‘Šè­¦è§¦å‘
  â†“
Level 1: é‚®ä»¶é€šçŸ¥ï¼ˆ5åˆ†é’Ÿï¼‰
  â†“ (æœªå¤„ç†)
Level 2: çŸ­ä¿¡é€šçŸ¥ï¼ˆ15åˆ†é’Ÿï¼‰
  â†“ (æœªå¤„ç†)
Level 3: ç”µè¯é€šçŸ¥ï¼ˆ30åˆ†é’Ÿï¼‰
  â†“ (æœªå¤„ç†)
Level 4: å‡çº§åˆ°ä¸Šçº§ï¼ˆ1å°æ—¶ï¼‰
```

### 6.3 å‡çº§é…ç½®

**å‡çº§é…ç½®ç¤ºä¾‹**ï¼š

```yaml
# alertmanager.yml
receivers:
  - name: 'escalation-receiver'
    email_configs:
      - to: 'oncall@example.com'
        send_resolved: false
    webhook_configs:
      - url: 'http://escalation-service:8080/escalate'
        json:
          level: 1
          alert: '{{ .GroupLabels.alertname }}'
          instance: '{{ .GroupLabels.instance }}'
```

---

## 7. å‘Šè­¦æ¨¡æ¿

### 7.1 é‚®ä»¶æ¨¡æ¿

è§4.1èŠ‚ã€‚

### 7.2 æ¶ˆæ¯æ¨¡æ¿

**Slackæ¶ˆæ¯æ¨¡æ¿**ï¼š

```json
{
  "text": "Redis Alert",
  "attachments": [
    {
      "color": "{{ if eq .Status \"firing\" }}danger{{ else }}good{{ end }}",
      "title": "{{ .GroupLabels.alertname }}",
      "fields": [
        {
          "title": "Instance",
          "value": "{{ .GroupLabels.instance }}",
          "short": true
        },
        {
          "title": "Severity",
          "value": "{{ .GroupLabels.severity }}",
          "short": true
        },
        {
          "title": "Description",
          "value": "{{ .CommonAnnotations.description }}",
          "short": false
        }
      ],
      "footer": "Alertmanager",
      "ts": "{{ .StartsAt.Unix }}"
    }
  ]
}
```

### 7.3 è‡ªå®šä¹‰æ¨¡æ¿

**è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶**ï¼š

```go
// templates/custom.tmpl
{{ define "custom.title" }}
Redis Alert: {{ .GroupLabels.alertname }}
{{ end }}

{{ define "custom.description" }}
å‘Šè­¦åç§°: {{ .GroupLabels.alertname }}
å®ä¾‹: {{ .GroupLabels.instance }}
é›†ç¾¤: {{ .GroupLabels.cluster }}
ä¸¥é‡ç¨‹åº¦: {{ .GroupLabels.severity }}
æè¿°: {{ .CommonAnnotations.description }}
æ—¶é—´: {{ .StartsAt }}
{{ end }}
```

---

## 8. å‘Šè­¦æµ‹è¯•ä¸éªŒè¯

### 8.1 å‘Šè­¦æµ‹è¯•

**å‘Šè­¦æµ‹è¯•è„šæœ¬**ï¼š

```bash
#!/bin/bash
# test-alert.sh

ALERTMANAGER_URL="http://alertmanager:9093"

# å‘é€æµ‹è¯•å‘Šè­¦
curl -X POST "${ALERTMANAGER_URL}/api/v1/alerts" \
  -H "Content-Type: application/json" \
  -d '[
    {
      "labels": {
        "alertname": "RedisDown",
        "instance": "redis-test-01",
        "severity": "critical",
        "cluster": "test-cluster"
      },
      "annotations": {
        "summary": "Test alert",
        "description": "This is a test alert"
      },
      "startsAt": "'$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)'"
    }
  ]'
```

### 8.2 å‘Šè­¦éªŒè¯

**å‘Šè­¦éªŒè¯è„šæœ¬**ï¼š

```python
#!/usr/bin/env python3
# verify-alerts.py

import requests
import time

def verify_alert(alertmanager_url, alertname, instance):
    """éªŒè¯å‘Šè­¦"""
    # å‘é€æµ‹è¯•å‘Šè­¦
    test_alert = {
        "labels": {
            "alertname": alertname,
            "instance": instance,
            "severity": "warning"
        },
        "annotations": {
            "summary": "Test alert",
            "description": "Verifying alert system"
        },
        "startsAt": time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime())
    }

    response = requests.post(
        f"{alertmanager_url}/api/v1/alerts",
        json=[test_alert]
    )

    if response.status_code == 200:
        print(f"Test alert sent successfully")
        print("Waiting for notification...")
        time.sleep(10)
        print("Check your notification channels")
    else:
        print(f"Failed to send test alert: {response.status_code}")

if __name__ == '__main__':
    verify_alert('http://alertmanager:9093', 'RedisHighMemoryUsage', 'redis-test-01')
```

### 8.3 å‘Šè­¦æ¼”ç»ƒ

**å‘Šè­¦æ¼”ç»ƒè„šæœ¬**ï¼š

```bash
#!/bin/bash
# alert-drill.sh

echo "Starting alert drill..."

# 1. å‘é€æµ‹è¯•å‘Šè­¦
echo "Sending test alerts..."
./test-alert.sh RedisDown redis-test-01
sleep 5

./test-alert.sh RedisHighMemoryUsage redis-test-01
sleep 5

./test-alert.sh RedisHighQPS redis-test-01

# 2. éªŒè¯å‘Šè­¦æ¥æ”¶
echo "Verifying alert reception..."
# æ£€æŸ¥é‚®ä»¶ã€çŸ­ä¿¡ã€Slackç­‰é€šçŸ¥æ¸ é“

# 3. éªŒè¯å‘Šè­¦å¤„ç†æµç¨‹
echo "Verifying alert handling process..."
# æ£€æŸ¥å‘Šè­¦æ˜¯å¦è¢«æ­£ç¡®å¤„ç†

echo "Alert drill completed"
```

---

## 9. å‘Šè­¦æœ€ä½³å®è·µ

### 9.1 å‘Šè­¦è§„åˆ™è®¾è®¡åŸåˆ™

**è®¾è®¡åŸåˆ™**ï¼š

1. **é¿å…å‘Šè­¦ç–²åŠ³**ï¼šåˆç†è®¾ç½®å‘Šè­¦é˜ˆå€¼å’ŒæŒç»­æ—¶é—´
2. **æä¾›å¯æ“ä½œä¿¡æ¯**ï¼šå‘Šè­¦æè¿°åŒ…å«å¤„ç†å»ºè®®
3. **åˆ†çº§å‘Šè­¦**ï¼šæ ¹æ®ä¸¥é‡ç¨‹åº¦è®¾ç½®ä¸åŒå“åº”æ—¶é—´
4. **é¿å…é‡å¤å‘Šè­¦**ï¼šä½¿ç”¨å‘Šè­¦æŠ‘åˆ¶å’Œåˆ†ç»„

### 9.2 å‘Šè­¦é€šçŸ¥æœ€ä½³å®è·µ

**é€šçŸ¥æœ€ä½³å®è·µ**ï¼š

1. **å¤šæ¸ é“é€šçŸ¥**ï¼šCriticalå‘Šè­¦ä½¿ç”¨å¤šç§é€šçŸ¥æ–¹å¼
2. **å‘Šè­¦èšåˆ**ï¼šç›¸å…³å‘Šè­¦èšåˆå‘é€ï¼Œé¿å…å‘Šè­¦é£æš´
3. **å‘Šè­¦é™é»˜**ï¼šç»´æŠ¤çª—å£æœŸé—´é™é»˜å‘Šè­¦
4. **å‘Šè­¦å‡çº§**ï¼šæœªå¤„ç†å‘Šè­¦è‡ªåŠ¨å‡çº§

### 9.3 å‘Šè­¦ç®¡ç†æœ€ä½³å®è·µ

**ç®¡ç†æœ€ä½³å®è·µ**ï¼š

1. **å®šæœŸå®¡æŸ¥**ï¼šå®šæœŸå®¡æŸ¥å‘Šè­¦è§„åˆ™å’Œé€šçŸ¥é…ç½®
2. **å‘Šè­¦æ¼”ç»ƒ**ï¼šå®šæœŸè¿›è¡Œå‘Šè­¦æ¼”ç»ƒ
3. **æŒç»­ä¼˜åŒ–**ï¼šæ ¹æ®å®é™…æƒ…å†µä¼˜åŒ–å‘Šè­¦è§„åˆ™
4. **æ–‡æ¡£åŒ–**ï¼šè®°å½•å‘Šè­¦å¤„ç†æµç¨‹å’ŒRunbook

---

## 10. æ‰©å±•é˜…è¯»

- [Prometheusç›‘æ§ä½“ç³»](07.03.01-Prometheusç›‘æ§ä½“ç³».md)
- [Grafanaå¯è§†åŒ–](07.03.02-Grafanaå¯è§†åŒ–.md)
- [æ•…éšœå¤„ç†æµç¨‹](../07.04-æ•…éšœå¤„ç†/07.04.01-æ•…éšœåˆ†ç±»ä¸å¤„ç†æµç¨‹.md)

---

## 11. æƒå¨å‚è€ƒ

- [Prometheuså‘Šè­¦](https://prometheus.io/docs/alerting/)
- [Alertmanagerå®˜æ–¹æ–‡æ¡£](https://prometheus.io/docs/alerting/latest/alertmanager/)

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.0
**æœ€åæ›´æ–°**ï¼š2025-01
**æ–‡æ¡£çŠ¶æ€**ï¼šâœ… å®Œæˆï¼ˆå·²å¤§å¹…æ‰©å……å†…å®¹ï¼Œä»68è¡Œæ‰©å……è‡³800+è¡Œï¼‰
