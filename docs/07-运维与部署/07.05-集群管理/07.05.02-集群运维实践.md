# 07.05.02 集群运维实践

## 目录

- [07.05.02 集群运维实践](#070502-集群运维实践)
  - [目录](#目录)
  - [1. 概述](#1-概述)
  - [2. 日常运维操作](#2-日常运维操作)
    - [2.1 集群状态检查](#21-集群状态检查)
      - [2.1.1 集群基本信息检查](#211-集群基本信息检查)
      - [2.1.2 集群健康状态检查](#212-集群健康状态检查)
    - [2.2 节点健康检查](#22-节点健康检查)
      - [2.2.1 节点状态检查](#221-节点状态检查)
      - [2.2.2 节点性能检查](#222-节点性能检查)
    - [2.3 槽位分布检查](#23-槽位分布检查)
    - [2.4 集群拓扑检查](#24-集群拓扑检查)
  - [3. 配置管理](#3-配置管理)
    - [3.1 配置更新](#31-配置更新)
      - [3.1.1 动态配置更新](#311-动态配置更新)
      - [3.1.2 配置文件更新](#312-配置文件更新)
    - [3.2 配置版本管理](#32-配置版本管理)
    - [3.3 批量配置更新](#33-批量配置更新)
    - [3.4 配置回滚](#34-配置回滚)
  - [4. 数据管理](#4-数据管理)
    - [4.1 数据备份](#41-数据备份)
      - [4.1.1 集群全量备份](#411-集群全量备份)
      - [4.1.2 增量备份](#412-增量备份)
    - [4.2 数据恢复](#42-数据恢复)
      - [4.2.1 从RDB恢复](#421-从rdb恢复)
      - [4.2.2 从AOF恢复](#422-从aof恢复)
    - [4.3 数据迁移](#43-数据迁移)
    - [4.4 数据一致性检查](#44-数据一致性检查)
  - [5. 性能监控](#5-性能监控)
    - [5.1 性能指标](#51-性能指标)
      - [5.1.1 关键性能指标](#511-关键性能指标)
    - [5.2 监控工具](#52-监控工具)
      - [5.2.1 Prometheus监控集成](#521-prometheus监控集成)
    - [5.3 性能分析](#53-性能分析)
    - [5.4 性能优化](#54-性能优化)
  - [6. 日常维护任务](#6-日常维护任务)
    - [6.1 定期健康检查](#61-定期健康检查)
    - [6.2 日志管理](#62-日志管理)
    - [6.3 清理任务](#63-清理任务)
    - [6.4 性能调优](#64-性能调优)
  - [7. 大规模集群运维实践](#7-大规模集群运维实践)
    - [7.1 100+节点集群运维](#71-100节点集群运维)
      - [7.1.1 大规模集群特点](#711-大规模集群特点)
      - [7.1.2 运维挑战](#712-运维挑战)
      - [7.1.3 运维策略](#713-运维策略)
    - [7.2 多地域集群运维](#72-多地域集群运维)
    - [7.3 跨云集群运维](#73-跨云集群运维)
  - [8. 运维自动化](#8-运维自动化)
    - [8.1 自动化脚本](#81-自动化脚本)
    - [8.2 运维平台](#82-运维平台)
    - [8.3 CI/CD集成](#83-cicd集成)
  - [9. 运维最佳实践](#9-运维最佳实践)
    - [9.1 运维规范](#91-运维规范)
    - [9.2 运维流程](#92-运维流程)
    - [9.3 运维工具链](#93-运维工具链)
  - [10. 扩展阅读](#10-扩展阅读)
  - [11. 权威参考](#11-权威参考)

---

## 1. 概述

集群运维实践涵盖Redis集群的日常运维操作、配置管理、数据管理、性能监控等核心内容，特别关注大规模集群（100+节点）的运维实践。

**运维目标**：

- ✅ 保证集群高可用性（99.99%+）
- ✅ 保证数据安全性和一致性
- ✅ 优化集群性能和资源利用率
- ✅ 实现运维自动化和标准化

**运维范围**：

- 日常运维操作（检查、监控、维护）
- 配置管理（更新、版本管理、回滚）
- 数据管理（备份、恢复、迁移、一致性检查）
- 性能监控（指标采集、分析、优化）
- 大规模集群运维（100+节点、多地域、跨云）

---

## 2. 日常运维操作

### 2.1 集群状态检查

#### 2.1.1 集群基本信息检查

```bash
#!/bin/bash
# check-cluster-info.sh

CLUSTER_NODE="127.0.0.1:7000"

echo "=== 集群基本信息 ==="
redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER INFO

echo -e "\n=== 集群节点信息 ==="
redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER NODES | \
    awk '{print $1, $2, $3, $8}' | column -t

echo -e "\n=== 集群槽位分布 ==="
redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER SLOTS | \
    head -20
```

#### 2.1.2 集群健康状态检查

```python
#!/usr/bin/env python3
# cluster-health-check.py

import redis
import json
from collections import defaultdict

def check_cluster_health(cluster_nodes):
    """检查集群健康状态"""

    cluster = redis.RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    health_status = {
        'cluster_state': 'ok',
        'nodes': {},
        'issues': []
    }

    # 获取集群信息
    cluster_info = cluster.cluster_info()
    health_status['cluster_state'] = cluster_info.get('cluster_state', 'unknown')

    # 检查每个节点
    nodes = cluster.cluster_nodes()
    for node_id, node_info in nodes.items():
        node_status = {
            'id': node_id,
            'host': node_info['host'],
            'port': node_info['port'],
            'flags': node_info['flags'],
            'master': node_info.get('master'),
            'slots': len(node_info.get('slots', [])),
            'connected': node_info.get('connected', False)
        }

        # 检查节点状态
        if 'fail' in node_info['flags']:
            health_status['issues'].append(f"节点 {node_id} 处于故障状态")
        if not node_info.get('connected', False):
            health_status['issues'].append(f"节点 {node_id} 未连接")

        health_status['nodes'][node_id] = node_status

    return health_status

# 使用示例
if __name__ == '__main__':
    cluster_nodes = [
        {'host': '127.0.0.1', 'port': 7000},
        {'host': '127.0.0.1', 'port': 7001},
        {'host': '127.0.0.1', 'port': 7002}
    ]

    health = check_cluster_health(cluster_nodes)
    print(json.dumps(health, indent=2, ensure_ascii=False))
```

### 2.2 节点健康检查

#### 2.2.1 节点状态检查

```bash
#!/bin/bash
# check-node-health.sh

NODE_HOST="127.0.0.1"
NODE_PORT=7000

echo "=== 节点基本信息 ==="
redis-cli -h $NODE_HOST -p $NODE_PORT INFO server | grep -E "redis_version|os|process_id|uptime"

echo -e "\n=== 节点内存使用 ==="
redis-cli -h $NODE_HOST -p $NODE_PORT INFO memory | grep -E "used_memory|used_memory_human|used_memory_peak|mem_fragmentation_ratio"

echo -e "\n=== 节点连接信息 ==="
redis-cli -h $NODE_HOST -p $NODE_PORT INFO clients | grep -E "connected_clients|blocked_clients"

echo -e "\n=== 节点统计信息 ==="
redis-cli -h $NODE_HOST -p $NODE_PORT INFO stats | grep -E "total_commands_processed|instantaneous_ops_per_sec|keyspace_hits|keyspace_misses"

echo -e "\n=== 节点复制信息 ==="
redis-cli -h $NODE_HOST -p $NODE_PORT INFO replication | grep -E "role|connected_slaves|master_repl_offset"
```

#### 2.2.2 节点性能检查

```python
#!/usr/bin/env python3
# check-node-performance.py

import redis
import time
import statistics

def check_node_performance(host, port, test_count=1000):
    """检查节点性能"""

    r = redis.Redis(host=host, port=port, decode_responses=True)

    # 测试SET操作延迟
    set_latencies = []
    for i in range(test_count):
        start = time.time()
        r.set(f"test:key:{i}", f"value:{i}")
        latency = (time.time() - start) * 1000  # 转换为毫秒
        set_latencies.append(latency)

    # 测试GET操作延迟
    get_latencies = []
    for i in range(test_count):
        start = time.time()
        r.get(f"test:key:{i}")
        latency = (time.time() - start) * 1000
        get_latencies.append(latency)

    # 清理测试数据
    for i in range(test_count):
        r.delete(f"test:key:{i}")

    # 统计结果
    results = {
        'set': {
            'p50': statistics.median(set_latencies),
            'p95': statistics.quantiles(set_latencies, n=20)[18],
            'p99': statistics.quantiles(set_latencies, n=100)[98],
            'avg': statistics.mean(set_latencies),
            'max': max(set_latencies)
        },
        'get': {
            'p50': statistics.median(get_latencies),
            'p95': statistics.quantiles(get_latencies, n=20)[18],
            'p99': statistics.quantiles(get_latencies, n=100)[98],
            'avg': statistics.mean(get_latencies),
            'max': max(get_latencies)
        }
    }

    return results

# 使用示例
if __name__ == '__main__':
    results = check_node_performance('127.0.0.1', 7000)
    print(f"SET操作延迟: P50={results['set']['p50']:.2f}ms, P95={results['set']['p95']:.2f}ms, P99={results['set']['p99']:.2f}ms")
    print(f"GET操作延迟: P50={results['get']['p50']:.2f}ms, P95={results['get']['p95']:.2f}ms, P99={results['get']['p99']:.2f}ms")
```

### 2.3 槽位分布检查

```python
#!/usr/bin/env python3
# check-slot-distribution.py

import redis
from collections import defaultdict

def check_slot_distribution(cluster_nodes):
    """检查槽位分布"""

    cluster = redis.RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    # 获取槽位分布
    slots = cluster.cluster_slots()

    slot_distribution = defaultdict(list)
    for slot_info in slots:
        start_slot = slot_info[0]
        end_slot = slot_info[1]
        master_node = slot_info[2]
        replica_nodes = slot_info[3:]

        node_id = f"{master_node[0]}:{master_node[1]}"
        slot_count = end_slot - start_slot + 1

        slot_distribution[node_id].append({
            'start': start_slot,
            'end': end_slot,
            'count': slot_count,
            'replicas': len(replica_nodes)
        })

    # 统计每个节点的槽位数
    node_slot_counts = {}
    for node_id, slot_list in slot_distribution.items():
        total_slots = sum(s['count'] for s in slot_list)
        node_slot_counts[node_id] = {
            'total_slots': total_slots,
            'slot_ranges': len(slot_list),
            'replicas': slot_list[0]['replicas'] if slot_list else 0
        }

    return node_slot_counts

# 使用示例
if __name__ == '__main__':
    cluster_nodes = [
        {'host': '127.0.0.1', 'port': 7000}
    ]

    distribution = check_slot_distribution(cluster_nodes)
    for node_id, info in distribution.items():
        print(f"节点 {node_id}: {info['total_slots']} 个槽位, {info['slot_ranges']} 个范围, {info['replicas']} 个副本")
```

### 2.4 集群拓扑检查

```bash
#!/bin/bash
# check-cluster-topology.sh

CLUSTER_NODE="127.0.0.1:7000"

echo "=== 集群拓扑结构 ==="
redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER NODES | \
    awk '{
        node_id=$1
        ip_port=$2
        flags=$3
        master=$4
        slots=$9

        if (master == "-") {
            print "主节点:", node_id, ip_port, "槽位:", slots
        } else {
            print "从节点:", node_id, ip_port, "主节点:", master
        }
    }'
```

---

## 3. 配置管理

### 3.1 配置更新

#### 3.1.1 动态配置更新

```python
#!/usr/bin/env python3
# update-cluster-config.py

import redis
from redis.cluster import RedisCluster

def update_cluster_config(cluster_nodes, config_updates):
    """更新集群配置"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    results = {}

    # 获取所有节点
    nodes = cluster.cluster_nodes()

    for node_id, node_info in nodes.items():
        host = node_info['host']
        port = node_info['port']

        try:
            node_client = redis.Redis(
                host=host,
                port=port,
                decode_responses=True
            )

            node_results = {}
            for config_key, config_value in config_updates.items():
                try:
                    # 动态更新配置
                    node_client.config_set(config_key, config_value)

                    # 验证配置
                    actual_value = node_client.config_get(config_key)[config_key]
                    node_results[config_key] = {
                        'status': 'success',
                        'expected': config_value,
                        'actual': actual_value
                    }
                except Exception as e:
                    node_results[config_key] = {
                        'status': 'failed',
                        'error': str(e)
                    }

            results[f"{host}:{port}"] = node_results

        except Exception as e:
            results[f"{host}:{port}"] = {'error': str(e)}

    return results

# 使用示例
if __name__ == '__main__':
    cluster_nodes = [
        {'host': '127.0.0.1', 'port': 7000}
    ]

    config_updates = {
        'maxmemory': '4gb',
        'maxmemory-policy': 'allkeys-lru',
        'timeout': '300'
    }

    results = update_cluster_config(cluster_nodes, config_updates)
    for node, node_results in results.items():
        print(f"节点 {node}:")
        for config_key, config_result in node_results.items():
            print(f"  {config_key}: {config_result}")
```

#### 3.1.2 配置文件更新

```bash
#!/bin/bash
# update-config-files.sh

CLUSTER_NODES=("127.0.0.1:7000" "127.0.0.1:7001" "127.0.0.1:7002")
CONFIG_DIR="/etc/redis/cluster"

# 更新配置文件
for node in "${CLUSTER_NODES[@]}"; do
    host=${node%%:*}
    port=${node##*:}
    config_file="${CONFIG_DIR}/redis-${port}.conf"

    # 备份原配置
    cp "$config_file" "${config_file}.backup.$(date +%Y%m%d_%H%M%S)"

    # 更新配置
    sed -i 's/^maxmemory .*/maxmemory 4gb/' "$config_file"
    sed -i 's/^maxmemory-policy .*/maxmemory-policy allkeys-lru/' "$config_file"

    # 重新加载配置
    redis-cli -h "$host" -p "$port" CONFIG REWRITE
done
```

### 3.2 配置版本管理

```python
#!/usr/bin/env python3
# config-version-manager.py

import json
import git
import os
from datetime import datetime

class ConfigVersionManager:
    """配置版本管理器"""

    def __init__(self, config_repo_path):
        self.repo_path = config_repo_path
        self.repo = git.Repo(config_repo_path)

    def save_config_version(self, node_id, config_dict, description=""):
        """保存配置版本"""

        # 创建配置目录
        config_dir = os.path.join(self.repo_path, 'configs', node_id)
        os.makedirs(config_dir, exist_ok=True)

        # 保存配置
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        config_file = os.path.join(config_dir, f'config_{timestamp}.json')

        config_data = {
            'node_id': node_id,
            'timestamp': timestamp,
            'description': description,
            'config': config_dict
        }

        with open(config_file, 'w') as f:
            json.dump(config_data, f, indent=2)

        # 提交到Git
        self.repo.index.add([config_file])
        self.repo.index.commit(f"Update config for {node_id}: {description}")

    def get_config_history(self, node_id):
        """获取配置历史"""

        config_dir = os.path.join(self.repo_path, 'configs', node_id)
        if not os.path.exists(config_dir):
            return []

        config_files = sorted(os.listdir(config_dir), reverse=True)
        history = []

        for config_file in config_files:
            if config_file.endswith('.json'):
                with open(os.path.join(config_dir, config_file), 'r') as f:
                    history.append(json.load(f))

        return history

    def rollback_config(self, node_id, timestamp):
        """回滚配置到指定版本"""

        config_dir = os.path.join(self.repo_path, 'configs', node_id)
        config_file = os.path.join(config_dir, f'config_{timestamp}.json')

        if not os.path.exists(config_file):
            raise FileNotFoundError(f"Config file not found: {config_file}")

        with open(config_file, 'r') as f:
            config_data = json.load(f)

        return config_data['config']

# 使用示例
if __name__ == '__main__':
    manager = ConfigVersionManager('/path/to/config/repo')

    # 保存配置版本
    config = {
        'maxmemory': '4gb',
        'maxmemory-policy': 'allkeys-lru'
    }
    manager.save_config_version('node-1', config, 'Update memory policy')

    # 获取配置历史
    history = manager.get_config_history('node-1')
    for entry in history:
        print(f"{entry['timestamp']}: {entry['description']}")
```

### 3.3 批量配置更新

```bash
#!/bin/bash
# batch-update-config.sh

CLUSTER_NODES_FILE="cluster-nodes.txt"
CONFIG_UPDATES_FILE="config-updates.txt"

# 读取节点列表
while IFS= read -r node; do
    host=${node%%:*}
    port=${node##*:}

    echo "更新节点 $host:$port 配置..."

    # 读取配置更新
    while IFS='=' read -r key value; do
        redis-cli -h "$host" -p "$port" CONFIG SET "$key" "$value"
    done < "$CONFIG_UPDATES_FILE"

    # 持久化配置
    redis-cli -h "$host" -p "$port" CONFIG REWRITE

done < "$CLUSTER_NODES_FILE"
```

### 3.4 配置回滚

```python
#!/usr/bin/env python3
# rollback-config.py

import redis
from redis.cluster import RedisCluster
import json

def rollback_node_config(cluster_nodes, node_id, config_timestamp):
    """回滚节点配置"""

    # 从版本管理器获取历史配置
    manager = ConfigVersionManager('/path/to/config/repo')
    old_config = manager.rollback_config(node_id, config_timestamp)

    # 连接到集群
    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    # 找到目标节点
    nodes = cluster.cluster_nodes()
    target_node = None
    for nid, node_info in nodes.items():
        if nid == node_id or f"{node_info['host']}:{node_info['port']}" == node_id:
            target_node = node_info
            break

    if not target_node:
        raise ValueError(f"Node {node_id} not found")

    # 连接到节点
    node_client = redis.Redis(
        host=target_node['host'],
        port=target_node['port'],
        decode_responses=True
    )

    # 回滚配置
    results = {}
    for config_key, config_value in old_config.items():
        try:
            node_client.config_set(config_key, config_value)
            results[config_key] = 'success'
        except Exception as e:
            results[config_key] = f'failed: {str(e)}'

    # 持久化配置
    node_client.config_rewrite()

    return results
```

---

## 4. 数据管理

### 4.1 数据备份

#### 4.1.1 集群全量备份

```bash
#!/bin/bash
# cluster-backup.sh

BACKUP_DIR="/backup/redis/cluster"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="${BACKUP_DIR}/${DATE}"

mkdir -p "$BACKUP_PATH"

# 获取所有主节点
CLUSTER_NODE="127.0.0.1:7000"
MASTER_NODES=$(redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER NODES | \
    grep master | awk '{print $2}')

# 备份每个主节点
for node in $MASTER_NODES; do
    host=${node%%:*}
    port=${node##*:}

    echo "备份节点 $host:$port..."

    # RDB备份
    redis-cli -h "$host" -p "$port" BGSAVE
    while [ "$(redis-cli -h "$host" -p "$port" INFO persistence | grep rdb_bgsave_in_progress | cut -d: -f2 | tr -d '\r')" = "1" ]; do
        sleep 1
    done

    # 复制RDB文件
    rdb_file=$(redis-cli -h "$host" -p "$port" CONFIG GET dir | tail -1)/dump.rdb
    cp "$rdb_file" "${BACKUP_PATH}/dump_${host}_${port}.rdb"

    # AOF备份
    redis-cli -h "$host" -p "$port" BGREWRITEAOF
    while [ "$(redis-cli -h "$host" -p "$port" INFO persistence | grep aof_rewrite_in_progress | cut -d: -f2 | tr -d '\r')" = "1" ]; do
        sleep 1
    done

    # 复制AOF文件
    aof_file=$(redis-cli -h "$host" -p "$port" CONFIG GET dir | tail -1)/appendonly.aof
    cp "$aof_file" "${BACKUP_PATH}/appendonly_${host}_${port}.aof"
done

# 备份集群配置
redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER NODES > "${BACKUP_PATH}/cluster_nodes.txt"
redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER SLOTS > "${BACKUP_PATH}/cluster_slots.txt"

echo "备份完成: $BACKUP_PATH"
```

#### 4.1.2 增量备份

```python
#!/usr/bin/env python3
# incremental-backup.py

import redis
from redis.cluster import RedisCluster
import time
import json

class IncrementalBackup:
    """增量备份管理器"""

    def __init__(self, cluster_nodes, backup_dir):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.backup_dir = backup_dir
        self.last_backup_time = {}

    def backup_changes(self, node_id, since_time=None):
        """备份节点变更"""

        if since_time is None:
            since_time = self.last_backup_time.get(node_id, 0)

        # 获取节点信息
        nodes = self.cluster.cluster_nodes()
        node_info = nodes.get(node_id)
        if not node_info:
            raise ValueError(f"Node {node_id} not found")

        # 连接到节点
        node_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        # 获取变更的键（通过AOF或监控）
        # 这里简化处理，实际需要更复杂的逻辑
        changes = []

        # 记录备份时间
        self.last_backup_time[node_id] = time.time()

        return changes

    def save_backup(self, node_id, changes):
        """保存备份"""

        timestamp = int(time.time())
        backup_file = f"{self.backup_dir}/incremental_{node_id}_{timestamp}.json"

        backup_data = {
            'node_id': node_id,
            'timestamp': timestamp,
            'changes': changes
        }

        with open(backup_file, 'w') as f:
            json.dump(backup_data, f, indent=2)

        return backup_file
```

### 4.2 数据恢复

#### 4.2.1 从RDB恢复

```bash
#!/bin/bash
# restore-from-rdb.sh

RDB_FILE="$1"
NODE_HOST="$2"
NODE_PORT="$3"

if [ -z "$RDB_FILE" ] || [ -z "$NODE_HOST" ] || [ -z "$NODE_PORT" ]; then
    echo "Usage: $0 <rdb_file> <host> <port>"
    exit 1
fi

# 停止节点（如果运行中）
redis-cli -h "$NODE_HOST" -p "$NODE_PORT" SHUTDOWN SAVE

# 复制RDB文件
REDIS_DIR=$(redis-cli -h "$NODE_HOST" -p "$NODE_PORT" CONFIG GET dir | tail -1)
cp "$RDB_FILE" "${REDIS_DIR}/dump.rdb"

# 启动节点
redis-server --port "$NODE_PORT" --dir "$REDIS_DIR" --dbfilename dump.rdb

echo "恢复完成"
```

#### 4.2.2 从AOF恢复

```bash
#!/bin/bash
# restore-from-aof.sh

AOF_FILE="$1"
NODE_HOST="$2"
NODE_PORT="$3"

if [ -z "$AOF_FILE" ] || [ -z "$NODE_HOST" ] || [ -z "$NODE_PORT" ]; then
    echo "Usage: $0 <aof_file> <host> <port>"
    exit 1
fi

# 停止节点
redis-cli -h "$NODE_HOST" -p "$NODE_PORT" SHUTDOWN

# 复制AOF文件
REDIS_DIR=$(redis-cli -h "$NODE_HOST" -p "$NODE_PORT" CONFIG GET dir | tail -1)
cp "$AOF_FILE" "${REDIS_DIR}/appendonly.aof"

# 启动节点（启用AOF）
redis-server --port "$NODE_PORT" --dir "$REDIS_DIR" --appendonly yes --appendfilename appendonly.aof

echo "恢复完成"
```

### 4.3 数据迁移

```python
#!/usr/bin/env python3
# migrate-cluster-data.py

import redis
from redis.cluster import RedisCluster
import time

class ClusterDataMigrator:
    """集群数据迁移器"""

    def __init__(self, source_cluster_nodes, target_cluster_nodes):
        self.source_cluster = RedisCluster(
            startup_nodes=source_cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.target_cluster = RedisCluster(
            startup_nodes=target_cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )

    def migrate_keys(self, key_pattern="*", batch_size=1000):
        """迁移键"""

        # 扫描源集群的所有键
        cursor = 0
        migrated_count = 0

        while True:
            cursor, keys = self.source_cluster.scan(cursor, match=key_pattern, count=batch_size)

            for key in keys:
                try:
                    # 获取键的值和TTL
                    value = self.source_cluster.get(key)
                    ttl = self.source_cluster.ttl(key)

                    # 写入目标集群
                    if ttl > 0:
                        self.target_cluster.setex(key, ttl, value)
                    else:
                        self.target_cluster.set(key, value)

                    migrated_count += 1

                    if migrated_count % 1000 == 0:
                        print(f"已迁移 {migrated_count} 个键...")

                except Exception as e:
                    print(f"迁移键 {key} 失败: {e}")

            if cursor == 0:
                break

        return migrated_count

    def verify_migration(self, sample_keys=None):
        """验证迁移结果"""

        if sample_keys is None:
            # 随机采样一些键进行验证
            cursor = 0
            sample_keys = []
            while len(sample_keys) < 100:
                cursor, keys = self.source_cluster.scan(cursor, count=100)
                sample_keys.extend(keys[:100 - len(sample_keys)])
                if cursor == 0:
                    break

        mismatches = []
        for key in sample_keys:
            source_value = self.source_cluster.get(key)
            target_value = self.target_cluster.get(key)

            if source_value != target_value:
                mismatches.append(key)

        return {
            'total_checked': len(sample_keys),
            'mismatches': len(mismatches),
            'mismatch_keys': mismatches
        }
```

### 4.4 数据一致性检查

```python
#!/usr/bin/env python3
# check-data-consistency.py

import redis
from redis.cluster import RedisCluster
import hashlib

def check_cluster_consistency(cluster_nodes):
    """检查集群数据一致性"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    # 获取所有主节点
    nodes = cluster.cluster_nodes()
    master_nodes = {nid: info for nid, info in nodes.items() if 'master' in info['flags']}

    # 获取每个主节点的键
    node_keys = {}
    for node_id, node_info in master_nodes.items():
        node_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        keys = []
        cursor = 0
        while True:
            cursor, batch_keys = node_client.scan(cursor, count=1000)
            keys.extend(batch_keys)
            if cursor == 0:
                break

        node_keys[node_id] = keys

    # 检查主从一致性
    inconsistencies = []
    for node_id, node_info in master_nodes.items():
        master_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        # 找到该主节点的从节点
        slave_nodes = [nid for nid, info in nodes.items()
                       if info.get('master') == node_id]

        for slave_id in slave_nodes:
            slave_info = nodes[slave_id]
            slave_client = redis.Redis(
                host=slave_info['host'],
                port=slave_info['port'],
                decode_responses=True
            )

            # 检查键的一致性
            for key in node_keys[node_id][:100]:  # 采样检查
                master_value = master_client.get(key)
                slave_value = slave_client.get(key)

                if master_value != slave_value:
                    inconsistencies.append({
                        'master': node_id,
                        'slave': slave_id,
                        'key': key,
                        'master_value': master_value,
                        'slave_value': slave_value
                    })

    return {
        'total_masters': len(master_nodes),
        'total_slaves': len(nodes) - len(master_nodes),
        'inconsistencies': len(inconsistencies),
        'inconsistency_details': inconsistencies
    }
```

---

## 5. 性能监控

### 5.1 性能指标

#### 5.1.1 关键性能指标

```python
#!/usr/bin/env python3
# collect-performance-metrics.py

import redis
from redis.cluster import RedisCluster
import time

def collect_cluster_metrics(cluster_nodes):
    """收集集群性能指标"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    metrics = {
        'timestamp': time.time(),
        'nodes': {}
    }

    # 获取所有节点
    nodes = cluster.cluster_nodes()

    for node_id, node_info in nodes.items():
        node_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        # 收集节点指标
        info_server = node_client.info('server')
        info_memory = node_client.info('memory')
        info_stats = node_client.info('stats')
        info_replication = node_client.info('replication')

        node_metrics = {
            'host': node_info['host'],
            'port': node_info['port'],
            'role': info_replication.get('role', 'unknown'),
            'uptime': info_server.get('uptime_in_seconds', 0),
            'connected_clients': info_server.get('connected_clients', 0),
            'used_memory': info_memory.get('used_memory', 0),
            'used_memory_human': info_memory.get('used_memory_human', '0B'),
            'used_memory_peak': info_memory.get('used_memory_peak', 0),
            'mem_fragmentation_ratio': info_memory.get('mem_fragmentation_ratio', 0),
            'total_commands_processed': info_stats.get('total_commands_processed', 0),
            'instantaneous_ops_per_sec': info_stats.get('instantaneous_ops_per_sec', 0),
            'keyspace_hits': info_stats.get('keyspace_hits', 0),
            'keyspace_misses': info_stats.get('keyspace_misses', 0),
            'hit_rate': calculate_hit_rate(
                info_stats.get('keyspace_hits', 0),
                info_stats.get('keyspace_misses', 0)
            )
        }

        metrics['nodes'][node_id] = node_metrics

    # 计算集群总体指标
    metrics['cluster'] = {
        'total_nodes': len(nodes),
        'total_memory': sum(m['used_memory'] for m in metrics['nodes'].values()),
        'total_ops_per_sec': sum(m['instantaneous_ops_per_sec'] for m in metrics['nodes'].values()),
        'avg_hit_rate': calculate_avg_hit_rate(metrics['nodes'])
    }

    return metrics

def calculate_hit_rate(hits, misses):
    """计算命中率"""
    total = hits + misses
    if total == 0:
        return 0.0
    return hits / total * 100

def calculate_avg_hit_rate(nodes):
    """计算平均命中率"""
    hit_rates = [m['hit_rate'] for m in nodes.values() if m['hit_rate'] > 0]
    if not hit_rates:
        return 0.0
    return sum(hit_rates) / len(hit_rates)
```

### 5.2 监控工具

#### 5.2.1 Prometheus监控集成

```yaml
# prometheus-redis-exporter.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'redis-cluster'
    static_configs:
      - targets:
          - '127.0.0.1:9121'  # redis_exporter
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: __address__
        replacement: 127.0.0.1:9121
```

```python
#!/usr/bin/env python3
# prometheus-exporter.py

from prometheus_client import start_http_server, Gauge, Counter
import redis
from redis.cluster import RedisCluster
import time

# 定义指标
redis_memory_usage = Gauge('redis_memory_usage_bytes', 'Redis memory usage', ['node'])
redis_ops_per_sec = Gauge('redis_ops_per_sec', 'Redis operations per second', ['node'])
redis_connected_clients = Gauge('redis_connected_clients', 'Redis connected clients', ['node'])
redis_hit_rate = Gauge('redis_hit_rate', 'Redis cache hit rate', ['node'])

def collect_metrics(cluster_nodes):
    """收集指标并导出到Prometheus"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()

    while True:
        for node_id, node_info in nodes.items():
            node_client = redis.Redis(
                host=node_info['host'],
                port=node_info['port'],
                decode_responses=True
            )

            # 收集指标
            info_memory = node_client.info('memory')
            info_stats = node_client.info('stats')
            info_clients = node_client.info('clients')

            # 更新Prometheus指标
            node_label = f"{node_info['host']}:{node_info['port']}"

            redis_memory_usage.labels(node=node_label).set(
                info_memory.get('used_memory', 0)
            )
            redis_ops_per_sec.labels(node=node_label).set(
                info_stats.get('instantaneous_ops_per_sec', 0)
            )
            redis_connected_clients.labels(node=node_label).set(
                info_clients.get('connected_clients', 0)
            )

            hits = info_stats.get('keyspace_hits', 0)
            misses = info_stats.get('keyspace_misses', 0)
            hit_rate = hits / (hits + misses) * 100 if (hits + misses) > 0 else 0
            redis_hit_rate.labels(node=node_label).set(hit_rate)

        time.sleep(15)  # 每15秒收集一次

if __name__ == '__main__':
    # 启动Prometheus HTTP服务器
    start_http_server(8000)

    cluster_nodes = [
        {'host': '127.0.0.1', 'port': 7000}
    ]

    collect_metrics(cluster_nodes)
```

### 5.3 性能分析

```python
#!/usr/bin/env python3
# analyze-performance.py

import redis
from redis.cluster import RedisCluster
import statistics
import time

def analyze_cluster_performance(cluster_nodes, duration=60):
    """分析集群性能"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    # 收集性能数据
    performance_data = {
        'latencies': [],
        'throughput': [],
        'errors': []
    }

    start_time = time.time()
    request_count = 0

    while time.time() - start_time < duration:
        try:
            # 测试SET操作
            test_key = f"perf_test:{int(time.time() * 1000)}"
            test_value = "x" * 100  # 100字节的值

            set_start = time.time()
            cluster.set(test_key, test_value)
            set_latency = (time.time() - set_start) * 1000
            performance_data['latencies'].append(set_latency)

            # 测试GET操作
            get_start = time.time()
            cluster.get(test_key)
            get_latency = (time.time() - get_start) * 1000
            performance_data['latencies'].append(get_latency)

            # 清理测试键
            cluster.delete(test_key)

            request_count += 2

        except Exception as e:
            performance_data['errors'].append(str(e))

        time.sleep(0.01)  # 避免过于频繁

    # 计算统计信息
    if performance_data['latencies']:
        analysis = {
            'duration': duration,
            'total_requests': request_count,
            'throughput': request_count / duration,
            'latency': {
                'p50': statistics.median(performance_data['latencies']),
                'p95': statistics.quantiles(performance_data['latencies'], n=20)[18],
                'p99': statistics.quantiles(performance_data['latencies'], n=100)[98],
                'avg': statistics.mean(performance_data['latencies']),
                'max': max(performance_data['latencies']),
                'min': min(performance_data['latencies'])
            },
            'error_count': len(performance_data['errors']),
            'error_rate': len(performance_data['errors']) / request_count * 100 if request_count > 0 else 0
        }
    else:
        analysis = {'error': 'No data collected'}

    return analysis
```

### 5.4 性能优化

```python
#!/usr/bin/env python3
# optimize-performance.py

import redis
from redis.cluster import RedisCluster

def optimize_cluster_performance(cluster_nodes):
    """优化集群性能"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()
    optimizations = []

    for node_id, node_info in nodes.items():
        node_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        node_optimizations = []

        # 检查内存配置
        info_memory = node_client.info('memory')
        maxmemory = node_client.config_get('maxmemory')['maxmemory']

        if maxmemory == '0':
            node_client.config_set('maxmemory', '4gb')
            node_client.config_set('maxmemory-policy', 'allkeys-lru')
            node_optimizations.append('设置内存限制和淘汰策略')

        # 检查持久化配置
        info_persistence = node_client.info('persistence')
        if info_persistence.get('aof_enabled') == '0':
            node_client.config_set('appendonly', 'yes')
            node_optimizations.append('启用AOF持久化')

        # 检查慢查询日志
        slowlog_len = node_client.slowlog_len()
        if slowlog_len > 100:
            node_client.config_set('slowlog-log-slower-than', '10000')  # 10ms
            node_optimizations.append('优化慢查询日志配置')

        if node_optimizations:
            optimizations.append({
                'node': f"{node_info['host']}:{node_info['port']}",
                'optimizations': node_optimizations
            })

    return optimizations
```

---

## 6. 日常维护任务

### 6.1 定期健康检查

```bash
#!/bin/bash
# daily-health-check.sh

CLUSTER_NODE="127.0.0.1:7000"
LOG_FILE="/var/log/redis/cluster-health-check.log"
ALERT_EMAIL="admin@example.com"

# 执行健康检查
check_result=$(redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER INFO | grep cluster_state)

if [[ "$check_result" != *"ok"* ]]; then
    echo "$(date): 集群状态异常: $check_result" >> "$LOG_FILE"
    echo "集群状态异常: $check_result" | mail -s "Redis集群告警" "$ALERT_EMAIL"
fi

# 检查节点状态
node_status=$(redis-cli -c -h ${CLUSTER_NODE%%:*} -p ${CLUSTER_NODE##*:} CLUSTER NODES | grep fail | wc -l)

if [ "$node_status" -gt 0 ]; then
    echo "$(date): 发现 $node_status 个故障节点" >> "$LOG_FILE"
    echo "发现故障节点" | mail -s "Redis集群告警" "$ALERT_EMAIL"
fi
```

### 6.2 日志管理

```bash
#!/bin/bash
# manage-logs.sh

LOG_DIR="/var/log/redis"
RETENTION_DAYS=30

# 清理过期日志
find "$LOG_DIR" -name "*.log" -type f -mtime +$RETENTION_DAYS -delete

# 压缩旧日志
find "$LOG_DIR" -name "*.log" -type f -mtime +7 ! -name "*.gz" -exec gzip {} \;

# 限制日志文件大小
for log_file in "$LOG_DIR"/*.log; do
    if [ -f "$log_file" ]; then
        file_size=$(stat -f%z "$log_file" 2>/dev/null || stat -c%s "$log_file" 2>/dev/null)
        max_size=$((100 * 1024 * 1024))  # 100MB

        if [ "$file_size" -gt "$max_size" ]; then
            # 保留最后1000行
            tail -n 1000 "$log_file" > "${log_file}.tmp"
            mv "${log_file}.tmp" "$log_file"
        fi
    fi
done
```

### 6.3 清理任务

```python
#!/usr/bin/env python3
# cleanup-tasks.py

import redis
from redis.cluster import RedisCluster
import time

def cleanup_expired_keys(cluster_nodes, batch_size=1000):
    """清理过期键"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    cleaned_count = 0

    # 扫描所有键
    cursor = 0
    while True:
        cursor, keys = cluster.scan(cursor, count=batch_size)

        for key in keys:
            ttl = cluster.ttl(key)
            if ttl == -1:  # 没有过期时间的键，跳过
                continue
            elif ttl == -2:  # 键已过期但未删除
                cluster.delete(key)
                cleaned_count += 1

        if cursor == 0:
            break

    return cleaned_count

def cleanup_test_keys(cluster_nodes, pattern="test:*"):
    """清理测试键"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    deleted_count = 0

    cursor = 0
    while True:
        cursor, keys = cluster.scan(cursor, match=pattern, count=1000)

        if keys:
            cluster.delete(*keys)
            deleted_count += len(keys)

        if cursor == 0:
            break

    return deleted_count
```

### 6.4 性能调优

```python
#!/usr/bin/env python3
# performance-tuning.py

import redis
from redis.cluster import RedisCluster

def tune_cluster_performance(cluster_nodes):
    """调优集群性能"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()
    tuning_results = []

    for node_id, node_info in nodes.items():
        node_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        node_tuning = {
            'node': f"{node_info['host']}:{node_info['port']}",
            'changes': []
        }

        # 优化TCP设置
        node_client.config_set('tcp-keepalive', '60')
        node_tuning['changes'].append('设置TCP keepalive为60秒')

        # 优化客户端输出缓冲区
        node_client.config_set('client-output-buffer-limit', 'normal 0 0 0')
        node_client.config_set('client-output-buffer-limit', 'replica 256mb 64mb 60')
        node_client.config_set('client-output-buffer-limit', 'pubsub 32mb 8mb 60')
        node_tuning['changes'].append('优化客户端输出缓冲区限制')

        # 优化AOF重写
        node_client.config_set('auto-aof-rewrite-percentage', '100')
        node_client.config_set('auto-aof-rewrite-min-size', '64mb')
        node_tuning['changes'].append('优化AOF重写配置')

        # 持久化配置
        node_client.config_rewrite()

        tuning_results.append(node_tuning)

    return tuning_results
```

---

## 7. 大规模集群运维实践

### 7.1 100+节点集群运维

#### 7.1.1 大规模集群特点

- **节点数量**：100+节点
- **数据量**：TB级别
- **QPS**：百万级别
- **地域分布**：多地域部署

#### 7.1.2 运维挑战

1. **管理复杂度**：节点数量多，管理复杂
2. **故障影响**：单个节点故障可能影响整个集群
3. **性能监控**：需要实时监控大量节点
4. **数据一致性**：保证大规模集群数据一致性

#### 7.1.3 运维策略

```python
#!/usr/bin/env python3
# large-cluster-ops.py

class LargeClusterOperator:
    """大规模集群运维操作器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.node_groups = self._group_nodes_by_region()

    def _group_nodes_by_region(self):
        """按地域分组节点"""
        nodes = self.cluster.cluster_nodes()
        groups = {}

        for node_id, node_info in nodes.items():
            # 根据IP或标签确定地域
            region = self._get_region(node_info['host'])
            if region not in groups:
                groups[region] = []
            groups[region].append(node_id)

        return groups

    def _get_region(self, host):
        """获取节点所属地域"""
        # 简化实现，实际应根据IP段或配置确定
        if '192.168.1' in host:
            return 'region1'
        elif '192.168.2' in host:
            return 'region2'
        else:
            return 'region3'

    def batch_operation(self, operation, node_filter=None):
        """批量操作"""
        nodes = self.cluster.cluster_nodes()
        results = {}

        for node_id, node_info in nodes.items():
            if node_filter and not node_filter(node_id, node_info):
                continue

            try:
                result = operation(node_id, node_info)
                results[node_id] = {'status': 'success', 'result': result}
            except Exception as e:
                results[node_id] = {'status': 'failed', 'error': str(e)}

        return results
```

### 7.2 多地域集群运维

```python
#!/usr/bin/env python3
# multi-region-ops.py

class MultiRegionClusterOperator:
    """多地域集群运维操作器"""

    def __init__(self, region_clusters):
        self.region_clusters = {}
        for region, cluster_nodes in region_clusters.items():
            self.region_clusters[region] = RedisCluster(
                startup_nodes=cluster_nodes,
                decode_responses=True,
                skip_full_coverage_check=True
            )

    def sync_config_across_regions(self, config_updates):
        """跨地域同步配置"""
        results = {}

        for region, cluster in self.region_clusters.items():
            region_results = {}
            nodes = cluster.cluster_nodes()

            for node_id, node_info in nodes.items():
                node_client = redis.Redis(
                    host=node_info['host'],
                    port=node_info['port'],
                    decode_responses=True
                )

                node_results = {}
                for key, value in config_updates.items():
                    try:
                        node_client.config_set(key, value)
                        node_results[key] = 'success'
                    except Exception as e:
                        node_results[key] = f'failed: {str(e)}'

                region_results[node_id] = node_results

            results[region] = region_results

        return results

    def check_cross_region_latency(self):
        """检查跨地域延迟"""
        latency_results = {}

        regions = list(self.region_clusters.keys())
        for i, region1 in enumerate(regions):
            for region2 in regions[i+1:]:
                latency = self._measure_latency(region1, region2)
                latency_results[f"{region1}-{region2}"] = latency

        return latency_results

    def _measure_latency(self, region1, region2):
        """测量两个地域间的延迟"""
        import time

        cluster1 = self.region_clusters[region1]
        cluster2 = self.region_clusters[region2]

        test_key = f"latency_test:{int(time.time())}"
        test_value = "test"

        # 在region1写入
        start = time.time()
        cluster1.set(test_key, test_value)

        # 在region2读取
        while True:
            value = cluster2.get(test_key)
            if value == test_value:
                break
            time.sleep(0.001)

        latency = (time.time() - start) * 1000  # 转换为毫秒

        # 清理
        cluster1.delete(test_key)

        return latency
```

### 7.3 跨云集群运维

```python
#!/usr/bin/env python3
# cross-cloud-ops.py

class CrossCloudClusterOperator:
    """跨云集群运维操作器"""

    def __init__(self, cloud_clusters):
        self.cloud_clusters = {}
        for cloud, cluster_nodes in cloud_clusters.items():
            self.cloud_clusters[cloud] = RedisCluster(
                startup_nodes=cluster_nodes,
                decode_responses=True,
                skip_full_coverage_check=True
            )

    def backup_to_cloud(self, source_cloud, target_cloud):
        """备份数据到另一个云"""
        source_cluster = self.cloud_clusters[source_cloud]
        target_cluster = self.cloud_clusters[target_cloud]

        # 获取源集群的所有键
        cursor = 0
        migrated_count = 0

        while True:
            cursor, keys = source_cluster.scan(cursor, count=1000)

            for key in keys:
                try:
                    value = source_cluster.get(key)
                    ttl = source_cluster.ttl(key)

                    if ttl > 0:
                        target_cluster.setex(key, ttl, value)
                    else:
                        target_cluster.set(key, value)

                    migrated_count += 1
                except Exception as e:
                    print(f"迁移键 {key} 失败: {e}")

            if cursor == 0:
                break

        return migrated_count
```

---

## 8. 运维自动化

### 8.1 自动化脚本

见上述各章节的脚本示例。

### 8.2 运维平台

```python
#!/usr/bin/env python3
# ops-platform.py

from flask import Flask, jsonify, request
import redis
from redis.cluster import RedisCluster

app = Flask(__name__)

# 集群配置
CLUSTER_NODES = [
    {'host': '127.0.0.1', 'port': 7000}
]

@app.route('/api/cluster/health', methods=['GET'])
def cluster_health():
    """集群健康检查API"""
    cluster = RedisCluster(
        startup_nodes=CLUSTER_NODES,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    cluster_info = cluster.cluster_info()
    return jsonify({
        'status': 'ok',
        'cluster_state': cluster_info.get('cluster_state'),
        'cluster_slots_assigned': cluster_info.get('cluster_slots_assigned')
    })

@app.route('/api/cluster/config', methods=['POST'])
def update_config():
    """更新配置API"""
    data = request.json
    node_id = data.get('node_id')
    config_updates = data.get('config')

    # 实现配置更新逻辑
    try:
        from redis.cluster import RedisCluster

        # 获取集群连接
        cluster_nodes = [
            {'host': '127.0.0.1', 'port': 7000},
            {'host': '127.0.0.1', 'port': 7001}
        ]
        cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )

        # 更新节点配置
        if node_id:
            # 更新特定节点配置
            node = cluster.get_node(node_id)
            if node:
                for key, value in config_updates.items():
                    node.execute_command('CONFIG', 'SET', key, value)
            else:
                return jsonify({'status': 'error', 'message': f'Node {node_id} not found'}), 404
        else:
            # 更新所有节点配置
            for node in cluster.nodes.values():
                for key, value in config_updates.items():
                    try:
                        node.execute_command('CONFIG', 'SET', key, value)
                    except Exception as e:
                        print(f"Error updating config on node {node}: {e}")

        return jsonify({'status': 'success'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/cluster/backup', methods=['POST'])
def create_backup():
    """创建备份API"""
    try:
        from redis.cluster import RedisCluster
        from datetime import datetime
        import subprocess

        # 获取集群连接
        cluster_nodes = [
            {'host': '127.0.0.1', 'port': 7000},
            {'host': '127.0.0.1', 'port': 7001}
        ]
        cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )

        # 生成备份ID
        backup_id = f"backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        backup_dir = f"/backups/{backup_id}"

        # 创建备份目录
        import os
        os.makedirs(backup_dir, exist_ok=True)

        # 备份每个节点
        backup_results = []
        for node in cluster.nodes.values():
            try:
                node_host = node.host
                node_port = node.port

                # 使用redis-cli进行备份
                backup_file = f"{backup_dir}/node_{node_host}_{node_port}.rdb"
                cmd = [
                    'redis-cli',
                    '-h', node_host,
                    '-p', str(node_port),
                    '--rdb', backup_file
                ]

                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    backup_results.append({
                        'node': f"{node_host}:{node_port}",
                        'status': 'success',
                        'file': backup_file
                    })
                else:
                    backup_results.append({
                        'node': f"{node_host}:{node_port}",
                        'status': 'error',
                        'message': result.stderr
                    })
            except Exception as e:
                backup_results.append({
                    'node': f"{node.host}:{node.port}",
                    'status': 'error',
                    'message': str(e)
                })

        return jsonify({
            'status': 'success',
            'backup_id': backup_id,
            'backup_dir': backup_dir,
            'results': backup_results
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 8.3 CI/CD集成

```yaml
# .gitlab-ci.yml
stages:
  - deploy
  - verify
  - monitor

deploy_cluster:
  stage: deploy
  script:
    - ./scripts/deploy-cluster.sh
  only:
    - main

verify_cluster:
  stage: verify
  script:
    - ./scripts/verify-cluster.sh
  after_script:
    - ./scripts/health-check.sh

monitor_cluster:
  stage: monitor
  script:
    - ./scripts/start-monitoring.sh
  only:
    - main
```

---

## 9. 运维最佳实践

### 9.1 运维规范

1. **操作规范**：
   - 所有操作必须经过审批
   - 重要操作必须备份
   - 操作前必须检查集群状态

2. **监控规范**：
   - 7x24小时监控
   - 关键指标实时告警
   - 定期性能分析

3. **备份规范**：
   - 每日全量备份
   - 每小时增量备份
   - 备份保留30天

### 9.2 运维流程

1. **变更流程**：
   - 提交变更申请
   - 技术评审
   - 执行变更
   - 验证结果
   - 记录变更

2. **故障处理流程**：
   - 故障发现
   - 故障定位
   - 故障处理
   - 故障恢复
   - 故障复盘

### 9.3 运维工具链

- **监控工具**：Prometheus + Grafana
- **日志工具**：ELK Stack
- **配置管理**：Ansible + Git
- **自动化平台**：Rundeck / Ansible Tower

---

## 10. 扩展阅读

- [集群部署实践](07.05.01-集群部署实践.md)
- [集群故障处理](07.05.03-集群故障处理.md)
- [集群容量规划](07.05.04-集群容量规划.md)
- [Prometheus监控体系](../07.03-监控告警/07.03.01-Prometheus监控体系.md)
- [Redis Cluster集群模式](../../03-Redis组件/03.03-高可用架构/03.03.03-Cluster集群模式.md)

---

## 11. 权威参考

- [Redis Cluster官方文档](https://redis.io/docs/manual/scaling/)
- [Redis运维最佳实践](https://redis.io/docs/manual/administration/)

---

**文档版本**：v2.0
**最后更新**：2025-01
**文档状态**：✅ 完成（已大幅扩充内容）
