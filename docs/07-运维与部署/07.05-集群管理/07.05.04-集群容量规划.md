# 07.05.04 集群容量规划

## 目录

- [07.05.04 集群容量规划](#070504-集群容量规划)
  - [目录](#目录)
  - [1. 概述](#1-概述)
  - [2. 容量评估方法](#2-容量评估方法)
    - [2.1 数据量评估](#21-数据量评估)
      - [2.1.1 当前数据量评估](#211-当前数据量评估)
      - [2.1.2 数据增长预测](#212-数据增长预测)
    - [2.2 QPS评估](#22-qps评估)
      - [2.2.1 当前QPS评估](#221-当前qps评估)
      - [2.2.2 QPS增长预测](#222-qps增长预测)
    - [2.3 内存需求评估](#23-内存需求评估)
    - [2.4 网络带宽评估](#24-网络带宽评估)
    - [2.5 存储容量评估](#25-存储容量评估)
  - [3. 容量规划工具](#3-容量规划工具)
    - [3.1 容量计算工具](#31-容量计算工具)
    - [3.2 容量预测工具](#32-容量预测工具)
    - [3.3 容量优化工具](#33-容量优化工具)
  - [4. 扩容策略](#4-扩容策略)
    - [4.1 垂直扩容](#41-垂直扩容)
      - [4.1.1 垂直扩容场景](#411-垂直扩容场景)
      - [4.1.2 垂直扩容步骤](#412-垂直扩容步骤)
    - [4.2 水平扩容](#42-水平扩容)
      - [4.2.1 水平扩容场景](#421-水平扩容场景)
      - [4.2.2 水平扩容步骤](#422-水平扩容步骤)
    - [4.3 混合扩容](#43-混合扩容)
    - [4.4 扩容时机选择](#44-扩容时机选择)
  - [5. 容量监控](#5-容量监控)
    - [5.1 监控指标](#51-监控指标)
    - [5.2 容量预警](#52-容量预警)
    - [5.3 容量趋势分析](#53-容量趋势分析)
  - [6. 大规模集群容量规划](#6-大规模集群容量规划)
    - [6.1 100+节点集群规划](#61-100节点集群规划)
      - [6.1.1 规划原则](#611-规划原则)
      - [6.1.2 规划示例](#612-规划示例)
    - [6.2 多地域集群规划](#62-多地域集群规划)
    - [6.3 跨云集群规划](#63-跨云集群规划)
  - [7. 容量优化](#7-容量优化)
    - [7.1 数据压缩](#71-数据压缩)
    - [7.2 数据清理](#72-数据清理)
    - [7.3 数据分层](#73-数据分层)
  - [8. 成本优化](#8-成本优化)
    - [8.1 资源利用率优化](#81-资源利用率优化)
    - [8.2 成本模型](#82-成本模型)
    - [8.3 成本优化策略](#83-成本优化策略)
  - [9. 容量规划最佳实践](#9-容量规划最佳实践)
    - [9.1 规划原则](#91-规划原则)
    - [9.2 规划流程](#92-规划流程)
    - [9.3 规划工具链](#93-规划工具链)
  - [10. 扩展阅读](#10-扩展阅读)
  - [11. 权威参考](#11-权威参考)

---

## 1. 概述

集群容量规划涵盖容量评估方法、规划工具、扩容策略和容量监控，特别关注大规模集群的容量规划实践。

**规划目标**：

- ✅ 准确评估容量需求
- ✅ 合理规划资源配置
- ✅ 及时扩容避免资源耗尽
- ✅ 优化资源利用率降低成本

**规划范围**：

- 数据量评估和预测
- QPS评估和预测
- 内存、CPU、网络、存储需求评估
- 扩容策略和时机选择
- 容量监控和预警
- 成本优化

---

## 2. 容量评估方法

### 2.1 数据量评估

#### 2.1.1 当前数据量评估

```python
#!/usr/bin/env python3
# assess-data-size.py

import redis
from redis.cluster import RedisCluster
import math

def assess_cluster_data_size(cluster_nodes):
    """评估集群数据量"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()
    total_data_size = 0
    total_key_count = 0

    # 评估每个主节点的数据量
    for node_id, node_info in nodes.items():
        if 'master' in node_info['flags']:
            node_client = redis.Redis(
                host=node_info['host'],
                port=node_info['port'],
                decode_responses=True
            )

            # 获取节点信息
            info_memory = node_client.info('memory')
            info_keyspace = node_client.info('keyspace')

            # 计算数据量
            used_memory = info_memory.get('used_memory', 0)
            used_memory_dataset = info_memory.get('used_memory_dataset', 0)

            # 计算键数量
            db_keys = 0
            for db, db_info in info_keyspace.items():
                if 'keys' in db_info:
                    db_keys += db_info['keys']

            total_data_size += used_memory_dataset
            total_key_count += db_keys

    # 考虑复制因子
    replication_factor = 2  # 假设1主1从
    total_data_size_with_replication = total_data_size * replication_factor

    return {
        'total_data_size': total_data_size,
        'total_data_size_human': format_bytes(total_data_size),
        'total_data_size_with_replication': total_data_size_with_replication,
        'total_data_size_with_replication_human': format_bytes(total_data_size_with_replication),
        'total_key_count': total_key_count,
        'avg_key_size': total_data_size / total_key_count if total_key_count > 0 else 0
    }

def format_bytes(bytes_size):
    """格式化字节大小"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.2f} PB"
```

#### 2.1.2 数据增长预测

```python
#!/usr/bin/env python3
# predict-data-growth.py

import redis
from redis.cluster import RedisCluster
from datetime import datetime, timedelta
import statistics

class DataGrowthPredictor:
    """数据增长预测器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.history = []

    def collect_history(self, days=30):
        """收集历史数据"""
        for i in range(days):
            date = datetime.now() - timedelta(days=days - i)
            data_size = self.get_current_data_size()

            self.history.append({
                'date': date,
                'data_size': data_size
            })

    def get_current_data_size(self):
        """获取当前数据量"""
        nodes = self.cluster.cluster_nodes()
        total_size = 0

        for node_id, node_info in nodes.items():
            if 'master' in node_info['flags']:
                node_client = redis.Redis(
                    host=node_info['host'],
                    port=node_info['port'],
                    decode_responses=True
                )
                info_memory = node_client.info('memory')
                total_size += info_memory.get('used_memory_dataset', 0)

        return total_size

    def predict_future_size(self, days_ahead=30):
        """预测未来数据量"""
        if len(self.history) < 7:
            raise ValueError("历史数据不足，至少需要7天的数据")

        # 计算日均增长率
        growth_rates = []
        for i in range(1, len(self.history)):
            prev_size = self.history[i-1]['data_size']
            curr_size = self.history[i]['data_size']
            if prev_size > 0:
                growth_rate = (curr_size - prev_size) / prev_size
                growth_rates.append(growth_rate)

        avg_growth_rate = statistics.mean(growth_rates)

        # 预测未来数据量
        current_size = self.history[-1]['data_size']
        predicted_size = current_size * (1 + avg_growth_rate) ** days_ahead

        return {
            'current_size': current_size,
            'predicted_size': predicted_size,
            'growth_rate': avg_growth_rate,
            'days_ahead': days_ahead,
            'size_increase': predicted_size - current_size
        }
```

### 2.2 QPS评估

#### 2.2.1 当前QPS评估

```python
#!/usr/bin/env python3
# assess-qps.py

import redis
from redis.cluster import RedisCluster
import time

def assess_cluster_qps(cluster_nodes, duration=60):
    """评估集群QPS"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()
    qps_data = {}

    # 收集每个节点的QPS
    for node_id, node_info in nodes.items():
        node_client = redis.Redis(
            host=node_info['host'],
            port=node_info['port'],
            decode_responses=True
        )

        # 获取初始统计
        info_stats_before = node_client.info('stats')
        commands_before = info_stats_before.get('total_commands_processed', 0)

        # 等待一段时间
        time.sleep(duration)

        # 获取最终统计
        info_stats_after = node_client.info('stats')
        commands_after = info_stats_after.get('total_commands_processed', 0)

        # 计算QPS
        qps = (commands_after - commands_before) / duration

        qps_data[node_id] = {
            'qps': qps,
            'instantaneous_ops_per_sec': info_stats_after.get('instantaneous_ops_per_sec', 0)
        }

    # 计算集群总QPS
    total_qps = sum(data['qps'] for data in qps_data.values())
    peak_qps = max(data['instantaneous_ops_per_sec'] for data in qps_data.values())

    return {
        'total_qps': total_qps,
        'peak_qps': peak_qps,
        'node_qps': qps_data,
        'avg_qps_per_node': total_qps / len(qps_data) if qps_data else 0
    }
```

#### 2.2.2 QPS增长预测

```python
#!/usr/bin/env python3
# predict-qps-growth.py

class QPSGrowthPredictor:
    """QPS增长预测器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.qps_history = []

    def collect_qps_history(self, days=30):
        """收集QPS历史数据"""
        for i in range(days):
            date = datetime.now() - timedelta(days=days - i)
            qps = self.get_current_qps()

            self.qps_history.append({
                'date': date,
                'qps': qps
            })

    def get_current_qps(self):
        """获取当前QPS"""
        nodes = self.cluster.cluster_nodes()
        total_qps = 0

        for node_id, node_info in nodes.items():
            node_client = redis.Redis(
                host=node_info['host'],
                port=node_info['port'],
                decode_responses=True
            )
            info_stats = node_client.info('stats')
            total_qps += info_stats.get('instantaneous_ops_per_sec', 0)

        return total_qps

    def predict_future_qps(self, days_ahead=30):
        """预测未来QPS"""
        if len(self.qps_history) < 7:
            raise ValueError("历史数据不足")

        # 计算日均增长率
        growth_rates = []
        for i in range(1, len(self.qps_history)):
            prev_qps = self.qps_history[i-1]['qps']
            curr_qps = self.qps_history[i]['qps']
            if prev_qps > 0:
                growth_rate = (curr_qps - prev_qps) / prev_qps
                growth_rates.append(growth_rate)

        avg_growth_rate = statistics.mean(growth_rates)

        # 预测未来QPS
        current_qps = self.qps_history[-1]['qps']
        predicted_qps = current_qps * (1 + avg_growth_rate) ** days_ahead

        return {
            'current_qps': current_qps,
            'predicted_qps': predicted_qps,
            'growth_rate': avg_growth_rate,
            'days_ahead': days_ahead
        }
```

### 2.3 内存需求评估

```python
#!/usr/bin/env python3
# assess-memory-requirements.py

def assess_memory_requirements(data_size, qps, replication_factor=2, overhead_factor=1.2):
    """评估内存需求"""

    # 基础数据内存
    base_memory = data_size

    # 复制内存
    replication_memory = base_memory * replication_factor

    # 开销内存（索引、元数据等）
    overhead_memory = replication_memory * (overhead_factor - 1)

    # 总内存需求
    total_memory = replication_memory * overhead_factor

    # 考虑峰值QPS的内存需求（连接、缓冲区等）
    # 假设每个连接需要1KB内存，峰值连接数为QPS的10倍
    peak_connections = qps * 10
    connection_memory = peak_connections * 1024  # 1KB per connection

    # 缓冲区内存（假设为数据量的10%）
    buffer_memory = base_memory * 0.1

    total_memory_with_overhead = total_memory + connection_memory + buffer_memory

    return {
        'base_memory': base_memory,
        'replication_memory': replication_memory,
        'overhead_memory': overhead_memory,
        'connection_memory': connection_memory,
        'buffer_memory': buffer_memory,
        'total_memory': total_memory_with_overhead,
        'total_memory_human': format_bytes(total_memory_with_overhead)
    }
```

### 2.4 网络带宽评估

```python
#!/usr/bin/env python3
# assess-network-bandwidth.py

def assess_network_bandwidth(qps, avg_request_size=100, avg_response_size=500, replication_factor=2):
    """评估网络带宽需求"""

    # 请求带宽（字节/秒）
    request_bandwidth = qps * avg_request_size

    # 响应带宽（字节/秒）
    response_bandwidth = qps * avg_response_size

    # 复制带宽（主从复制）
    replication_bandwidth = request_bandwidth * replication_factor

    # 总带宽需求
    total_bandwidth = request_bandwidth + response_bandwidth + replication_bandwidth

    # 转换为Mbps
    total_bandwidth_mbps = total_bandwidth * 8 / (1024 * 1024)

    return {
        'request_bandwidth': request_bandwidth,
        'response_bandwidth': response_bandwidth,
        'replication_bandwidth': replication_bandwidth,
        'total_bandwidth': total_bandwidth,
        'total_bandwidth_mbps': total_bandwidth_mbps,
        'recommended_bandwidth_mbps': total_bandwidth_mbps * 1.5  # 1.5倍冗余
    }
```

### 2.5 存储容量评估

```python
#!/usr/bin/env python3
# assess-storage-capacity.py

def assess_storage_capacity(data_size, aof_enabled=True, rdb_enabled=True, retention_days=30):
    """评估存储容量需求"""

    # RDB存储需求
    rdb_size = data_size if rdb_enabled else 0

    # AOF存储需求（假设AOF是RDB的1.5倍）
    aof_size = data_size * 1.5 if aof_enabled else 0

    # 备份存储需求（每日备份，保留30天）
    daily_backup_size = data_size
    backup_storage = daily_backup_size * retention_days

    # 日志存储需求（假设每日日志为数据量的1%）
    daily_log_size = data_size * 0.01
    log_storage = daily_log_size * retention_days

    # 总存储需求
    total_storage = rdb_size + aof_size + backup_storage + log_storage

    return {
        'rdb_storage': rdb_size,
        'aof_storage': aof_size,
        'backup_storage': backup_storage,
        'log_storage': log_storage,
        'total_storage': total_storage,
        'total_storage_human': format_bytes(total_storage)
    }
```

---

## 3. 容量规划工具

### 3.1 容量计算工具

```python
#!/usr/bin/env python3
# capacity-calculator.py

class CapacityCalculator:
    """容量计算器"""

    def __init__(self):
        # 默认配置
        self.memory_per_node = 8 * 1024 * 1024 * 1024  # 8GB
        self.qps_per_node = 100000  # 10万QPS
        self.network_bandwidth_per_node = 10 * 1024 * 1024 * 1024 / 8  # 10Gbps
        self.cpu_cores_per_node = 8

    def calculate_nodes_required(self, data_size, qps, replication_factor=2):
        """计算所需节点数"""

        # 数据量计算
        total_data_size = data_size * replication_factor * 1.2  # 20%开销
        nodes_for_data = math.ceil(total_data_size / self.memory_per_node)

        # QPS计算
        nodes_for_qps = math.ceil(qps / self.qps_per_node)

        # 网络带宽计算
        network_bandwidth = self.estimate_network_bandwidth(qps)
        nodes_for_network = math.ceil(network_bandwidth / self.network_bandwidth_per_node)

        # 取最大值
        required_nodes = max(nodes_for_data, nodes_for_qps, nodes_for_network)

        # 添加冗余节点（20%冗余）
        final_nodes = int(required_nodes * 1.2)

        return {
            'nodes_for_data': nodes_for_data,
            'nodes_for_qps': nodes_for_qps,
            'nodes_for_network': nodes_for_network,
            'required_nodes': required_nodes,
            'final_nodes': final_nodes,
            'redundancy': final_nodes - required_nodes
        }

    def estimate_network_bandwidth(self, qps):
        """估算网络带宽"""
        avg_request_size = 100  # 100字节
        avg_response_size = 500  # 500字节

        bandwidth = qps * (avg_request_size + avg_response_size) * 8 / (1024 * 1024)  # Mbps
        return bandwidth

    def calculate_cost(self, nodes, cost_per_node_per_month=100):
        """计算成本"""
        return {
            'nodes': nodes,
            'cost_per_node_per_month': cost_per_node_per_month,
            'total_cost_per_month': nodes * cost_per_node_per_month,
            'total_cost_per_year': nodes * cost_per_node_per_month * 12
        }
```

### 3.2 容量预测工具

见2.1.2和2.2.2节。

### 3.3 容量优化工具

```python
#!/usr/bin/env python3
# capacity-optimizer.py

class CapacityOptimizer:
    """容量优化器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )

    def analyze_capacity_utilization(self):
        """分析容量利用率"""

        nodes = self.cluster.cluster_nodes()
        utilization_data = {}

        for node_id, node_info in nodes.items():
            node_client = redis.Redis(
                host=node_info['host'],
                port=node_info['port'],
                decode_responses=True
            )

            info_memory = node_client.info('memory')
            info_stats = node_client.info('stats')
            info_clients = node_client.info('clients')

            used_memory = info_memory.get('used_memory', 0)
            maxmemory = int(node_client.config_get('maxmemory')['maxmemory'])
            memory_utilization = (used_memory / maxmemory * 100) if maxmemory > 0 else 0

            qps = info_stats.get('instantaneous_ops_per_sec', 0)
            max_qps = 100000  # 假设最大QPS
            qps_utilization = (qps / max_qps * 100) if max_qps > 0 else 0

            connected_clients = info_clients.get('connected_clients', 0)
            max_clients = 10000  # 假设最大连接数
            client_utilization = (connected_clients / max_clients * 100) if max_clients > 0 else 0

            utilization_data[node_id] = {
                'memory_utilization': memory_utilization,
                'qps_utilization': qps_utilization,
                'client_utilization': client_utilization,
                'overall_utilization': max(memory_utilization, qps_utilization, client_utilization)
            }

        return utilization_data

    def recommend_optimization(self):
        """推荐优化方案"""

        utilization = self.analyze_capacity_utilization()
        recommendations = []

        for node_id, util in utilization.items():
            if util['overall_utilization'] > 80:
                recommendations.append({
                    'node': node_id,
                    'issue': 'high_utilization',
                    'utilization': util['overall_utilization'],
                    'recommendation': '考虑扩容或优化数据'
                })
            elif util['overall_utilization'] < 30:
                recommendations.append({
                    'node': node_id,
                    'issue': 'low_utilization',
                    'utilization': util['overall_utilization'],
                    'recommendation': '考虑缩容以降低成本'
                })

        return recommendations
```

---

## 4. 扩容策略

### 4.1 垂直扩容

#### 4.1.1 垂直扩容场景

- 单节点性能瓶颈
- 内存不足但节点数充足
- CPU不足但节点数充足

#### 4.1.2 垂直扩容步骤

```bash
#!/bin/bash
# vertical-scale.sh

NODE_HOST="$1"
NODE_PORT="$2"
NEW_MEMORY="$3"

if [ -z "$NODE_HOST" ] || [ -z "$NODE_PORT" ] || [ -z "$NEW_MEMORY" ]; then
    echo "Usage: $0 <host> <port> <new_memory>"
    exit 1
fi

echo "垂直扩容节点 $NODE_HOST:$NODE_PORT"

# 1. 备份数据
echo "备份数据..."
redis-cli -h "$NODE_HOST" -p "$NODE_PORT" BGSAVE

# 2. 停止节点
echo "停止节点..."
redis-cli -h "$NODE_HOST" -p "$NODE_PORT" SHUTDOWN SAVE

# 3. 更新配置（在实际环境中，需要更新服务器配置）
echo "更新内存配置为 $NEW_MEMORY"

# 4. 启动节点
echo "启动节点..."
redis-server --port "$NODE_PORT" --maxmemory "$NEW_MEMORY"

# 5. 验证
sleep 5
if redis-cli -h "$NODE_HOST" -p "$NODE_PORT" PING | grep -q PONG; then
    echo "垂直扩容成功"
else
    echo "垂直扩容失败"
    exit 1
fi
```

### 4.2 水平扩容

#### 4.2.1 水平扩容场景

- 需要增加整体容量
- 需要提高整体QPS
- 需要提高可用性

#### 4.2.2 水平扩容步骤

```python
#!/usr/bin/env python3
# horizontal-scale.py

import redis
from redis.cluster import RedisCluster

def horizontal_scale(cluster_nodes, new_nodes, slots_per_node=328):
    """水平扩容"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    # 1. 添加新节点
    for new_node in new_nodes:
        cluster.cluster_meet(new_node['host'], new_node['port'])

    # 2. 等待节点加入
    import time
    time.sleep(5)

    # 3. 迁移槽位
    nodes = cluster.cluster_nodes()
    master_nodes = [nid for nid, info in nodes.items() if 'master' in info['flags']]

    # 计算每个新节点需要迁移的槽位数
    total_slots = 16384
    existing_masters = len(master_nodes) - len(new_nodes)
    slots_to_migrate_per_new_node = total_slots // (existing_masters + len(new_nodes))

    # 从现有节点迁移槽位到新节点
    for i, new_node in enumerate(new_nodes):
        new_node_id = None
        for nid, info in nodes.items():
            if info['host'] == new_node['host'] and info['port'] == new_node['port']:
                new_node_id = nid
                break

        if not new_node_id:
            continue

        # 选择源节点（选择槽位最多的节点）
        source_node_id = max(master_nodes, key=lambda nid: len(nodes[nid].get('slots', [])))

        # 迁移槽位
        migrate_slots(cluster, source_node_id, new_node_id, slots_to_migrate_per_new_node)

    return {'status': 'success', 'message': '水平扩容完成'}

def migrate_slots(cluster, source_node_id, target_node_id, slot_count):
    """迁移槽位"""
    nodes = cluster.cluster_nodes()
    source_node = nodes[source_node_id]
    target_node = nodes[target_node_id]

    source_client = redis.Redis(
        host=source_node['host'],
        port=source_node['port'],
        decode_responses=True
    )

    target_client = redis.Redis(
        host=target_node['host'],
        port=target_node['port'],
        decode_responses=True
    )

    # 获取源节点的槽位
    source_slots = source_node.get('slots', [])
    slots_to_migrate = source_slots[:slot_count]

    # 迁移每个槽位
    for slot in slots_to_migrate:
        # 设置导入/迁移状态
        target_client.cluster_setslot(slot, 'IMPORTING', source_node_id)
        source_client.cluster_setslot(slot, 'MIGRATING', target_node_id)

        # 迁移键
        migrate_keys(source_client, target_client, slot)

        # 设置正常状态
        target_client.cluster_setslot(slot, 'NODE', target_node_id)
        source_client.cluster_setslot(slot, 'NODE', target_node_id)

def migrate_keys(source_client, target_client, slot):
    """迁移槽位中的键"""
    cursor = 0
    while True:
        cursor, keys = source_client.scan(cursor, count=100)
        for key in keys:
            # 检查键是否属于该槽位
            # 这里简化处理，实际需要计算键的槽位
            try:
                source_client.migrate(
                    target_client.connection_pool.connection_kwargs['host'],
                    target_client.connection_pool.connection_kwargs['port'],
                    key,
                    0,
                    5000
                )
            except:
                pass

        if cursor == 0:
            break
```

### 4.3 混合扩容

结合垂直扩容和水平扩容，根据实际情况选择最优方案。

### 4.4 扩容时机选择

```python
#!/usr/bin/env python3
# determine-scale-timing.py

class ScaleTimingDeterminer:
    """扩容时机确定器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )

    def should_scale(self, utilization_threshold=80, growth_rate_threshold=0.1):
        """判断是否应该扩容"""

        # 获取当前利用率
        utilization = self.get_current_utilization()

        # 获取增长趋势
        growth_rate = self.get_growth_rate()

        # 判断是否需要扩容
        if utilization > utilization_threshold:
            return {
                'should_scale': True,
                'reason': 'current_utilization_high',
                'utilization': utilization,
                'urgency': 'high' if utilization > 90 else 'medium'
            }

        if growth_rate > growth_rate_threshold:
            # 预测未来利用率
            predicted_utilization = utilization * (1 + growth_rate) ** 30  # 30天后

            if predicted_utilization > utilization_threshold:
                return {
                    'should_scale': True,
                    'reason': 'predicted_utilization_high',
                    'current_utilization': utilization,
                    'predicted_utilization': predicted_utilization,
                    'growth_rate': growth_rate,
                    'urgency': 'medium'
                }

        return {
            'should_scale': False,
            'reason': 'utilization_normal',
            'utilization': utilization
        }

    def get_current_utilization(self):
        """获取当前利用率"""
        nodes = self.cluster.cluster_nodes()
        max_utilization = 0

        for node_id, node_info in nodes.items():
            if 'master' in node_info['flags']:
                node_client = redis.Redis(
                    host=node_info['host'],
                    port=node_info['port'],
                    decode_responses=True
                )

                info_memory = node_client.info('memory')
                used_memory = info_memory.get('used_memory', 0)
                maxmemory = int(node_client.config_get('maxmemory')['maxmemory'])

                if maxmemory > 0:
                    utilization = used_memory / maxmemory * 100
                    max_utilization = max(max_utilization, utilization)

        return max_utilization

    def get_growth_rate(self):
        """获取增长速率"""
        # 简化实现，实际需要历史数据
        return 0.05  # 5%日均增长率
```

---

## 5. 容量监控

### 5.1 监控指标

见5.1.1节性能指标收集。

### 5.2 容量预警

```python
#!/usr/bin/env python3
# capacity-alert.py

class CapacityAlert:
    """容量预警系统"""

    def __init__(self, cluster_nodes, alert_thresholds):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.thresholds = alert_thresholds

    def check_capacity_alerts(self):
        """检查容量告警"""

        alerts = []
        nodes = self.cluster.cluster_nodes()

        for node_id, node_info in nodes.items():
            if 'master' in node_info['flags']:
                node_client = redis.Redis(
                    host=node_info['host'],
                    port=node_info['port'],
                    decode_responses=True
                )

                # 检查内存使用率
                info_memory = node_client.info('memory')
                used_memory = info_memory.get('used_memory', 0)
                maxmemory = int(node_client.config_get('maxmemory')['maxmemory'])

                if maxmemory > 0:
                    memory_usage = used_memory / maxmemory * 100

                    if memory_usage > self.thresholds.get('memory_critical', 95):
                        alerts.append({
                            'node': node_id,
                            'type': 'memory_critical',
                            'severity': 'critical',
                            'usage': memory_usage,
                            'message': f"内存使用率 {memory_usage:.2f}% 超过临界阈值"
                        })
                    elif memory_usage > self.thresholds.get('memory_warning', 80):
                        alerts.append({
                            'node': node_id,
                            'type': 'memory_warning',
                            'severity': 'warning',
                            'usage': memory_usage,
                            'message': f"内存使用率 {memory_usage:.2f}% 超过警告阈值"
                        })

                # 检查QPS
                info_stats = node_client.info('stats')
                qps = info_stats.get('instantaneous_ops_per_sec', 0)
                max_qps = self.thresholds.get('max_qps', 100000)

                if qps > max_qps * 0.9:
                    alerts.append({
                        'node': node_id,
                        'type': 'qps_high',
                        'severity': 'warning',
                        'qps': qps,
                        'message': f"QPS {qps} 接近上限"
                    })

        return alerts
```

### 5.3 容量趋势分析

```python
#!/usr/bin/env python3
# capacity-trend-analysis.py

class CapacityTrendAnalyzer:
    """容量趋势分析器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.history = []

    def collect_capacity_metrics(self):
        """收集容量指标"""
        metrics = {
            'timestamp': time.time(),
            'data_size': self.get_data_size(),
            'qps': self.get_qps(),
            'memory_usage': self.get_memory_usage(),
            'node_count': len(self.cluster.cluster_nodes())
        }

        self.history.append(metrics)

        # 保留最近90天的数据
        if len(self.history) > 90:
            self.history = self.history[-90:]

        return metrics

    def analyze_trend(self):
        """分析趋势"""
        if len(self.history) < 7:
            return {'error': '数据不足'}

        # 计算增长率
        data_size_growth = self.calculate_growth_rate([m['data_size'] for m in self.history])
        qps_growth = self.calculate_growth_rate([m['qps'] for m in self.history])
        memory_growth = self.calculate_growth_rate([m['memory_usage'] for m in self.history])

        # 预测未来容量需求
        current_metrics = self.history[-1]
        days_ahead = 30

        predicted_data_size = current_metrics['data_size'] * (1 + data_size_growth) ** days_ahead
        predicted_qps = current_metrics['qps'] * (1 + qps_growth) ** days_ahead
        predicted_memory = current_metrics['memory_usage'] * (1 + memory_growth) ** days_ahead

        return {
            'current': current_metrics,
            'growth_rates': {
                'data_size': data_size_growth,
                'qps': qps_growth,
                'memory': memory_growth
            },
            'predicted': {
                'data_size': predicted_data_size,
                'qps': predicted_qps,
                'memory': predicted_memory,
                'days_ahead': days_ahead
            }
        }

    def calculate_growth_rate(self, values):
        """计算增长率"""
        if len(values) < 2:
            return 0

        growth_rates = []
        for i in range(1, len(values)):
            if values[i-1] > 0:
                growth_rate = (values[i] - values[i-1]) / values[i-1]
                growth_rates.append(growth_rate)

        return statistics.mean(growth_rates) if growth_rates else 0

    def get_data_size(self):
        """获取数据量"""
        # 实现获取数据量的逻辑
        return 0

    def get_qps(self):
        """获取QPS"""
        # 实现获取QPS的逻辑
        return 0

    def get_memory_usage(self):
        """获取内存使用率"""
        # 实现获取内存使用率的逻辑
        return 0
```

---

## 6. 大规模集群容量规划

### 6.1 100+节点集群规划

#### 6.1.1 规划原则

- **分片策略**：合理分配16384个槽位
- **冗余设计**：每个主节点至少1个从节点
- **地域分布**：考虑多地域部署
- **容量预留**：预留20-30%容量

#### 6.1.2 规划示例

```python
#!/usr/bin/env python3
# plan-large-cluster.py

def plan_large_cluster(total_data_size, total_qps, regions=3):
    """规划大规模集群"""

    # 每个地域的容量
    data_per_region = total_data_size / regions
    qps_per_region = total_qps / regions

    # 每个地域的节点规划
    region_plan = {}

    for region in range(1, regions + 1):
        # 计算所需主节点数
        memory_per_node = 8 * 1024 * 1024 * 1024  # 8GB
        nodes_for_data = math.ceil(data_per_region / memory_per_node)

        qps_per_node = 100000
        nodes_for_qps = math.ceil(qps_per_region / qps_per_node)

        master_nodes = max(nodes_for_data, nodes_for_qps)

        # 添加20%冗余
        master_nodes = int(master_nodes * 1.2)

        # 从节点数（1:1主从）
        slave_nodes = master_nodes

        # 槽位分配
        slots_per_master = 16384 // (master_nodes * regions)

        region_plan[f'region_{region}'] = {
            'master_nodes': master_nodes,
            'slave_nodes': slave_nodes,
            'total_nodes': master_nodes + slave_nodes,
            'slots_per_master': slots_per_master,
            'data_size': data_per_region,
            'qps': qps_per_region
        }

    # 总规划
    total_plan = {
        'regions': regions,
        'total_master_nodes': sum(p['master_nodes'] for p in region_plan.values()),
        'total_slave_nodes': sum(p['slave_nodes'] for p in region_plan.values()),
        'total_nodes': sum(p['total_nodes'] for p in region_plan.values()),
        'region_plans': region_plan
    }

    return total_plan
```

### 6.2 多地域集群规划

见6.1节。

### 6.3 跨云集群规划

```python
#!/usr/bin/env python3
# plan-cross-cloud-cluster.py

def plan_cross_cloud_cluster(total_data_size, total_qps, clouds=['aws', 'aliyun', 'tencent']):
    """规划跨云集群"""

    # 主云（AWS）承担主要负载
    primary_cloud_data = total_data_size * 0.6
    primary_cloud_qps = total_qps * 0.6

    # 备份云各承担20%负载
    backup_cloud_data = total_data_size * 0.2
    backup_cloud_qps = total_qps * 0.2

    cloud_plan = {}

    # 主云规划
    cloud_plan[clouds[0]] = plan_cluster_capacity(primary_cloud_data, primary_cloud_qps)
    cloud_plan[clouds[0]]['role'] = 'primary'

    # 备份云规划
    for cloud in clouds[1:]:
        cloud_plan[cloud] = plan_cluster_capacity(backup_cloud_data, backup_cloud_qps)
        cloud_plan[cloud]['role'] = 'backup'

    return cloud_plan

def plan_cluster_capacity(data_size, qps):
    """规划集群容量"""
    # 实现集群容量规划逻辑
    return {
        'nodes': 10,
        'data_size': data_size,
        'qps': qps
    }
```

---

## 7. 容量优化

### 7.1 数据压缩

```python
#!/usr/bin/env python3
# optimize-data-compression.py

def optimize_data_compression(cluster_nodes):
    """优化数据压缩"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()
    optimization_results = []

    for node_id, node_info in nodes.items():
        if 'master' in node_info['flags']:
            node_client = redis.Redis(
                host=node_info['host'],
                port=node_info['port'],
                decode_responses=True
            )

            # 检查压缩配置
            compression_enabled = node_client.config_get('list-compress-depth')['list-compress-depth']

            if compression_enabled == '0':
                # 启用压缩
                node_client.config_set('list-compress-depth', '1')
                optimization_results.append({
                    'node': node_id,
                    'optimization': 'enabled_compression',
                    'savings': 'estimated_20-30%'
                })

    return optimization_results
```

### 7.2 数据清理

见6.3节清理任务。

### 7.3 数据分层

```python
#!/usr/bin/env python3
# implement-data-tiering.py

class DataTieringManager:
    """数据分层管理器"""

    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.hot_cluster = cluster_nodes  # 热数据集群
        self.cold_cluster = None  # 冷数据集群（如SSD存储）

    def tier_data_by_access_pattern(self):
        """根据访问模式分层数据"""

        # 分析访问模式
        access_patterns = self.analyze_access_patterns()

        # 迁移冷数据
        for key, pattern in access_patterns.items():
            if pattern['access_frequency'] < 0.01:  # 访问频率低于1%
                self.move_to_cold_tier(key)

    def analyze_access_patterns(self):
        """分析访问模式"""
        # 实现访问模式分析逻辑
        return {}

    def move_to_cold_tier(self, key):
        """移动到冷层"""
        # 实现数据迁移逻辑
        pass
```

---

## 8. 成本优化

### 8.1 资源利用率优化

```python
#!/usr/bin/env python3
# optimize-resource-utilization.py

def optimize_resource_utilization(cluster_nodes):
    """优化资源利用率"""

    cluster = RedisCluster(
        startup_nodes=cluster_nodes,
        decode_responses=True,
        skip_full_coverage_check=True
    )

    nodes = cluster.cluster_nodes()
    utilization_data = []

    # 收集利用率数据
    for node_id, node_info in nodes.items():
        if 'master' in node_info['flags']:
            node_client = redis.Redis(
                host=node_info['host'],
                port=node_info['port'],
                decode_responses=True
            )

            info_memory = node_client.info('memory')
            info_stats = node_client.info('stats')

            used_memory = info_memory.get('used_memory', 0)
            maxmemory = int(node_client.config_get('maxmemory')['maxmemory'])
            memory_utilization = (used_memory / maxmemory * 100) if maxmemory > 0 else 0

            qps = info_stats.get('instantaneous_ops_per_sec', 0)
            max_qps = 100000
            qps_utilization = (qps / max_qps * 100) if max_qps > 0 else 0

            utilization_data.append({
                'node': node_id,
                'memory_utilization': memory_utilization,
                'qps_utilization': qps_utilization,
                'overall_utilization': max(memory_utilization, qps_utilization)
            })

    # 分析利用率
    avg_utilization = statistics.mean([u['overall_utilization'] for u in utilization_data])

    # 优化建议
    recommendations = []

    if avg_utilization < 50:
        recommendations.append({
            'type': 'consolidation',
            'message': '平均利用率低于50%，建议合并节点以降低成本'
        })

    low_utilization_nodes = [u for u in utilization_data if u['overall_utilization'] < 30]
    if low_utilization_nodes:
        recommendations.append({
            'type': 'scale_in',
            'message': f"发现 {len(low_utilization_nodes)} 个低利用率节点，建议缩容"
        })

    return {
        'average_utilization': avg_utilization,
        'node_utilization': utilization_data,
        'recommendations': recommendations
    }
```

### 8.2 成本模型

```python
#!/usr/bin/env python3
# cost-model.py

class CostModel:
    """成本模型"""

    def __init__(self):
        self.node_cost_per_month = 100  # 每个节点每月成本
        self.storage_cost_per_gb_per_month = 0.1  # 每GB存储每月成本
        self.network_cost_per_gb = 0.05  # 每GB网络流量成本

    def calculate_cluster_cost(self, nodes, data_size, monthly_traffic_gb):
        """计算集群成本"""

        # 节点成本
        node_cost = nodes * self.node_cost_per_month

        # 存储成本
        storage_cost = data_size / (1024 ** 3) * self.storage_cost_per_gb_per_month

        # 网络成本
        network_cost = monthly_traffic_gb * self.network_cost_per_gb

        # 总成本
        total_cost = node_cost + storage_cost + network_cost

        return {
            'node_cost': node_cost,
            'storage_cost': storage_cost,
            'network_cost': network_cost,
            'total_cost': total_cost,
            'cost_per_month': total_cost,
            'cost_per_year': total_cost * 12
        }

    def optimize_cost(self, current_cost, target_reduction=0.2):
        """优化成本"""

        target_cost = current_cost * (1 - target_reduction)

        # 优化策略
        strategies = [
            {
                'strategy': 'reduce_nodes',
                'description': '减少节点数',
                'potential_savings': current_cost * 0.3
            },
            {
                'strategy': 'optimize_storage',
                'description': '优化存储',
                'potential_savings': current_cost * 0.1
            },
            {
                'strategy': 'optimize_network',
                'description': '优化网络',
                'potential_savings': current_cost * 0.05
            }
        ]

        return {
            'current_cost': current_cost,
            'target_cost': target_cost,
            'target_reduction': target_reduction,
            'strategies': strategies
        }
```

### 8.3 成本优化策略

1. **资源利用率优化**：提高资源利用率，减少浪费
2. **按需扩容**：根据实际需求扩容，避免过度配置
3. **数据分层**：将冷数据迁移到低成本存储
4. **自动化运维**：减少人工运维成本

---

## 9. 容量规划最佳实践

### 9.1 规划原则

1. **前瞻性**：提前规划，避免被动扩容
2. **冗余性**：预留20-30%容量冗余
3. **可扩展性**：设计可扩展的架构
4. **成本效益**：平衡性能和成本

### 9.2 规划流程

1. **需求分析**：分析业务需求和增长趋势
2. **容量评估**：评估当前和未来容量需求
3. **方案设计**：设计扩容方案
4. **成本评估**：评估扩容成本
5. **方案实施**：执行扩容方案
6. **效果验证**：验证扩容效果

### 9.3 规划工具链

- **容量评估工具**：容量计算器、预测工具
- **监控工具**：Prometheus、Grafana
- **自动化工具**：Ansible、Terraform
- **成本工具**：成本计算器、优化工具

---

## 10. 扩展阅读

- [集群部署实践](07.05.01-集群部署实践.md)
- [集群运维实践](07.05.02-集群运维实践.md)
- [集群故障处理](07.05.03-集群故障处理.md)
- [成本优化策略](../../04-架构设计/04.05-成本优化/04.05.05-成本优化策略.md)

---

## 11. 权威参考

- [Redis容量规划](https://redis.io/docs/manual/scaling/)
- [Redis性能优化](https://redis.io/docs/manual/optimization/)

---

**文档版本**：v2.0
**最后更新**：2025-01
**文档状态**：✅ 完成（已大幅扩充内容）
