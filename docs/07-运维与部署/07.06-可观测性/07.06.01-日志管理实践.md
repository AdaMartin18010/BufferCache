# 07.06.01 日志管理实践

## 目录

- [07.06.01 日志管理实践](#070601-日志管理实践)
  - [目录](#目录)
  - [1. 概述](#1-概述)
    - [1.1 定义与背景](#11-定义与背景)
    - [1.2 应用价值](#12-应用价值)
  - [2. Redis日志体系](#2-redis日志体系)
    - [2.1 Redis日志类型](#21-redis日志类型)
    - [2.2 日志级别](#22-日志级别)
    - [2.3 日志配置](#23-日志配置)
  - [3. ELK日志栈](#3-elk日志栈)
    - [3.1 Elasticsearch部署](#31-elasticsearch部署)
    - [3.2 Logstash配置](#32-logstash配置)
    - [3.3 Kibana可视化](#33-kibana可视化)
    - [3.4 Filebeat采集](#34-filebeat采集)
  - [4. Loki日志系统](#4-loki日志系统)
    - [4.1 Loki部署](#41-loki部署)
    - [4.2 Promtail采集](#42-promtail采集)
    - [4.3 Grafana集成](#43-grafana集成)
  - [5. 日志采集实践](#5-日志采集实践)
    - [5.1 Redis日志采集](#51-redis日志采集)
    - [5.2 应用日志采集](#52-应用日志采集)
    - [5.3 系统日志采集](#53-系统日志采集)
  - [6. 日志分析实践](#6-日志分析实践)
    - [6.1 错误日志分析](#61-错误日志分析)
    - [6.2 性能日志分析](#62-性能日志分析)
    - [6.3 安全日志分析](#63-安全日志分析)
  - [7. 日志存储优化](#7-日志存储优化)
    - [7.1 日志压缩](#71-日志压缩)
    - [7.2 日志轮转](#72-日志轮转)
    - [7.3 日志清理](#73-日志清理)
  - [8. 生产环境部署实践](#8-生产环境部署实践)
    - [8.1 完整ELK栈部署脚本](#81-完整elk栈部署脚本)
      - [8.1.1 一键部署脚本](#811-一键部署脚本)
      - [8.1.2 集群部署脚本](#812-集群部署脚本)
    - [8.2 大规模日志采集工具](#82-大规模日志采集工具)
      - [8.2.1 分布式日志采集器](#821-分布式日志采集器)
      - [8.2.2 日志分析工具](#822-日志分析工具)
    - [8.3 生产环境监控脚本](#83-生产环境监控脚本)
      - [8.3.1 日志系统健康检查](#831-日志系统健康检查)
    - [8.4 最佳实践](#84-最佳实践)
    - [8.4.1 日志格式规范](#841-日志格式规范)
    - [8.4.2 日志级别选择](#842-日志级别选择)
    - [8.4.3 日志采样策略](#843-日志采样策略)
  - [9. 扩展阅读](#9-扩展阅读)
  - [10. 权威参考](#10-权威参考)
    - [10.1 官方文档](#101-官方文档)
    - [10.2 经典书籍](#102-经典书籍)
    - [10.3 在线资源](#103-在线资源)

---

## 1. 概述

### 1.1 定义与背景

日志管理是Redis运维的重要组成部分，通过集中收集、存储和分析日志，实现系统可观测性。

**日志管理背景**：

- **集中管理**：统一收集和存储所有日志
- **快速检索**：快速定位问题和分析性能
- **实时监控**：实时监控系统状态和异常

### 1.2 应用价值

**日志管理价值**：

- ✅ **问题定位**：快速定位故障原因
- ✅ **性能分析**：分析系统性能瓶颈
- ✅ **安全审计**：记录安全事件和操作
- ✅ **趋势分析**：分析系统运行趋势

---

## 2. Redis日志体系

### 2.1 Redis日志类型

**Redis日志类型**：

- **服务器日志**：Redis服务器运行日志
- **慢查询日志**：慢查询命令记录
- **AOF日志**：持久化操作日志
- **复制日志**：主从复制日志

### 2.2 日志级别

**Redis日志级别**：

```conf
# redis.conf
loglevel notice  # debug, verbose, notice, warning
```

**日志级别说明**：

- **debug**：详细调试信息
- **verbose**：详细信息
- **notice**：重要信息（默认）
- **warning**：警告信息

### 2.3 日志配置

**Redis日志配置**：

```conf
# redis.conf
# 日志级别
loglevel notice

# 日志文件
logfile /var/log/redis/redis-server.log

# 慢查询日志
slowlog-log-slower-than 10000  # 10ms
slowlog-max-len 128

# AOF日志
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
```

---

## 3. ELK日志栈

### 3.1 Elasticsearch部署

**Elasticsearch部署（Docker）**：

```yaml
# docker-compose.yml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - elk

volumes:
  es_data:

networks:
  elk:
    driver: bridge
```

**Elasticsearch部署（Kubernetes）**：

```yaml
# elasticsearch-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
spec:
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        ports:
        - containerPort: 9200
        env:
        - name: discovery.type
          value: "single-node"
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        volumeMounts:
        - name: es-data
          mountPath: /usr/share/elasticsearch/data
      volumes:
      - name: es-data
        emptyDir: {}
```

### 3.2 Logstash配置

**Logstash配置（Redis日志）**：

```ruby
# logstash.conf
input {
  file {
    path => "/var/log/redis/redis-server.log"
    start_position => "beginning"
    codec => "json"
  }
}

filter {
  if [message] =~ /ERROR/ {
    mutate {
      add_tag => ["error"]
    }
  }

  if [message] =~ /WARNING/ {
    mutate {
      add_tag => ["warning"]
    }
  }

  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:content}"
    }
  }

  date {
    match => ["timestamp", "ISO8601"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "redis-logs-%{+YYYY.MM.dd}"
  }
}
```

**Logstash部署（Docker）**：

```yaml
# docker-compose.yml
services:
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - /var/log/redis:/var/log/redis:ro
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch
    networks:
      - elk
```

### 3.3 Kibana可视化

**Kibana部署（Docker）**：

```yaml
# docker-compose.yml
services:
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - elk
```

**Kibana Dashboard配置**：

```json
{
  "title": "Redis日志分析",
  "panels": [
    {
      "type": "visualization",
      "id": "error-rate",
      "title": "错误率趋势"
    },
    {
      "type": "visualization",
      "id": "log-level-distribution",
      "title": "日志级别分布"
    },
    {
      "type": "visualization",
      "id": "top-errors",
      "title": "Top错误"
    }
  ]
}
```

### 3.4 Filebeat采集

**Filebeat配置**：

```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/redis/redis-server.log
  fields:
    log_type: redis
    environment: production
  fields_under_root: true
  multiline.pattern: '^\d{4}-\d{2}-\d{2}'
  multiline.negate: true
  multiline.match: after

output.logstash:
  hosts: ["logstash:5044"]

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
```

**Filebeat部署（Docker）**：

```yaml
# docker-compose.yml
services:
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/log/redis:/var/log/redis:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - logstash
    networks:
      - elk
```

---

## 4. Loki日志系统

### 4.1 Loki部署

**Loki部署（Docker）**：

```yaml
# docker-compose.yml
version: '3.8'
services:
  loki:
    image: grafana/loki:2.9.0
    container_name: loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    networks:
      - observability

volumes:
  loki_data:

networks:
  observability:
    driver: bridge
```

**Loki配置**：

```yaml
# loki-config.yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/index_cache
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
```

### 4.2 Promtail采集

**Promtail配置**：

```yaml
# promtail-config.yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: redis
    static_configs:
      - targets:
          - localhost
        labels:
          job: redis
          __path__: /var/log/redis/*.log
    pipeline_stages:
      - regex:
          expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>\w+)\] (?P<message>.*)$'
      - labels:
          level:
      - timestamp:
          source: timestamp
          format: '2006-01-02 15:04:05'
```

**Promtail部署（Docker）**：

```yaml
# docker-compose.yml
services:
  promtail:
    image: grafana/promtail:2.9.0
    container_name: promtail
    volumes:
      - ./promtail-config.yaml:/etc/promtail/config.yml
      - /var/log/redis:/var/log/redis:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - observability
```

### 4.3 Grafana集成

**Grafana数据源配置**：

```yaml
# grafana-datasource.yaml
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    jsonData:
      maxLines: 1000
```

**Grafana Dashboard查询**：

```logql
# 错误日志查询
{job="redis"} |= "ERROR"

# 按级别统计
sum by (level) (count_over_time({job="redis"}[5m]))

# Top错误
topk(10, sum by (message) (count_over_time({job="redis"} |= "ERROR" [1h])))
```

---

## 5. 日志采集实践

### 5.1 Redis日志采集

**Redis日志采集脚本**：

```python
#!/usr/bin/env python3
"""
Redis日志采集脚本
"""
import subprocess
import json
import time
from datetime import datetime

class RedisLogCollector:
    def __init__(self, log_file, output_file):
        self.log_file = log_file
        self.output_file = output_file

    def collect_logs(self):
        """采集Redis日志"""
        with open(self.log_file, 'r') as f:
            # 读取新日志
            f.seek(0, 2)  # 移动到文件末尾
            while True:
                line = f.readline()
                if line:
                    log_entry = self.parse_log(line)
                    self.send_to_logstash(log_entry)
                else:
                    time.sleep(0.1)

    def parse_log(self, line):
        """解析日志行"""
        # Redis日志格式: timestamp [level] message
        parts = line.strip().split(']', 1)
        if len(parts) == 2:
            level_part = parts[0].split('[')
            if len(level_part) == 2:
                timestamp = level_part[0].strip()
                level = level_part[1].strip()
                message = parts[1].strip()

                return {
                    'timestamp': timestamp,
                    'level': level,
                    'message': message,
                    'source': 'redis',
                    'host': self.get_hostname()
                }
        return None

    def send_to_logstash(self, log_entry):
        """发送到Logstash"""
        if log_entry:
            print(json.dumps(log_entry))

    def get_hostname(self):
        """获取主机名"""
        import socket
        return socket.gethostname()

if __name__ == '__main__':
    collector = RedisLogCollector(
        '/var/log/redis/redis-server.log',
        '/tmp/redis-logs.json'
    )
    collector.collect_logs()
```

### 5.2 应用日志采集

**应用日志采集配置**：

```python
# app_logging.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    """JSON日志格式化器"""
    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }

        if record.exc_info:
            log_entry['exception'] = self.formatException(record.exc_info)

        return json.dumps(log_entry)

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[
        logging.FileHandler('/var/log/app/application.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
logger.handlers[0].setFormatter(JSONFormatter())

# 使用示例
logger.info("Redis连接成功", extra={'redis_host': 'localhost', 'redis_port': 6379})
logger.error("Redis连接失败", extra={'error': 'Connection refused'})
```

### 5.3 系统日志采集

**系统日志采集（rsyslog）**：

```conf
# /etc/rsyslog.conf
# Redis日志转发
local0.*    @@logstash:514

# 系统日志转发
*.info;mail.none;authpriv.none;cron.none    @@logstash:514
```

**rsyslog配置（Redis）**：

```conf
# /etc/rsyslog.d/redis.conf
# Redis日志配置
$ModLoad imfile
$InputFileName /var/log/redis/redis-server.log
$InputFileTag redis:
$InputFileStateFile redis-state
$InputFileSeverity info
$InputFileFacility local0
$InputRunFileMonitor

# 转发到Logstash
local0.*    @@logstash:514
```

---

## 6. 日志分析实践

### 6.1 错误日志分析

**错误日志分析查询（Elasticsearch）**：

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "level": "ERROR"
          }
        },
        {
          "range": {
            "@timestamp": {
              "gte": "now-1h"
            }
          }
        }
      ]
    }
  },
  "aggs": {
    "error_types": {
      "terms": {
        "field": "message.keyword",
        "size": 10
      }
    },
    "error_timeline": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "5m"
      }
    }
  }
}
```

**错误日志分析脚本**：

```python
#!/usr/bin/env python3
"""
错误日志分析脚本
"""
from elasticsearch import Elasticsearch
from datetime import datetime, timedelta

class ErrorLogAnalyzer:
    def __init__(self, es_host='localhost', es_port=9200):
        self.es = Elasticsearch([{'host': es_host, 'port': es_port}])

    def analyze_errors(self, hours=1):
        """分析错误日志"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)

        query = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"level": "ERROR"}},
                        {"range": {"@timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }}}
                    ]
                }
            },
            "aggs": {
                "error_types": {
                    "terms": {
                        "field": "message.keyword",
                        "size": 10
                    }
                }
            }
        }

        result = self.es.search(
            index="redis-logs-*",
            body=query
        )

        return result

    def get_top_errors(self, top_n=10):
        """获取Top错误"""
        result = self.analyze_errors()
        errors = result['aggregations']['error_types']['buckets']

        print(f"Top {top_n} 错误:")
        for i, error in enumerate(errors[:top_n], 1):
            print(f"{i}. {error['key']}: {error['doc_count']} 次")

if __name__ == '__main__':
    analyzer = ErrorLogAnalyzer()
    analyzer.get_top_errors()
```

### 6.2 性能日志分析

**性能日志分析查询**：

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "message": "slow"
          }
        }
      ]
    }
  },
  "aggs": {
    "slow_commands": {
      "terms": {
        "field": "command.keyword",
        "size": 10
      },
      "aggs": {
        "avg_duration": {
          "avg": {
            "field": "duration"
          }
        }
      }
    }
  }
}
```

### 6.3 安全日志分析

**安全日志分析查询**：

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "message": "AUTH"
          }
        },
        {
          "match": {
            "level": "WARNING"
          }
        }
      ]
    }
  },
  "aggs": {
    "failed_auth": {
      "terms": {
        "field": "client_ip.keyword",
        "size": 10
      }
    }
  }
}
```

---

## 7. 日志存储优化

### 7.1 日志压缩

**日志压缩配置（Elasticsearch）**：

```json
{
  "settings": {
    "index": {
      "codec": "best_compression",
      "number_of_shards": 1,
      "number_of_replicas": 1
    }
  },
  "mappings": {
    "properties": {
      "@timestamp": {
        "type": "date"
      },
      "message": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 256
          }
        }
      }
    }
  }
}
```

### 7.2 日志轮转

**日志轮转配置（logrotate）**：

```conf
# /etc/logrotate.d/redis
/var/log/redis/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    create 0644 redis redis
    sharedscripts
    postrotate
        /usr/bin/redis-cli CONFIG SET logfile /var/log/redis/redis-server.log
    endscript
}
```

### 7.3 日志清理

**日志清理脚本**：

```bash
#!/bin/bash
# 清理旧日志

# Elasticsearch索引清理
curl -X DELETE "localhost:9200/redis-logs-$(date -d '30 days ago' +%Y.%m.%d)"

# Loki日志清理
# Loki自动清理，配置retention_period
```

**Loki保留策略配置**：

```yaml
# loki-config.yaml
limits_config:
  retention_period: 720h  # 30天
```

---

## 8. 生产环境部署实践

### 8.1 完整ELK栈部署脚本

#### 8.1.1 一键部署脚本

```bash
#!/bin/bash
# deploy-elk-stack.sh

set -e

ELK_VERSION="8.11.0"
ELASTIC_PASSWORD="${ELASTIC_PASSWORD:-changeme}"
LOG_DIR="/var/log/redis"
DATA_DIR="/data/elk"

echo "开始部署ELK栈..."

# 创建目录
mkdir -p "$DATA_DIR"/{elasticsearch,logstash,kibana}
mkdir -p "$LOG_DIR"

# 设置Elasticsearch配置
cat > "$DATA_DIR/elasticsearch/elasticsearch.yml" <<EOF
cluster.name: redis-logs-cluster
node.name: es-node-1
network.host: 0.0.0.0
discovery.type: single-node
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
path.data: /usr/share/elasticsearch/data
path.logs: /usr/share/elasticsearch/logs
EOF

# 设置Logstash配置
cat > "$DATA_DIR/logstash/logstash.conf" <<EOF
input {
  beats {
    port => 5044
  }
  file {
    path => "/var/log/redis/*.log"
    start_position => "beginning"
    codec => "json"
    type => "redis"
  }
}

filter {
  if [type] == "redis" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{GREEDYDATA:content}"
      }
    }

    date {
      match => ["timestamp", "ISO8601"]
    }

    if [level] == "ERROR" {
      mutate {
        add_tag => ["error"]
      }
    }

    if [content] =~ /slow/i {
      mutate {
        add_tag => ["slow_query"]
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    index => "redis-logs-%{+YYYY.MM.dd}"
  }
}
EOF

# 设置Kibana配置
cat > "$DATA_DIR/kibana/kibana.yml" <<EOF
server.host: "0.0.0.0"
server.shutdownTimeout: "5s"
elasticsearch.hosts: ["http://elasticsearch:9200"]
elasticsearch.username: "elastic"
elasticsearch.password: "${ELASTIC_PASSWORD}"
xpack.security.enabled: true
EOF

# 创建docker-compose文件
cat > "$DATA_DIR/docker-compose.yml" <<EOF
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELK_VERSION}
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - xpack.security.enabled=true
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - $DATA_DIR/elasticsearch:/usr/share/elasticsearch/config
      - es_data:/usr/share/elasticsearch/data
    networks:
      - elk
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  logstash:
    image: docker.elastic.co/logstash/logstash:${ELK_VERSION}
    container_name: logstash
    volumes:
      - $DATA_DIR/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - $LOG_DIR:/var/log/redis:ro
    ports:
      - "5044:5044"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - elk
    environment:
      - LS_JAVA_OPTS=-Xmx1g -Xms1g

  kibana:
    image: docker.elastic.co/kibana/kibana:${ELK_VERSION}
    container_name: kibana
    ports:
      - "5601:5601"
    volumes:
      - $DATA_DIR/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - elk
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}

volumes:
  es_data:
    driver: local

networks:
  elk:
    driver: bridge
EOF

# 启动服务
cd "$DATA_DIR"
docker-compose up -d

echo "等待服务启动..."
sleep 30

# 验证服务
echo "验证Elasticsearch..."
curl -u elastic:${ELASTIC_PASSWORD} http://localhost:9200/_cluster/health

echo "验证Kibana..."
curl http://localhost:5601/api/status

echo "ELK栈部署完成！"
echo "Kibana: http://localhost:5601"
echo "用户名: elastic"
echo "密码: ${ELASTIC_PASSWORD}"
```

#### 8.1.2 集群部署脚本

```bash
#!/bin/bash
# deploy-elk-cluster.sh

set -e

ELK_VERSION="8.11.0"
ELASTIC_PASSWORD="${ELASTIC_PASSWORD:-changeme}"
ES_NODES=3
LOG_DIR="/var/log/redis"
DATA_DIR="/data/elk-cluster"

echo "开始部署ELK集群（${ES_NODES}个节点）..."

# 创建目录
for i in $(seq 1 $ES_NODES); do
    mkdir -p "$DATA_DIR/elasticsearch/node-$i"/{data,logs,config}
done

mkdir -p "$DATA_DIR"/{logstash,kibana}
mkdir -p "$LOG_DIR"

# 生成Elasticsearch集群配置
for i in $(seq 1 $ES_NODES); do
    cat > "$DATA_DIR/elasticsearch/node-$i/config/elasticsearch.yml" <<EOF
cluster.name: redis-logs-cluster
node.name: es-node-$i
node.roles: [master, data, ingest]
network.host: 0.0.0.0
discovery.seed_hosts: ["es-node-1", "es-node-2", "es-node-3"]
cluster.initial_master_nodes: ["es-node-1", "es-node-2", "es-node-3"]
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
path.data: /usr/share/elasticsearch/data
path.logs: /usr/share/elasticsearch/logs
EOF
done

# 创建docker-compose集群文件
cat > "$DATA_DIR/docker-compose.yml" <<EOF
version: '3.8'
services:
EOF

for i in $(seq 1 $ES_NODES); do
    cat >> "$DATA_DIR/docker-compose.yml" <<EOF
  elasticsearch-node-$i:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELK_VERSION}
    container_name: elasticsearch-node-$i
    hostname: es-node-$i
    environment:
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - xpack.security.enabled=true
    ports:
      - "$((9200 + i - 1)):9200"
    volumes:
      - $DATA_DIR/elasticsearch/node-$i/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - es_data_$i:/usr/share/elasticsearch/data
    networks:
      - elk
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

EOF
done

cat >> "$DATA_DIR/docker-compose.yml" <<EOF
  logstash:
    image: docker.elastic.co/logstash/logstash:${ELK_VERSION}
    container_name: logstash
    volumes:
      - $DATA_DIR/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - $LOG_DIR:/var/log/redis:ro
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch-node-1
    networks:
      - elk
    environment:
      - LS_JAVA_OPTS=-Xmx2g -Xms2g

  kibana:
    image: docker.elastic.co/kibana/kibana:${ELK_VERSION}
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch-node-1
    networks:
      - elk
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch-node-1:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}

volumes:
EOF

for i in $(seq 1 $ES_NODES); do
    echo "  es_data_$i:" >> "$DATA_DIR/docker-compose.yml"
    echo "    driver: local" >> "$DATA_DIR/docker-compose.yml"
done

cat >> "$DATA_DIR/docker-compose.yml" <<EOF

networks:
  elk:
    driver: bridge
EOF

# 启动服务
cd "$DATA_DIR"
docker-compose up -d

echo "等待集群启动..."
sleep 60

# 验证集群
echo "验证Elasticsearch集群..."
curl -u elastic:${ELASTIC_PASSWORD} http://localhost:9200/_cluster/health?pretty

echo "ELK集群部署完成！"
```

### 8.2 大规模日志采集工具

#### 8.2.1 分布式日志采集器

```python
#!/usr/bin/env python3
# distributed-log-collector.py

import redis
import json
import time
import threading
import queue
from datetime import datetime
from collections import defaultdict
import requests
from concurrent.futures import ThreadPoolExecutor

class DistributedLogCollector:
    """分布式日志采集器"""

    def __init__(self, redis_cluster_nodes, logstash_url, batch_size=1000, flush_interval=10):
        self.redis_cluster = redis.RedisCluster(
            startup_nodes=redis_cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True
        )
        self.logstash_url = logstash_url
        self.batch_size = batch_size
        self.flush_interval = flush_interval

        self.log_queue = queue.Queue(maxsize=10000)
        self.batch_buffer = []
        self.stats = defaultdict(int)
        self.lock = threading.Lock()

        # 启动工作线程
        self.worker_thread = threading.Thread(target=self._worker, daemon=True)
        self.worker_thread.start()

        self.flush_thread = threading.Thread(target=self._flush_periodically, daemon=True)
        self.flush_thread.start()

    def collect_redis_logs(self, log_file_path):
        """采集Redis日志"""
        with open(log_file_path, 'r') as f:
            f.seek(0, 2)  # 移动到文件末尾

            while True:
                line = f.readline()
                if line:
                    log_entry = self._parse_log_line(line)
                    if log_entry:
                        self._add_log_entry(log_entry)
                else:
                    time.sleep(0.1)

    def _parse_log_line(self, line):
        """解析日志行"""
        try:
            # Redis日志格式: timestamp [level] message
            parts = line.strip().split(']', 1)
            if len(parts) == 2:
                level_part = parts[0].split('[')
                if len(level_part) == 2:
                    timestamp_str = level_part[0].strip()
                    level = level_part[1].strip()
                    message = parts[1].strip()

                    return {
                        'timestamp': timestamp_str,
                        'level': level,
                        'message': message,
                        'source': 'redis',
                        'host': self._get_hostname(),
                        'collected_at': datetime.utcnow().isoformat()
                    }
        except Exception as e:
            print(f"解析日志行失败: {e}")

        return None

    def _add_log_entry(self, log_entry):
        """添加日志条目到队列"""
        try:
            self.log_queue.put_nowait(log_entry)
            with self.lock:
                self.stats['collected'] += 1
        except queue.Full:
            with self.lock:
                self.stats['dropped'] += 1

    def _worker(self):
        """工作线程：处理日志队列"""
        while True:
            try:
                log_entry = self.log_queue.get(timeout=1)
                self.batch_buffer.append(log_entry)

                if len(self.batch_buffer) >= self.batch_size:
                    self._flush_batch()
            except queue.Empty:
                continue
            except Exception as e:
                print(f"工作线程错误: {e}")

    def _flush_periodically(self):
        """定期刷新批次"""
        while True:
            time.sleep(self.flush_interval)
            if self.batch_buffer:
                self._flush_batch()

    def _flush_batch(self):
        """刷新批次到Logstash"""
        if not self.batch_buffer:
            return

        batch = self.batch_buffer.copy()
        self.batch_buffer.clear()

        try:
            # 发送到Logstash
            response = requests.post(
                f"{self.logstash_url}/_bulk",
                json=batch,
                timeout=10
            )

            if response.status_code == 200:
                with self.lock:
                    self.stats['sent'] += len(batch)
            else:
                with self.lock:
                    self.stats['failed'] += len(batch)
                    # 重新加入队列
                    for entry in batch:
                        try:
                            self.log_queue.put_nowait(entry)
                        except queue.Full:
                            pass
        except Exception as e:
            print(f"发送批次失败: {e}")
            with self.lock:
                self.stats['failed'] += len(batch)

    def get_stats(self):
        """获取统计信息"""
        with self.lock:
            return dict(self.stats)

    def _get_hostname(self):
        """获取主机名"""
        import socket
        return socket.gethostname()

# 使用示例
if __name__ == '__main__':
    redis_cluster_nodes = [
        {'host': '127.0.0.1', 'port': 7000}
    ]

    collector = DistributedLogCollector(
        redis_cluster_nodes=redis_cluster_nodes,
        logstash_url='http://logstash:5044',
        batch_size=1000,
        flush_interval=10
    )

    # 采集多个日志文件
    log_files = [
        '/var/log/redis/redis-server.log',
        '/var/log/redis/slowlog.log',
        '/var/log/redis/aof.log'
    ]

    with ThreadPoolExecutor(max_workers=len(log_files)) as executor:
        for log_file in log_files:
            executor.submit(collector.collect_redis_logs, log_file)

    # 定期打印统计信息
    while True:
        time.sleep(60)
        stats = collector.get_stats()
        print(f"统计信息: {stats}")
```

#### 8.2.2 日志分析工具

```python
#!/usr/bin/env python3
# log-analyzer.py

from elasticsearch import Elasticsearch
from datetime import datetime, timedelta
from collections import defaultdict
import json

class RedisLogAnalyzer:
    """Redis日志分析器"""

    def __init__(self, es_host='localhost', es_port=9200, es_user='elastic', es_password='changeme'):
        self.es = Elasticsearch(
            [{'host': es_host, 'port': es_port}],
            http_auth=(es_user, es_password),
            timeout=30
        )

    def analyze_errors(self, hours=1, top_n=10):
        """分析错误日志"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)

        query = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"level": "ERROR"}},
                        {"range": {"@timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }}}
                    ]
                }
            },
            "aggs": {
                "error_types": {
                    "terms": {
                        "field": "message.keyword",
                        "size": top_n
                    }
                },
                "error_timeline": {
                    "date_histogram": {
                        "field": "@timestamp",
                        "interval": "5m"
                    }
                },
                "error_by_host": {
                    "terms": {
                        "field": "host.keyword",
                        "size": 10
                    }
                }
            },
            "size": 0
        }

        result = self.es.search(
            index="redis-logs-*",
            body=query
        )

        return {
            'total_errors': result['hits']['total']['value'],
            'error_types': result['aggregations']['error_types']['buckets'],
            'error_timeline': result['aggregations']['error_timeline']['buckets'],
            'error_by_host': result['aggregations']['error_by_host']['buckets']
        }

    def analyze_slow_queries(self, hours=1, min_duration_ms=100):
        """分析慢查询"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)

        query = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"tags": "slow_query"}},
                        {"range": {"@timestamp": {
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }}}
                    ]
                }
            },
            "aggs": {
                "slow_commands": {
                    "terms": {
                        "field": "command.keyword",
                        "size": 10
                    },
                    "aggs": {
                        "avg_duration": {
                            "avg": {
                                "field": "duration_ms"
                            }
                        }
                    }
                }
            },
            "size": 0
        }

        result = self.es.search(
            index="redis-logs-*",
            body=query
        )

        return {
            'total_slow_queries': result['hits']['total']['value'],
            'slow_commands': result['aggregations']['slow_commands']['buckets']
        }

    def analyze_log_volume(self, hours=24):
        """分析日志量"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours)

        query = {
            "query": {
                "range": {
                    "@timestamp": {
                        "gte": start_time.isoformat(),
                        "lte": end_time.isoformat()
                    }
                }
            },
            "aggs": {
                "log_volume": {
                    "date_histogram": {
                        "field": "@timestamp",
                        "interval": "1h"
                    }
                },
                "log_by_level": {
                    "terms": {
                        "field": "level.keyword"
                    }
                }
            },
            "size": 0
        }

        result = self.es.search(
            index="redis-logs-*",
            body=query
        )

        return {
            'total_logs': result['hits']['total']['value'],
            'log_volume': result['aggregations']['log_volume']['buckets'],
            'log_by_level': result['aggregations']['log_by_level']['buckets']
        }

    def generate_report(self, hours=24):
        """生成分析报告"""
        print(f"=== Redis日志分析报告（过去{hours}小时）===\n")

        # 错误分析
        error_analysis = self.analyze_errors(hours=hours)
        print(f"总错误数: {error_analysis['total_errors']}")
        print("\nTop错误类型:")
        for i, error_type in enumerate(error_analysis['error_types'][:10], 1):
            print(f"  {i}. {error_type['key']}: {error_type['doc_count']}次")

        # 慢查询分析
        slow_analysis = self.analyze_slow_queries(hours=hours)
        print(f"\n总慢查询数: {slow_analysis['total_slow_queries']}")
        print("\n慢查询命令:")
        for i, cmd in enumerate(slow_analysis['slow_commands'][:10], 1):
            print(f"  {i}. {cmd['key']}: 平均延迟 {cmd['avg_duration']['value']:.2f}ms")

        # 日志量分析
        volume_analysis = self.analyze_log_volume(hours=hours)
        print(f"\n总日志数: {volume_analysis['total_logs']}")
        print("\n日志级别分布:")
        for level in volume_analysis['log_by_level']['buckets']:
            print(f"  {level['key']}: {level['doc_count']}条")

# 使用示例
if __name__ == '__main__':
    analyzer = RedisLogAnalyzer(
        es_host='localhost',
        es_port=9200,
        es_user='elastic',
        es_password='changeme'
    )

    analyzer.generate_report(hours=24)
```

### 8.3 生产环境监控脚本

#### 8.3.1 日志系统健康检查

```bash
#!/bin/bash
# check-log-system-health.sh

ELASTICSEARCH_URL="${ELASTICSEARCH_URL:-http://localhost:9200}"
LOGSTASH_URL="${LOGSTASH_URL:-http://localhost:5044}"
KIBANA_URL="${ELASTICSEARCH_URL:-http://localhost:5601}"
ES_USER="${ES_USER:-elastic}"
ES_PASSWORD="${ES_PASSWORD:-changeme}"

echo "=== 日志系统健康检查 ==="

# 检查Elasticsearch
echo -e "\n1. 检查Elasticsearch..."
es_health=$(curl -s -u "${ES_USER}:${ES_PASSWORD}" "${ELASTICSEARCH_URL}/_cluster/health")
es_status=$(echo "$es_health" | grep -o '"status":"[^"]*"' | cut -d'"' -f4)

if [ "$es_status" == "green" ] || [ "$es_status" == "yellow" ]; then
    echo "✓ Elasticsearch状态: $es_status"
else
    echo "✗ Elasticsearch状态异常: $es_status"
    exit 1
fi

# 检查索引
echo -e "\n2. 检查索引..."
indices=$(curl -s -u "${ES_USER}:${ES_PASSWORD}" "${ELASTICSEARCH_URL}/_cat/indices/redis-logs-*?v")
echo "$indices"

# 检查Logstash
echo -e "\n3. 检查Logstash..."
if curl -s "${LOGSTASH_URL}" > /dev/null 2>&1; then
    echo "✓ Logstash可访问"
else
    echo "✗ Logstash不可访问"
fi

# 检查Kibana
echo -e "\n4. 检查Kibana..."
kibana_status=$(curl -s "${KIBANA_URL}/api/status" | grep -o '"status":"[^"]*"' | cut -d'"' -f4)
if [ "$kibana_status" == "green" ]; then
    echo "✓ Kibana状态: $kibana_status"
else
    echo "✗ Kibana状态异常: $kibana_status"
fi

# 检查日志采集
echo -e "\n5. 检查日志采集..."
recent_logs=$(curl -s -u "${ES_USER}:${ES_PASSWORD}" "${ELASTICSEARCH_URL}/redis-logs-*/_search" \
    -H 'Content-Type: application/json' \
    -d '{
        "query": {
            "range": {
                "@timestamp": {
                    "gte": "now-5m"
                }
            }
        }
    }' | grep -o '"total":[0-9]*' | cut -d':' -f2)

if [ "$recent_logs" -gt 0 ]; then
    echo "✓ 最近5分钟采集日志数: $recent_logs"
else
    echo "⚠ 最近5分钟未采集到日志"
fi

echo -e "\n=== 健康检查完成 ==="
```

### 8.4 最佳实践

### 8.4.1 日志格式规范

**结构化日志格式**：

```json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "INFO",
  "service": "redis",
  "host": "redis-01",
  "message": "Command executed",
  "command": "GET",
  "key": "user:123",
  "duration_ms": 0.5,
  "client_ip": "192.168.1.100",
  "trace_id": "abc123",
  "span_id": "def456"
}
```

### 8.4.2 日志级别选择

**日志级别使用指南**：

- **DEBUG**：详细调试信息，开发环境使用
- **INFO**：一般信息，正常操作记录
- **WARNING**：警告信息，需要关注但不影响运行
- **ERROR**：错误信息，需要立即处理

### 8.4.3 日志采样策略

**日志采样配置**：

```python
# 高频日志采样
import random

def should_log(level, sample_rate=0.1):
    """决定是否记录日志"""
    if level == "DEBUG":
        return random.random() < sample_rate
    return True

# 使用示例
if should_log("DEBUG", sample_rate=0.1):
    logger.debug("Detailed debug information")
```

---

## 9. 扩展阅读

- [Elasticsearch官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Logstash官方文档](https://www.elastic.co/guide/en/logstash/current/index.html)
- [Loki官方文档](https://grafana.com/docs/loki/latest/)
- [Promtail官方文档](https://grafana.com/docs/loki/latest/clients/promtail/)

---

## 10. 权威参考

### 10.1 官方文档

- Redis官方文档：日志配置
- Elasticsearch官方文档
- Grafana Loki官方文档

### 10.2 经典书籍

- 《Elasticsearch: The Definitive Guide》
- 《Logging and Monitoring with Elasticsearch》

### 10.3 在线资源

- Elastic Stack官方博客
- Grafana官方博客
